{
  "hash": "4d9db2bc3a00af79bc80fbf765ed5c0c",
  "result": {
    "markdown": "# Specification and Assessment Issues {#sec-specification}\n\nThis concluding chapter deals with an epistemological question, namely how to convey your key results, and an ontological question, how do you know whether what you convey is true (e.g., unbiased). To do so, we first look at specification^[Again, with specification is meant which variables you include in your regression model.] issues in @sec-specificationmodel. Which variables should you include? Thereafter, @sec-presentation discusses how to present your results.\nAnd, subsequently, @sec-sourcesbias deals with the question about the validity of your results. When do you know that you really estimated an **unbiased** parameter. The final section concludes.\n\n## Specification of your model {#sec-specificationmodel}\n\nA long-standing but simple question is how to decide which variables to include in a regression model. Unfortunately, the answer to this question is rather complex. A straightforward but naive approach would be to include them all. So, throw every variable in that is in your database. This, however, leads to \"causal salad\" [a term coined by @mcelreath2020statistical] as displayed in Figure @fig-causalsalad and can actually lead to a biased estimator. One reason for this is that if you include a variable that is related to the error term then all other parameters are biased as well.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Causal salad](./figures/causalsalad.jpg){#fig-causalsalad fig-align='center' width=600px}\n:::\n:::\n\n\n\nSo, for the final time, we return to our Californian school district data and now try to devise a specification that *mimimizes* the chances upon a biased estimator. So, our focus is to get an unbiased estimate of the effect on test scores of changing class size, holding constant student and school characteristics but not necessarily holding constant the budget (we do not want to control for budget as this actually governs class sizes as well).\n\nTo do this we need to think about what variables to include and what specifications to run---and we should do this before we actually sit down before the computer. Think beforehand about your model specification and try to avoid throwing everything in (your causal salad).\n\nIn practice, and especially economics, most follow the following general approach to variable selection and model specification: \n\n1. First you specify a base or benchmark model. In this case that is the univariate regression of test scores on class size.\n2. Then you specify a range of plausible alternative models, which include additional candidate variables.\n3. Then you assess whether a candidate variable changes the coefficient of interest ($\\hat{\\beta}_1$). You keep focusing on the effect of class size!\n4. You assess whether a candidate variable is statistically significantly different from zero; so whether it has an impact of not.\n    - Use judgment, not a mechanical recipe, meaning that a variable being statistically insignificantly different from zero should not automatically be thrown out.\n    - In all cases, do not just try to maximize $\\bar{R^2}$. You focus on identifying a causal effect, not on prediction.\n\nConsidering the last point, it is easy to fall into the trap of maximizing the $\\bar{R^2}$---but this loses sight of our real objective, an unbiased estimator of the class size effect. Recall that a high $\\bar{R^2}$ means that the regressors explain the variation in $Y$. It does **not** mean\n\n-   that you have eliminated omitted variable bias;\n-   that you have an unbiased estimator of a causal effect $(\\beta_1)$;\n-   that the included variables are statistically significant.\n\nSo, in our Californian class size case, what variables would you want---ideally---to include to estimate the effect on test scores of $STR$ using school district data? There is a whole set of potential relevant variables in the California class size data set, being:\n\n-   student-teacher ratio ($STR$)---the variable we focus on;\n-   percent English learners in the district ($PctEL$)---as a proxy for large migrant communities;\n-   school expenditures per pupil---largely correlated with student-teacher ratio;\n-   name of the district (so we could look up average rainfall, for example);\n-   percent eligible for subsidized/free lunch---proxies district income;\n-   percent on public income assistance---proxies district income;\n-   average district income---a measure for district affluency.\n\nSo, which of these variables would you want to include?\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Test scores versus various independent variables](./figures/cadata.png){#fig-cadata fig-align='center' width=800px}\n:::\n:::\n\n\n\nLooking at @fig-cadata, all three percentage variables (English learners, subsidized lunch, and income assistance) behave in a similar manner. But interestingly, the strongest relation is between subsidized lunch and test scores and that is at least the variable that we would like to include.\n\n## Presentation of results {#sec-presentation}\n\nSo, we have a number of regressions (also called specifications) and we want to report them. Often, it is awkward and difficult to read regressions written out in equation form, so instead it is conventional to report them in a table. Note that reading regression estimates from computer output is even more difficult. On top of that it is ugly and contains way too much information.\nTry to avoid statistical computer output as much as possible---at least in your thesis. \nNow, regression tables should include a couple of elements:\n\n-   The estimated regression coefficients.\n-   The standard errors or the $t$-statistics. Having both of them is too much. Do not report $p$-values, because often they are not informative (as they often are reported as $p = 0.000$).\n-   Some measures of fit (usually just the $\\bar{R^2}$ would do).\n-   The number of observations.\n-   Some relevant $F$-statistics, if any. Usually they are not included.\n-   Any other pertinent information but typically there is none.\n\nYou can find most of this information in the final estimation @fig-catable as presented in @stock2003introduction.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Various specifications of test score models](./figures/catable.png){#fig-catable fig-align='center' width=800px}\n:::\n:::\n\n\n\nSo, here the variable of interest (student-teacher ratio) is the first variable on top. And the table keeps focusing on that one.\nMoreover, specification (3) and (5) seems to be preferred as they have the highest $\\bar{R^2}$, although that is perhaps of lesser importance. What we can infer from this is that the estimate for student-teacher ratio remains robust around $-1$ and is significantly different from $0$. Does this now mean that this effect is **unbiased**? Most likely not, but that is something that the next section will discuss.\n\n## Potential sources of bias {#sec-sourcesbias}\n\nAs a last issue of this chapter, we would like to answer the question whether there is a systematic way to assess regression studies?\nWe already have seen that multivariate regression models have some key virtues:\n\n1.  They provide an estimate of the marginal effect of the variable of interest $X$ on $Y$.\n2.  They resolve the problem of omitted variable bias, if an omitted variable can be measured and included.\n3.  They can handle nonlinear relations (effects that vary with the $X$'s) and therefore resolve the problem of misspecification bias.\n\nStill, OLS might yield a **biased** estimator of the true causal effect. In other words, it might not yield valid inferences. That what you want to measure is not what you actually measure. In general there is two ways to assess statistical studies: threats to internal and threats to external validity.\n\n-   **Internal validity**: the statistical inferences about causal effects are valid for the population being studied.\n-   **External validity**: the statistical inferences can be generalized from the population and setting studied to other populations and setting.\n\n### Threats to external validity\n\nSo, above we came to a (tentative) conclusion about the impact of class size on test scores. But we have done so in the context of Californian school districts in the year 2005. Can we extend this finding and generalize class size results from California school districts to other population, for example to that of Massachusetts or Mexico in 2005? And can we do so for differences in institutional settings as there are different legal requirements concerning special education, different treatment of bilingual education, and differences in teacher characteristics across regions and countries.\n\nWe therefore always to be careful to transfer our finding to that of other settings. Note that this is as well a special case of omitted variable bias but now outside the scope of our study (our pre-defined population).\n\n### Threats to internal validity\n\nIn applied econometrics, the following five threats to the internal validity of regression studies are usually given (in statistics there is a different framework for this, but in most cases they come down to the same thing):\n\n1.  omitted variable bias;\n2.  wrong functional form;\n3.  errors-in-variables bias or measurement error;\n4.  sample selection bias;\n5.  simultaneous causality bias.\n\nAll of these imply that $E(u_i|X_{1i},\\ldots,X_{ki}) \\neq 0$, in which case the OLS estimates are therefore **biased**. Let's have a closer look at each of them.\n\n#### Omitted variable bias\n\nOmitted variable bias arises if an omitted variable is both a determinant of $Y$ and a determinant of at least one included regressor.\nWe first discussed omitted variable bias in regression with a single $X$, but omitted variable bias will arise when there are multiple $X$'s as well, if the omitted variable satisfies the two conditions above. Fortunately, there are potential solutions to omitted variable bias:\n\n-   if the variable can be measured, include it as an additional regressor in multiple regression;\n-   possibly, use panel data in which each entity (individual) is observed more than once;\n-   if the variable cannot be measured, use instrumental variables regression (for later courses);\n-   run a randomized controlled experiment.\n\n#### Wrong functional form\n\nThis threat to internal validity arises if the functional form is incorrect. For example, if an interaction term is incorrectly omitted, then inferences on causal effects will be biased. There is a potential solution to functional form misspecification and that is to use the appropriate nonlinear specifications in $X$ (logarithms, interactions, etc.). Sometimes this is not possible and then one has to resort to direct non-linear estimation techniques.\n\n#### Errors-in-variables bias or measurement error\n\nThe third threat is measurement error or sometimes know as errors-in-variables bias. So far we have assumed that $X$ is measured without error. In reality, (economic) data is often measured with measurement error. Especially surveys are prone to measurement error.\nFor example recollection errors that arise with questions as \"which month did you start your current job?\". Or ambiguous questions problems as \"what was your income last year?\" What is meant with the latter: monthly or yearly income, gross or net income? Also respondents sometimes have an incentive not to answer honestly (intentionally false response problems) with questions as \"What is the current value of your financial assets?\" or \"How often do you drink and drive?\". There are potential solutions to errors-in-variables bias, such as:\n\n-   Obtain better data, but that is often not feasible.\n-   Develop a specific model of the measurement error process. This is only possible if a lot is known about the nature of the measurement error---for example a sub-sample of the data are cross-checked using administrative records and the discrepancies are analyzed and modeled.\n-   Instrumental variables regression (but again this is for later courses).\n\n#### Sample selection bias\n\nSo far we have assumed simple random sampling of the population. In some cases, simple random sampling is thwarted because the sample, in effect, **selects itself**. In this case, sample selection bias arises when a selection process both influences the availability of data and if that process is related to the dependent variable. To illustrate this, I will adopt a hypothetical example given by @mcelreath2020statistical. Here we want to look at the relation between trustworthy science and newsworthy science. This example is motivated by the fact that newsworthy science (clickbait in the social media) oftentimes turns out not to be true. To given a reason why this might happen, we first simulate an artificial database of 400 observations of both newsworthy and trustworthy science. \nBoth variables are constructed such that they are $i.i.d.$ and standard normally distributed. So, there is no relation whatsoever and, indeed, @fig-Rplot shows a rather random cloud plot.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Random observations of newsworthy and trustworthy](./figures/Rplot.png){#fig-Rplot fig-align='center' width=600px}\n:::\n:::\n\n\n\nBut what if editors on social media have a decision rule: scientific output should be either thrustworthy or newsworthy, and preferably both. So, as a rule of thumb they select only the top 10% scientific outcomes, so the ones that score in the top 10% when both scores are added up ($\\text{trustworthy} + \\text{newsworthy}$). If we now depict the selected ones in grey in  @fig-Rplot01, then clearly and suddenly a negative relation emerges between newsworthiness and thrustworthiness. And that negative relation is caused by the selection (external) editors make. So, if there is a selection somewhere in the process, estimates of what you want to estimate can quickly become biased.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Negative relation amongst the selected points](./figures/Rplot01.png){#fig-Rplot01 fig-align='center' width=600px}\n:::\n:::\n\n\n\nThis process occurs more often than you might think. Consider the two following examples:\n\n1.  **Aircraft noise externality**.\nHere the question is to what extent people \"value\" aircraft noise (that is in a negative sense)? To aim for an answer we adopt the  following empirical strategy; we collect housing prices close to Schiphol airport (say the small town of Zwanenburg) and compare them with identical houses further away (say the small Dutch town of Schagen which we now assume to have similar features as Zwanenburg except for its location). We have data for individual housing prices (including characteristics) since 1985. As an estimator we assess then the average mean difference between the Zwanenburg and Schagen locations. Now the question is whether there is sample selection bias. And indeed there is and that is caused by the fact that humans react on their own situation based upon their preferences. In this case, they react by means of moving residence. So, those who have strong negative preference regarding aircraft noise are the first to move out Zwanenburg (if possible). And people who do not mind the noise that much are more inclined to stay. So the population in both locations is **not** identical, instead they sorted out spatially.\n\n2.  **Returns to education**.\nThe question here is rather straightforward and involves the monetary returns to an additional year of education. As empirical strategy we collect data of all employed workers in the Netherlands (actually this data exists and is called micro-data), including worker characteristics, years of education, and hourly wages. Our approach is here to regress $\\ln(Earnings)$ on $YearsEducation$ and a large set of other characteristics. Now, ignore issues of omitted variable bias and measurement error, then the question is: is there sample selection bias? And, indeed, there is again, as you only sample those people who are employed and not the unemployed (they have no current wage). And this leads to a different population than you wanted in the first place.\n\nThere are some potential solutions to sample selection bias and most of them deal with data issues. For example, you might want to collect the sample in a way that avoids sample selection. For example you might want to focus on those people who moved between Schagen and Zwanenburg or you include the unemployed as well in the returns to education example.\n\n#### Simultaneous causality bias in equations\n\nFinally, our last threat to causality is simultaneous or reverse causality bias. This means that the causal effect might go either way as in the following system\n\n-   Causal effect on $Y$ of $X$: $Y_i = \\beta_0 + \\beta_1 X_i + u_i$\n-   Causal effect on $X$ of $Y$: $X_i = \\gamma_0 + \\gamma_1 Y_i + v_i$\n\nWhere a large $u_i$ means a large $Y_i$, which implies large $X_i$ (if $\\gamma_1>0$) and therefore, by definition, $corr(X_i,u_i) \\neq 0$. Thus, $\\hat{\\beta_1}$ is biased and inconsistent. In our Californian school district example it might as well be that a district with particularly bad test scores given the $STR$ (negative $u_i$) receives extra resources, thereby lowering its $STR$; so $STR_i$ and $u_i$ are then correlated\n\nThere are some potential solutions to simultaneous causality bias\n\nThe first and always the best one is to conduct a randomized controlled experiment. Because,if $X_i$ is chosen at random by the experimenter, there is no feedback from the outcome variable to $Y_i$ (assuming perfect compliance). Secondly, you can develop and estimate a complete model of both directions of causality. This is the idea behind many large macro-economic models (e.g. those of the Federal Reserve Bank in the US). This is difficult in practice.\n\nFinally, you can use instrumental variables regression again to estimate the causal effect of interest (effect of $X$ on $Y$, ignoring effect of $Y$ on $X$). But, again, that is something you will encounter in more advanced courses.\n\n## Concluding remarks {#sec-conclusionspec}\n\nAt first sight this chapters seems to consists of two not so much related subjects: presentation and validity issues. But in fact they are very much related. If you present your work as open, translucent and clear as possible, then it is easier for others (and yourself by the way) to detect flaws in the underlying reasoning and errors in the execution. And making errors it not per se a bad thing---we all make them. You only want to detect them as early as possible. Good presentation helps enormously with that. \n\nValidity issues and especially the threats to internal validity (all those biases) seem a bit daunting. So many things can go wrong! And that is true but already knowing possible threats to your own estimation can bring you further. It can point you to paths where to find additional data, use other methods or simply know that a particular research question can not (yet) be answered. At least, it should make you modest in your claims in your conclusion. Finding real causal mechanism is hard, but every analysis (when well donw) brings us a bit further in our understanding. Albeit slow, science does progress.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}