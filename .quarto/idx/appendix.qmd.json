{"title":"Reviewing probability and statistics","markdown":{"headingText":"Reviewing probability and statistics","headingAttr":{"id":"sec-reviewstat","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n```{r}\n#| warning: FALSE\n#| message: FALSE\n#| echo: FALSE\nlibrary(tidyverse)\nlibrary(kableExtra)\n```\n\nThis appendix very briefly reviews the statistical knowledge you need for applied econometrics. We assume you had an introductory course in statistics already and will go over this material very quickly.\n\n## Reviewing probability\n\n### Probability\n\nAs a definition of probability we use the concept of *empirical probability* which is the proportion of time that something (a specific outcome or event $X$) occurs in the long-run of total events. Usually it is give by:\n\n\\begin{equation}\n \\text{probability} = p = \\frac{\\text{Number of times specific event $X$ happens}}{\\text{Total amount of events that can happen}}\n\\end{equation}\n\nNow probabilities are *defined* by a set of definations (axioms). These are:\n\n1) Probabilities, $p$, are always between 0 and 1. So, $0 \\leq p \\leq 1$\n2) If something does not happen, then $p = 0$\n3) If something always happens, then $p = 1$\n4) Probabilities for the total amount of events always add up to $1$. So, if the probability that something happens is $p$, then the probabilities that it will not happen is $1 -p$ (see that $p + 1 - p = 1$)\n\n### Population & random variables\n\nIn general we see a population as the the group or collection of all possible entities of interest (school districts, inhabitants of the Netherlands, homeowners) and we will think of populations as infinitely large ($\\infty$). From this population we then *sample* specific observations. This sample contains then a **random** variable $Y$, which denotes a characteristics of the entity (district average test score, prices of houses, prices of meat). An important feature is that the specific contents of the sample is unknown, that is before measurement ($y$), after measurement the sample is know and is called data. \n\nSo, a random variable (also called a stochastic variable) is a mathematical formalization of something that depends on **random** outcomes. Unfortunately, randomness is not clearly defined and depends on specific scientific philosophical schools. The scientific philosophical school we implicitly assume in this course---and, in fact, in most statistical courses---is that of frequentist statistics. Here we assume that all things we measure are *intrinsically* random. In fact, this is an *ontological* argument---in other words, what are our beliefs in the state of the world. Because all things we measure are random, every time we measure something our measurements are (slightly) different. However, the more we measure, the more precise we *know* something. But there is still randomness. \n\nIn general, there are two types of random variables. First, there are *discrete* random variables, where outcomes can be counted, such as $0, 1, 2, 3, \\ldots$ and *continuous* random variables, where outcomes can be any real number.^[There is slightly more to this as fractions such as $\\frac{1}{2}$ can in fact be counted as well, and continuous outcomes can be as well complex numbers. But for now we typically see integer numbers as discrete, and real numbers as continuous.]\n\n### Distribution functions\n\nRandom variables are governed by *distribution functions* which are mathematical functions that provides the probabilities of occurrence of all different possible outcomes of a specific *experiment*^[This could be the throw of a dice but as well the measurement of 10,000 house prices.]: e.g. for a discrete distribution, $f(x) = \\Pr(Y = y)$  $\\forall y$. Or, in other words, the distribution function maps discrete outcomes to probabilities. For continuous distribution function, this is not possible as there an infinite number of possible outcomes, so that means that for each specific $y$ must yield $\\Pr(Y = y) = 0$. Therefore, with continuous distributions, often the *cumulative distribution function* is used, which is defined as $F(x) = \\Pr(Y \\leq y)$. This is why we always use the surface of areas under the *normal* distribution function. \n\nDistribution functions have characteristics of which the most important are:\n\n- The mean, also known as the expected value (or expectation) of $Y$. It is usually denoted as $E(Y) = \\mu_Y$ and can as well be interpreted as the long-run average value of $Y$ over repeated realizations of $Y$: $\\frac{1}{n}\\sum_{i = 1}^{n}y_{i}$\n- The variance, which is denoted as $E(Y - \\mu_Y)^2$. Usually it is associated with the symbol $\\sigma^2_Y$ and provides a measure of the squared spread of the distribution. If we take the square root then we have the standard deviation ($=\\sqrt{\\text{variance}} = \\sigma_Y$). For a symmetrical normal distribution, it is useful to know that the mean plus or minus 1 time the standard deviation governs about $2/3$ of all probability while the mean plus or minus 2 times the standard deviation governs about 95% of all probability associated with that random variable.\n\nNow, in statistics we are usually related in relations between random variables, and luckily most entities in real life are related. To capture that relation we need two concepts, joint distributions and covariance. If we assume that that random variables $X$ and $Z$ have a joint distribution then the covariance between $X$ and $Z$ is:\n\\begin{equation}\ncov(X,Z) = E[(X- \\mu_X)(Z- \\mu_Z)] = \\sigma_{XZ}\n\\end{equation}\n\nNote that this covariance is a measure of the *linear* association between $X$ and $Z$ and that its units are units of $X$ times units of $Z$. $cov(X,Z) > 0$ means a positive relation between $X$ and $Z$, and finally if $X$ and $Z$ are independently distributed, then $cov(X,Z) = 0$. Note that the covariance of a random variable with itself is just its variance:\n\\begin{equation}\ncov(X,X) = E[(X-\\mu_X)(X - \\mu_X)] = E[(X - \\mu_X)^2] = \\sigma^2_X\n\\end{equation}\n\nHowever, the covariance is still measured in the units of $X$ and $Z$. To correct for that, we often use the correlation coefficient, defined by:\n\\begin{equation}\ncorr(X,Z) = \\frac{cov(X,Z)}{\\sqrt{var(X)var(Z)}} = \\frac{\\sigma_{XZ}}{\\sigma{_X}\\sigma{_Z}} = r_{XZ}\n\\end{equation}\nwhere $-1 \\leq corr(X,Z) \\leq 1$, a $corr(X,Z) = 1$ means perfect positive linear association,  a $corr(X,Z) = -1$ means perfect negative linear association, and a $corr(X,Z) = 0$ denotes no linear association. \n\n```{r, echo=FALSE, fig.cap = \"The correlation coefficient and the relation between observed $x$ and $y$\", label='fig-corrcoef'}\nknitr::include_graphics(\"./figures/Sheet27.png\")\n```\n\nIt is very important to notice that a correlation coefficient measures **linear** association. So, $corr(X,Z) = 0$ does not mean that there is no relation, there is only no linear correlation. This is illustrated by @fig-corrcoef. In panel (a) there is clearly a positive relation, and panel (b) shows a negative relation, but what about panel (d)? Here, the correlation coefficient is 0, just as in panel (c), but obviously there is a clear **non-linear** relation. \n\n### Conditional distributions and conditional means\n\nAn important notion in applied statistics (and in applied econometrics) is that of the conditional distribution, that is the distribution of $Y$, given value(s) of some other random variable, $X$. For example, in our California school example, we might want to know something about the distribution of test scores, **given** that $STR < 20$. Therefore, we use the concept of conditional mean, which is defined as the mean of a conditional distribution = $E(Y\\mid X = x)$. Note here the $\\mid$ symbol---it means the expected value of $Y$ **given** that a random variable $X$ is measured with $x$. As an example: $E(Test scores \\mid STR < 20)$ which denotes the mean of test scores among districts with small class sizes. We also denote this with the *conditional* mean.\n\nNow, if we want to know the difference in means, then we can denote that with \n\\begin{equation}\n\\Delta = E(Test scores \\mid STR < 20)-  E(Test scores \\mid STR \\geq 20),\n\\end{equation}\nwhich is a very important concept in applied economics as it resembles two groups of which one received *treatment* and the other one not. Other examples of the use of conditional means: difference in wages among gender (in the possible case of a glass ceiling for females) and mortality rate differences between those who are treated and those who are. Now if $E(X \\mid Z)$ is constant, then $corr(X,Z) = 0$. We then say that $X$ and $Z$ are independent. \n\n## Sampling in frequentist statistics {#sec-sampling}\n\nSo, we mentioned above that we sample from the population which is assumed to be infinitely large. Now, how does this sampling then carry over to statistics. For that we need a statistical framework based on *random sampling*. First, choose an individual, $i$, (or district, firm, etc.) at random from the population. Now, prior to sample selection, the value of what we want to know $Y_i$ is random because the individual is **randomly** selected. Once the individual is selected and the value of $Y$ is observed, then $Y$ is just a number---not random anymore but data. And then we say it has the value $y$. Hence the notation $\\Pr(Y = y$). \n\nIf we sample multiple entities, then we can construct a data set that looks like $(y_1, y_2,\\dots, y_n)$, where $y_i$ = value of $y$ for the $i^{\\mathrm{th}}$ individual (district, entity) sampled. Again the lower case here denotes a realisation---the dataset. Now, we want to know what the distribution of the random variables $Y_1, \\ldots, Y_n$ is under simple random sampling. Note that because entities (say individuals) \\#1 and \\#2 are selected at random, the value of $Y_1$ has no information content for $Y_2$.  Thus: $Y_1$ and $Y_2$ are independently distributed. And if $Y_1$ and $Y_2$ come from the same distribution, that is, $Y_1$, $Y_2$ are identically distributed, then we say that, under simple random sampling, $Y_1$ and $Y_2$ are independently and identically distributed (*i.i.d.*). More generally, under simple random sampling, $Y_i$, $i = 1,\\ldots, n$, are *i.i.d*---this term always come back in all sorts of statistics.\n\nThis simple framework already allows rigorous statistical inferences about, e.g., *the mean* $\\bar{Y}$ of population distributions using a sample of data from that population. The next subsection does this because the mean is not only an important statistic, but because the results can be immediately transferred to the regression context as well. \n\n### The sampling distribution of $\\bar{Y}$\n\nNow because $\\bar{Y}$ is formed by a sample of $\\{Y_i\\}'s$ it is as well a random variable, and its properties are determined by the *sampling distribution* of $\\bar{Y}$. Again, we assume that the elements in the sample are drawn at random, that thus the values of $(Y_1,\\ldots, Y_n)$ are random, and that thus functions of $(Y_1,\\ldots, Y_n)$, such as $\\bar{Y}$, are random: had a different sample been drawn, they would have taken on a different value. Finally, the distribution of $\\bar{Y}$ over different possible samples of size $n$ is called the sampling distribution of $\\bar{Y}$, which underpins all of *frequentists* statistics.\n\n### Example: simple binomial random variables\n\nSo how does this work. Let's take the easiest statistical example: coin flipping, where the coin is this case is notoriously biased. Suppose the random variable $Y$ takes on 0 (head) or 1 (tails) with the following probability distribution, $\\Pr[Y = 0] = 0.22$, $\\Pr(Y =1) = 0.78$. Then the mean and variance are given by:\n\\begin{eqnarray} \n\\mu_{Y} &=& p \\times 1 + (1-  p) \\times 0 = p = 0.78 \\notag\\\\\n\\sigma^2_Y&=& E[Y - \\mu_{Y}]^2 = p(1 - p) \\notag\\\\\n&=& 0.78 \\times 0.22 = 0.17\n\\end{eqnarray}\nBut this is only one throw ($throw = 1$). We would like to have multiple observations to derive at our sampling distribution of $\\bar{Y}$, which we assume to depend on the number of throws, $n$. \n\nConsider therefore first the case of $throw = 2$. The sampling distribution of $\\bar{Y}$ is,\n\\begin{eqnarray} \n\\Pr(\\bar{Y}  = 0) &= 0.22^2 &= 0.05 \\notag \\\\\n\\Pr(\\bar{Y}  = 1/2) &=  2 \\times 0.22 \\times 0.78 &= 0.34 \\notag \\\\\n\\Pr(\\bar{Y}  = 1) &= 0.78^2 &= 0.61. \n\\end{eqnarray}\n\nbut this start to become boring as the number of throws increases. Therefore, we turn to `R`. Let's first check for $throw = 2$.\n\n```{r}\n#| fig.cap: \"Sampling distribution when you throw a coin 2 times\"\n#| label: 'fig-coin2'\nthrow <- 2\nreps <- 10000\n\n# perform random sampling\nsamples <- replicate(reps, rbinom(n = 1, size = throw, prob = 0.78) )/throw # 2 x 10000 sample matrix\n\n# Create histogram\nhist(samples, \n     col = \"steelblue\" , \n     freq = F,\n     breaks = 10,\n     yaxt = 'n',\n     ylab = '',\n     main = NULL,\n     xlab=\"Sample Averages\")\n```\n\nThe first two lines of this code sets the number of throws (`throws`) and how often I do this (`reps`). So, I throw a coin twice, for 10000 times in a row. The third line generates the outcomes, which in this case are no heads (0), head once (1), or two heads (2). To arrive at probabilities I divide by 2 again. Finally, the third line gives a histogram of the density. \n\nBut what if I do this a 100 times, so $throw = 100$?\n\n```{r}\n#| fig.cap: \"Sampling distribution when you throw a coin 100 times\"\n#| label: 'fig-coin100'\n\nthrow <- 100\nreps <- 10000\n\n# perform random sampling\nsamples <- replicate(reps, rbinom(n = 1, size = throw, prob = 0.78) )/throw # 2 x 10000 sample matrix\n\n# Create histogram\nhist(samples, \n     col = \"steelblue\" , \n     freq = F,,\n     breaks = 25,\n     yaxt = 'n',\n     ylab = '',\n     main = NULL,\n     xlab=\"Sample Averages\")\n```\n\nThe histogram can be seen now in @fig-coin100. \n\nBut isn't this strange. We can now observe a couple of things. First, the average of the distribution of Figure @fig-coin100 is very close to 0.78, which is the actual probability that our biased coin provides tails. But, more importantly the distribution starts to look like a symmetric normal distribution. And we started with a binomial distribution! \n\nThis is the result of two amazing statistical theorems:\n\n1. **The law of large numbers**: the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed. That is, if there a no biases in the experiment itself. It also means that with more experiments the precision become better, or the variance decreases. In general this implies that:\n    - $\\bar{Y}$ is an *unbiased* estimator of $\\mu_Y$ (that is, $E(\\bar{Y}) = \\mu_Y$)\n    - var($\\bar{Y}$) is *inversely proportional* to $n$\n    - The standard error associated with $\\bar{Y}$ is $\\sqrt{\\frac{\\sigma_Y^2}{n}}$ (that means that with larger samples there is less uncertainty but see the square-root law)\n2. **The Central Limit Theorem**: when independent random variables are summed up^[Taking the mean is as well a sum but then divided by a constant.], their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed. So $\\bar{Y}$ is approximately distributed $N(\\mu_Y,\\frac{\\sigma^2_Y}{n})$\n    - When working with standardized variables then $\\bar{Y} = \\frac{\\bar{Y}-\\mu_Y}{\\sigma_Y/\\sqrt{n}}$ is approximately distributed as $N(0,1)$ \\newline\n    - The larger is $n$, the better is the approximation. And this already holds for $n \\geq 50$.^[All applied econometrics assumes the number of observations to be larger than 50.] So with a reasonable amount of observations, the mean of *i.i.d.* variables is normally distributed\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":[{"text":"<script>\nwindow.MathJax = {\n  tex: {\n    tags: 'ams'\n  }\n};\n</script>\n"}],"toc-depth":4,"output-file":"appendix.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.553","bibliography":["references.bib"],"fontsize":"1.0em","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"appendix.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}