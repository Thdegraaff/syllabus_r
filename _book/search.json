[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Econometrics (online)",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 Introduction\nIn this first introductory chapter, we will lay out the relations between theory development and theory testing as they are the cornerstones of scientific progress. After all, a theory or idea can only be scientific if the theory can be tested and, if need be, refuted. If the theory cannot be tested then it is not science. We will also explain the basic workflow of scientific research and the tools needed with specific emphasis on research in the social sciences. We end with a reading guide where we discuss each chapter in this syllabus and the relations between the chapters."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Amrhein, Valentin, Sander Greenland, and Blake McShane. 2019.\n“Scientists Rise up Against Statistical Significance.”\nNature Publishing Group.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in\nHereditary Stature.” The Journal of the Anthropological\nInstitute of Great Britain and Ireland 15: 246–63.\n\n\nLevitt, Steven D, and Jack Porter. 2001. “How Dangerous Are\nDrinking Drivers?” Journal of Political Economy 109 (6):\n1198–1237.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. Chapman; Hall/CRC.\n\n\nPearl, Judea. 2009. Causality. Cambridge university press.\n\n\nPopper, Karl. 2005. The Logic of Scientific Discovery.\nRoutledge.\n\n\nSenn, Stephen. 2011. “Francis Galton and Regression to the\nMean.” Significance 8 (3): 124–26.\n\n\nStock, James H, Mark W Watson, et al. 2003. Introduction to\nEconometrics. Vol. 104. Addison Wesley Boston."
  },
  {
    "objectID": "index.html#what",
    "href": "index.html#what",
    "title": "Applied Econometrics (online)",
    "section": "What",
    "text": "What\nThis syllabus provides the course material for the course Applied Econometrics (online). With this course I would like to bridge the gap between theoretical statistics and applied econometrics. Moreover, I focus as well on putting all this in practice when performing empirical research. As such this course can as well be seen as preparation for an economic master, specifically the master Spatial, Transport and Enviroment Economics. But above all, the course aims to provide students with some tools that we see as very useful for research; not only in the socio-economic sciences but outside them as well.\nAs we only have a limited amount of time available for this course, the amount of topics we can deal with is by nature restricted. I decided to focus on the basics of applied econometrics and as such builds upon the foundations of (introductionary) statistical courses students in most programs received in the first or second bachelor year. But now we challenge the student to build more elaborate statistical models where specific attention is given to presentation and interpretation of the results."
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "Applied Econometrics (online)",
    "section": "Why",
    "text": "Why\nAlthough there are many and very good introductory textbooks on applied econometrics (for a full course we recommend Stock, Watson, et al. 2003), these textbooks are either too large, not applied enough or not focused on the spatial domain. Apart from that there are two reasons why we wanted to write our own material. First, usually less time is spent on why certain, and at first sight very restrictive, assumptions are made. I want to bridge that gap and provide the student with more intuition on where models, evidence, and finally perhaps the “truth” (if there is such a thing) comes from. Second, how to present statistical evidence and the interpretation of that evidence I think is very important but usually not given much attention."
  },
  {
    "objectID": "index.html#for-whom",
    "href": "index.html#for-whom",
    "title": "Applied Econometrics (online)",
    "section": "For Whom",
    "text": "For Whom\nThis syllabus assumes that the reader has a basic working knowledge of statistics and some calculus (typically those method courses students enjoy in their first year). The book can however be read as stand-alone, although that requires more attentive reading and especially practising. Where we think it is necessary we provide (references to) background material.\n\n\n\n\n\n\n\n\n\n\nStock, James H, Mark W Watson, et al. 2003. Introduction to Econometrics. Vol. 104. Addison Wesley Boston."
  },
  {
    "objectID": "intro.html#theory-models-and-hypotheses",
    "href": "intro.html#theory-models-and-hypotheses",
    "title": "1  Introduction",
    "section": "1.1 Theory, Models and Hypotheses",
    "text": "1.1 Theory, Models and Hypotheses\nIn 2021, Guido Imbens, Joshua Angrist and David Card received the Nobel price for economics (officially The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel). The field they work in is applied econometrics with specific focus on finding causal relations. That means that with data they want to test whether phenomenon \\(X\\) has an effect on phenomenon \\(Y\\). More, in detail, with a causal effect we mean that when we change \\(X\\), there will be an effect on \\(Y\\), ceteris paribus, and not the other way. So, when we change \\(Y\\), \\(X\\) will not necessarily change.\nAnd identifying causal effects is what most applied econometric work nowadays is focused on. And we will focus on that as well in Chapter 2, Chapter 3 and Chapter 4. But how do you know what to test, or, in other words, where do phenomena \\(X\\) and \\(Y\\) come from? Those phenomena and possible relations originate from scientific theories as you will have in all disciplines. And those theories are typically casts in models—usually in a very abstract manner. Models come in the form of computer simulations (such as with agent-based modeling), real physical models (as with displays), but often models are formulated in mathematical notation with the aim of being as precise, lucid and clear as possible. But note that these models are not necessarily theory. Theory is the underlying set of relations and assumptions that can say something about the specific structure of models. But very often theory can lead to multiple types of models, each perhaps highlighting different aspects of the underlying theory. An example of such a theory is the Law of Diminishing Marginal Utility: each additional unit of the same good is appreciated less by consumers. This theory can be expressed in many mathematical ways but the underlying concept as displayed in Figure 1.1 always remains the same. This type of function, increasing but slower and slower, belongs to the family of concave functions. A function with exponential growth (such as \\(e^{gt}\\) belongs to the family of convex functions).\n\n\n\n\n\nFigure 1.1: Law of Diminishing Marginal Utility\n\n\n\n\nSo how to relate this with each other in scientific research? Well, when doing research you are interested in something that is not yet known (the research gap). Your aim is to (partly) fill this research gap by answering a research question. To answer this research question you need theory (a theoretical framework); what do you need to assume, what are the most important (moderator) variables, how do they relate with each other, and so on and so forth. From this theory you construct a model. Not necessarily a mathematical one. For example, you can also make a model in a Geographical Information System environment where you visualize layers of information that you think are most relevant based upon theory (in this case often previous scientific literature). Or you make a simulation model examining risks of flooding by rivers. The final step is the stage where your model should provide you with some answers. Sometimes they are concerned with optimality (what is the best location of a new road in a GIS environment), prediction (where are river dikes most vulnerable), or with establishing a (causal) relation. And it is the latter that this course deals with. How can we know that there is a relation between phenomenon \\(X\\) and phenomenon \\(Y\\) and how do we know whether that relation is causal?\nFor that we use applied econometrics (which is a form of applied statistics but then in the social-economic sciences domain—the exact difference will be discussed in Chapter 2). And to establish a, hopefully causal, relation, we test our models with empirical data. Be aware, though, that the applied econometrics materials we teach in this course (and in all introductory courses of applied statistics and econometrics all over the world—the “101” courses) is based on so-called frequentist statistics (we will revisit its basic assumptions in Chapter 2). The exact definition is not important for now but know that it is intrinsically related with hypothesis testing.\nAnd hypothesis testing is most often associated with that scientific philosopher—and perhaps the only one you know—Karl Popper (as displayed in Figure 1.2).\n\n\n\n\n\nFigure 1.2: Karl Popper\n\n\n\n\nPopper was a so-called empiricist and claimed that theories in the empirical sciences (that includes most of the social sciences) can never be proven, only rejected. That is why you can reject a null-hypothesis (\\(H_0\\)), but never accept the alternative hypothesis (\\(H_a\\)). And this is intrinsically highly related with the theoretical framework behind frequentist statistics. Loosely speaking, in frequentist statistics you construct a world where \\(H_0\\) is true and you try to reject that world with data (we will come back to this in Chapter 2)—but that world does not say anything about the validity of the alternative hypothesis. Now, Popper never claimed that rejecting one null-hypothesis will reject a whole theory. For that you need a larger body of evidence, including results from all sorts of studies—not only statistical ones, leading to a general consensus amongst the whole scientific community (Popper 2005).\nIn truth, although the scientific approach of working with null-hypotheses is a very valuable one (and remarkably practical), it does not always lead to definitive answers. That is because contradicting models can lead to similar null-hypotheses. Moreover, often the connection between research question and null-hypothesis is not a direct one. Consider that you want to know the effect of \\(X\\) on \\(Y\\), so your research question is: “What is the effect of \\(X\\) on \\(Y\\)?”. But, in a frequentist world you only reject hypotheses—thus leading to results in the line of: “The effect of \\(X\\) on \\(Y\\) is not \\(\\ldots\\)”. This makes the evidence for your research question at least circumstantial and in the best case indirect. The bottom-line here is that one needs to be careful in drawing conclusions based on null-hypotheses (and in a broader sense based on models in general). Scientific research typically advances very slowly—but hopefully in a robust and parsimonious way!"
  },
  {
    "objectID": "intro.html#doing-research-in-the-social-sciences",
    "href": "intro.html#doing-research-in-the-social-sciences",
    "title": "1  Introduction",
    "section": "1.2 Doing Research (in the Social Sciences)",
    "text": "1.2 Doing Research (in the Social Sciences)\nIt is remarkable that, although in principle students are (should be) prepared for scientific research, they receive little guidance in how they should do scientific research. What are the tips & tricks of the trade and what—and, more importantly why—should you use with respect to specific (types of) applications and what is the relation between them. In our view most of this should belong in your first course upon entering the university (with the appropriate course title “Research Methods 101”). And some of it you indeed have learned in your first year, but in our experience students still lack “operational” knowledge. Therefore, we discuss below the four elements we think are among the most important—at least for this course. There are others, but for now this will do.\n\n1.2.1 Work tidy\nOur first and most important tip is to work tidy. Try to make your work look good. And with work we mean everything you submit (such as tutorials, papers, examinations, and theses). And that is because lecturers are just like people and often think from primary instincts with their reptilian brain: if it doesn’t look good, not much time is spent on arguing and thinking as well! Moreover, when your work is difficult to read, lecturers get annoyed. Making your work look good and in the same time more lucid and transparent also serves a higher purpose as it is then easier to detect mistakes. Namely, everyone makes mistakes. The important thing is to detect them early, learn from them and remedy them. This advances science in general and is a very important feature of the scientific process. Chapter 4 will spent additional time on working tidy and making it looking good.\n\n\n1.2.2 Know where your stuff is\nA second very straightforward tip is to be organised and to know where your stuff is. Often, students come to us for help with all their files piled on a stack on their desktop and facing difficulties finding where their work is. It is always advisable to use a folder structure and have one folder for one project (or for one course). And to the use sub-folders for data, text, code, pdf’s and so forth. A second tip for organisation is to think about versioning. As the well-known Figure Figure 1.3 shows the number of versions of one file very quickly can get out of hand. Think at least about a consistent naming structure (perhaps with the date involved such as paper_20221215.doc).\n\n\n\n\n\nFigure 1.3: Version confusion\n\n\n\n\n\n\n1.2.3 Make notes\nOne skill that in our opinion is given not too much attention is making useful notes. It has been proven that writing things down is beneficial; not only for remembering but also for understanding. And that seems to be best just by using a pen as this slows writing down and you have to think about what to write down. Underlining or marking is useful less beneficial than writing accompanying notes. But when should you write notes? Well, when attending lectures of course but also when reading. To leverage your notes as much as possible it is important that you have a system where you can retrieve your notes and compare them with other notes. The latter is the hardest part, but is in the long run the most rewarding as new connections are created between lectures, courses, books, and years. You do not need any fancy tools for this (there is literally a ton of applications to be found on internet), Microsoft’s Onenote or Evernote are more than good enough. Where the workflow typically is to first use pen and paper to capture notes and thereafter rewrite and organise your notes in a notes system.\n\n\n1.2.4 Use a reference manager!\nPerhaps the tool that has the quickest pay-off is a reference manager. For those of you who are not using one yet: do it. Why? Because you never have to think about your reference list again. All reference managers come with plugins for Word or other text-editors (or type-setters such as \\(\\LaTeX\\)) that enable you to automatically generate reference list based upon in-text citations which the reference manager can also provide. You only need less than an hour to set it up, but you very quickly become more efficient (and thus save time in future work). There are many reference managers out there, but we advise Zotero as it is open source. There is both a cloud and desktop version and it comes with a handy tutorial. It also provides a plugin for your browser to automatically import the bibliographic details of the paper you are reading at the moment."
  },
  {
    "objectID": "intro.html#statistical-software",
    "href": "intro.html#statistical-software",
    "title": "1  Introduction",
    "section": "1.3 Statistical software",
    "text": "1.3 Statistical software\nAs quantitative research becomes more and more important in the social sciences you need software to manage your data and provides statistical and applied econometric analyses. Ideally, an open-source package is used (such as R or Python), but they have a steep-learning curve and do not work immediately out-of-the-box. The statistical software we use in this course is STATA and is more intuitive (compared to R or Python that is!) and, above all, all economists use it. So, the user base is large and that is important, because for each problem there is much material to be found on internet (including videos). Be aware though that there is one disadvantage in using STATA and that is that it is not open-source."
  },
  {
    "objectID": "intro.html#reading-guide",
    "href": "intro.html#reading-guide",
    "title": "1  Introduction",
    "section": "1.4 Reading Guide",
    "text": "1.4 Reading Guide\nThis course will not concern itself with theory as such, but more with how to test that theory (the applied econometrics). Chapter 2 introduces the basic concepts of applied econometrics in the form of univariate regression. Chapter 3 extends this framework to a multivariate regression setting, but in the same deals as well with the translation of theoretical (socio-economic) models to empirical models that are testable. Chapter 4 discusses how to specify your model—which variables should you include and which variables not—and how to present your findings to a wider audience (that includes assessors). The final chapter summarizes and provides a general discussion.\n\n\n\n\nPopper, Karl. 2005. The Logic of Scientific Discovery. Routledge."
  },
  {
    "objectID": "linear_regression.html",
    "href": "linear_regression.html",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "",
    "text": "3 {stata} # #| label: workingdirectory # #| eval: FALSE # cd  \"/Users/tomba/Dropbox/Thomas/Colleges/M&T_AED_2022/\" #"
  },
  {
    "objectID": "linear_regression.html#introduction",
    "href": "linear_regression.html#introduction",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nWhy should we have something as applied econometrics in the social sciences? That is because we have theories and those theories contain variables such as in the direct utility function of model (\\(\\ref{directutility}\\)):\n\\[\\begin{equation}\nU(x_1, x2) = x_1^\\alpha \\cdot x_2^\\beta\n\\label{directutility}\n\\end{equation}\\]\nHere, the quantities for the goods \\(x_1\\) and \\(x_2\\) are considered to be known—also often referred to as data. In theoretical work they are fictional or sometimes simulated. The parameters \\(\\alpha\\) and \\(\\beta\\) are not known and we often want to estimate them. A certain amount of consumption of both goods lead then some level of utility which rational consumers want to maximise.\nThis course is about using data to quantify (socio-economic) parameters. Moreover, we focus on measuring causal effects, instead of mere correlations. Note, that in an ideal world, we would like conducting experiments as to measure a causal relation of a phenomenon \\(X\\) on \\(Y\\). However, we almost always only have observational data on, for example, demand and prices. Therefore, the second part of this course and syllabus deals with difficulties arising from using observational data to estimate these causal effects and to rewrite models as (\\(\\ref{directutility}\\)) such that we can actually use data to tease out values for—in this case—\\(\\alpha\\) and \\(\\beta\\).\nThis chapter is organised as follows. The next section addresses the problem of finding a relation between some \\(X\\) and some \\(Y\\). Here, we follow an example from the well-known textbook of Stock, Watson, et al. (2003) where we look at the relation between school class size and school class performance. At the same time, we also introduce some STATA commands. To do so, this section deals as well with the statistical framework that is needed for applied econometrics. Note that we assume that the reader already had a course in introductionary statistics and that we provide only the basic concepts most important for this course in Appendix @ref(appreviewstat)."
  },
  {
    "objectID": "linear_regression.html#secproblem",
    "href": "linear_regression.html#secproblem",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "2.2 So, what is the problem?",
    "text": "2.2 So, what is the problem?\nAs explained in the introduction above, applied econometrics aims to give the policy maker well-informed, and evidence based, values for variables she needs. She needs these variables basically for two separate things:\n\nCausal inference: The policy maker wants to assess the effect of a change in one variable (typically called \\(X\\)) on another variables (often called \\(Y\\)).\nPrediction: If you know what variable \\(X\\) is, what should \\(Y\\) then be?\n\nNowadays, most applied econometric techniques are concerned with causal inference, not so much with prediction. Even more, the techniques often applied are beneficial for correct causal inference, but might harm prediction. However, see that without correct causal inference (so knowing the true causal effect) prediction is always cumbersome. That is why current methods such as machine learning methods first focus on finding the correct causal mechanism (even without sometimes specifying what they may be) and then optimize prediction.\nThus, finding (causal) mechanism helps the scientist or policy maker in assessing the outcomes of a particular (policy) intervention. In the economic realm one could think about trying to assess the following quantities:\n\nTo what extent do people eat less meat if we increase the prices with 1% (using a meat-tax)?\nIf we increase Dutch dikes with one meter, how much less flood risk will there be?\nHow much do classes perform better is we reduce class-sizes with one student?\n\n\n2.2.1 A first encounter with STATA\nFor this section, we will focus on the last question. And this is an important question for policy as teachers are costly, but parents value school performance very highly. To start answering this question we use data. In STATA data is in a specific format, named .dta. Note that this format is not in a text format and cannot be read with the human eye. To start using STATA we first need to set the working directory to the appropriate directory1. You can do this by using the dropdown menu File in STATA and click on Change working directly but you can also do this with the following command:"
  },
  {
    "objectID": "linear_regression.html#sec-problem",
    "href": "linear_regression.html#sec-problem",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "2.2 So, what is the problem?",
    "text": "2.2 So, what is the problem?\nAs explained in the introduction above, applied econometrics aims to give the policy maker well-informed, and evidence based, values for variables she needs. She needs these variables basically for two separate things:\n\nCausal inference: The policy maker wants to assess the effect of a change in one variable (typically called \\(X\\)) on another variables (often called \\(Y\\)).\nPrediction: If you know what variable \\(X\\) is, what should \\(Y\\) then be?\n\nNowadays, most applied econometric techniques are concerned with causal inference, not so much with prediction. Even more, the techniques often applied are beneficial for correct causal inference, but might harm prediction. However, see that without correct causal inference (so knowing the true causal effect) prediction is always cumbersome. That is why current methods such as machine learning methods first focus on finding the correct causal mechanism (even without sometimes specifying what they may be) and then optimize prediction.\nThus, finding (causal) mechanism helps the scientist or policy maker in assessing the outcomes of a particular (policy) intervention. In the economic realm one could think about trying to assess the following quantities:\n\nTo what extent do people eat less meat if we increase the prices with 1% (using a meat-tax)?\nIf we increase Dutch dikes with one meter, how much less flood risk will there be?\nHow much do classes perform better is we reduce class-sizes with one student?\n\n\n2.2.1 A first encounter with STATA\nFor this section, we will focus on the last question. And this is an important question for policy as teachers are costly, but parents value school performance very highly. To start answering this question we use data. In STATA data is in a specific format, named .dta. Note that this format is not in a text format and cannot be read with the human eye. To start using STATA we first need to set the working directory to the appropriate directory.1 You can do this by using the drop down menu File in STATA and click on Change working directly but you can also do this with the following command:\n\n\n\n\n\n\n\ncd  \"/Users/tomba/Dropbox/Thomas/project/preparatory/\"\n\n/Users/tomba/Dropbox/Thomas/project/preparatory\n\n\nNote here that this file path is used on an Apple or Linux system. On a Windows system you need forward slashes. It is also good to have subdirectories in your project/course folder for e.g. slides, data and tutorials. Now suppose you have the subdirectory data in your course folder2 and in that data directory you have a file called caschool.dta.3 Now, this dataset describes 420 school districts in California and, amongst other things, their average performance (measured by a testscore) and their financial constraints (measured by the amount of students per teacher).\nTo import the data in STATA you make use of the use command (again, you can make use of the dropdown menus), as follows:\n\nuse \"./data/caschool.dta\", clear\n\nNote that an option is added using the syntax , clear. The comma indicates that an option is expected and the verb clear indicates that memory of STATA should be cleared from data. That is because, STATA can only have one dataset in its memory.\nIn addition, we would like to know how the dataset looks like, for example what kind of variables it contains. We do this by invoking the command describe:\n\ndescribe\n\nContains data from ./data/caschool.dta\n Observations:           420                  \n    Variables:            18                  20 F\n> eb 2017 13:10\n--------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Vari\n> able label\n--------------------------------------------------\nobservat        float   %9.0g                 \ndist_cod        float   %9.0g                 \ncounty          str18   %18s                  \ndistrict        str53   %53s                  \ngr_span         str8    %8s                   \nenrl_tot        float   %9.0g                 \nteachers        float   %9.0g                 \ncalw_pct        float   %9.0g                 \nmeal_pct        float   %9.0g                 \ncomputer        float   %9.0g                 \ntestscr         float   %9.0g                 \ncomp_stu        float   %9.0g                 \nexpn_stu        float   %9.0g                 \nstr             float   %9.0g                 \navginc          float   %9.0g                 \nel_pct          float   %9.0g                 \nread_scr        float   %9.0g                 \nmath_scr        float   %9.0g                 \n--------------------------------------------------\nSorted by: \n\n\nThis provides information about the amount of observations and variables and the names and types of variables. In this case variables are either a float (a real number) or a string (text). Note as well, that this kind of output is cumbersome and ugly and not fit directly for reporting. Later, we will try to make this look better in an automatic way.\nThe command summarize gives descriptive statistics. Suppose that in this case we are only interested in the variables testscr (average testscore by district) and str (the student-teacher ratio by district). Then we invoke this by:\n\nsummarize testscr str\n\n    Variable |        Obs        Mean    Std. dev.\n>        Min        Max\n-------------+------------------------------------\n> ---------------------\n     testscr |        420    654.1565    19.05335 \n>     605.55     706.75\n         str |        420    19.64043    1.891812 \n>         14       25.8\n\n\nNow we see descriptive statistics for two variables, containing number of observations, the mean, the standard deviation and the minimum and maximum of each variable. For first insight in the relation between class size and class performance we might want to draw a so-called scatterplot. These type of plots relate the values of two variables in a two-dimensional way by giving the values as coordinates. The following syntax will do so.\n\nscatter testscr str\n\nAnd this provides the following STATA output.\n\n\n\n\n\nFigure 2.1: A scatterplot with tests-cores on the \\(y\\)-axis and student-teacher ratio on the \\(x\\)-axis\n\n\n\n\nThis “cloud” of dots do not yield a clearly visible relation between class performance and class size. However, this can be deceptive. Often it is difficult to discern clear relations from raw data only. Therefore we need to resort to numerical evidence.\n\n\n2.2.2 Numerical evidence\nTo assess whether there is a relation between class performance and class size as displayed in Figure 2.1 we need numerical or statistical evidence. Before we start to engage in regression analysis, we first perform a rather simple analysis, but the underlying mechanisms are identical to that of regression analysis. We first create two groups: namely, districts with “small” (number of students per teacher is below 20 or STR \\(<\\) 20) and “large” (number of students per teacher is equal or above 20 or STR \\(\\geq\\) 20) class sizes. Then we can adopt three relatively straightforward strategies here:\n\nEstimation\n\nHere, we compare the average test scores in districts with low student-teacher ratios to those with high student-teacher ratios. So, we basically try to assess whether average behaviour is different.\n\nHypothesis testing\n\nNow, we aim to test the “null” hypothesis that the mean test scores in the two types of districts are the same, against the “alternative” hypothesis that they differ.\n\nConfidence intervals\n\nThis strategy estimates an interval for the difference in the mean test scores, so small versus large student-teacher ratio districts.\n\n\nIn STATA we can make a start with this data analysis by executing the following two commands:\n\ngenerate large = (str >= 20)\ntabulate large, summarize(testscr) mean standard obs\n\n            |         Summary of testscr\n      large |        Mean   Std. dev.         Obs\n------------+------------------------------------\n          0 |   657.35126   19.358012         238\n          1 |   649.97885   17.853364         182\n------------+------------------------------------\n      Total |   654.15655   19.053348         420\n\n\nThe first command generates a new variable called “large” and denotes an indicator being 0 if STR \\(< 20\\) and 1 if STR \\(\\geq 20\\). The second command summarizes the testscore variable again, but now only gives the mean, standard deviation and the number of observation and does this by each value of the new variable large. Of course, this output is rather ugly and it is better to make a nice table such as Table 2.1).\n\n\n\n\nTable 2.1:  Descriptive statistics of small and large classes \n \n  \n    Class size \n    Average score \n    Standard deviation \n    Observations \n  \n \n\n  \n    Small \n    657.4 \n    19.4 \n    238 \n  \n  \n    Large \n    650.0 \n    17.9 \n    182 \n  \n\n\n\n\n\n\nNow, for all three strategies (estimation, testing, confidence intervals) we want to know something about the difference—usually denoted as \\(\\Delta\\). Or, specifically:\n\nFor estimation: determine the \\(\\Delta\\) or the difference between group means\nFor hypothesis testing: can we reject the null-hypothesis that the difference is zero, or\\(\\Delta = 0\\)\nFor confidence intervals: can we construct a confidence interval for \\(\\Delta\\)\n\n\n2.2.2.1 Estimation\nIn his case the concept of estimation (that is to determine the difference between the two groups’ average scores) is rather straightforward as we need to calculate the difference between the mean test scores within each group, or:\n\\[\\begin{align}\n\\bar{Y}_{small} - \\bar{Y}_{large} &= \\frac{1}{n_{small}} \\sum_{i=1}^{n_{small}}Y_i - \\frac{1}{n_{large}} \\sum_{i=1}^{n_{large}} Y_i \\notag \\\\\n&= 657.4-650.0 \\notag \\\\\n&=7.4\n\\label{eq:estimationlarge}\n\\end{align}\\]\nThis basically mean subtracting the average scores of Table 2.1 (later we see how to do this automatically in STATA). Now, the difference—\\(\\Delta\\)—equals 7.4. We then have to ask ourselves whether this is a large difference in a real-world sense? Note that test scores seem to range from 600 to 800 and do not really have a direct meaning for us. A useful trick then is to look at the standard deviation (note that if things are normally distributed, 95% of all probability mass is within the range mean, plus or minus two times the standard deviation). In this case, the difference is about \\(1/3\\) of the standard deviation. A different way of looking at this is looking at the percentiles of test scores. In STATA this looks like:\n\ntabstat testscr, statistics(p10 p25 p50 p75 p90)\n\n    Variable |       p10       p25       p50\n-------------+------------------------------\n     testscr |   630.375       640    654.45\n--------------------------------------------\n\n    Variable |       p75       p90\n-------------+--------------------\n     testscr |   666.675     679.1\n----------------------------------\n\n\nwhere the command tabstat asks for a tabulation of certain statistics and px gives the \\(x\\)-th percentile of that statistic. Now note that between the 50th and 65th percentile there is only 12 points. So given this information, the difference of \\(7.4\\) is rather sizable. But whether this difference is big enough to be important for school reform discussions, for parents, or for a school committee is a question we cannot answer with this analysis.\n\n\n2.2.2.2 Hypothesis testing\nAn alternative is to test the null-hypothesis that the difference \\(\\Delta = 0\\). For that we need a so-called difference-in-means test and compute the corresponding \\(t\\)-statistic,4 \\[\\begin{equation}\nt = \\frac{\\bar{Y}_s - \\bar{Y}_l}{\\sqrt{\\frac{s^2_s}{n_s} +\\frac{s^2_l}{n_l} }} = \\frac{\\bar{Y}_s - \\bar{Y}_l}{SE(\\bar{Y}_s - \\bar{Y}_l)}\n\\label{eq:testinglarge}\n\\end{equation}\\] where \\(SE(\\bar{Y}_s - \\bar{Y}_l)\\) is the standard error of \\((\\bar{Y}_s - \\bar{Y}_l)\\), the subscripts \\(s\\) and \\(l\\) refer to “small” and “large” STR districts, and \\(s_s^2 = \\frac{1}{n_{small}}\\sum_{i=1}^{n_s}(Y_i - \\bar{Y}_s)^2\\)\nWe can compute this difference-of-means \\(t\\)-statistic by filling this in with the numbers of Table 2.1: \\[\\begin{equation}\nt = \\frac{\\bar{Y}_s - \\bar{Y}_l}{\\sqrt{\\frac{s^2_s}{n_s} +\\frac{s^2_l}{n_l} }}  = \\frac{657.4 - 650.0}{\\sqrt{\\frac{19.4^2}{238} +\\frac{17.9^2}{182} }} = \\frac{7.4}{1.83} = 4.05\n\\end{equation}\\]\nBut then what? Well, recall that we reject a null-hypothesis when the critical value is below a certain threshold (usually 5%). In this case that is equivalent with stating that \\(|t|>1.96\\). So, we reject (at the 5% significance level) the null hypothesis that the two means are the same. We will come back to this procedure in Section @ref(sec:unitesting).\n\n\n2.2.2.3 Confidence interval\nFinally, we can construct a 95% confidence interval for the difference between the means, which is: \\[\\begin{equation}\n(\\bar{Y}_s - \\bar{Y}_l)\\pm 1.96 \\times SE(\\bar{Y}_s - \\bar{Y}_l) = 7.4 \\pm 1.96 \\times 1.84 = (3.7, 11.0)\n\\label{eq:cilarge}\n\\end{equation}\\] So what does this mean again. Well, two things. First, the 95% confidence interval for \\(\\Delta\\) doesn’t include 0 and, second, the hypothesis that \\(\\Delta = 0\\) is rejected at the 5% level. We will come back to confidence intervals as well, but for now a confidence interval can be seen as an interval of numbers that will not be rejected as null-hypothesis.\n\n\n\n2.2.3 Always be smart (and a bit lazy)\nSo, why give this rather simple procedure so much attention. That is because all “classical” statistics are centered around these three elements and statistical computer output will always give, at least, these three. And they are as well very much related with each other. Once you know two of them, you know the third one as well.\nNow, although the procedure is rather straightforward, it is also a bit cumbersome and prone to errors. Therefore, it is much easier to let STATA do it:\n\nttest testscr, by(large) unequal\n\nTwo-sample t test with unequal variances\n--------------------------------------------------\n> ----------------------------\n   Group |     Obs        Mean    Std. err.   Std.\n>  dev.                                           \n>         [95% conf. interval]\n---------+----------------------------------------\n> ----------------------------\n       0 |     238    657.3513    1.254794    19.3\n> 5801                                            \n>         654.8793                                \n>                     659.8232\n       1 |     182    649.9788    1.323379    17.8\n> 5336                                            \n>         647.3676                                \n>                     652.5901\n---------+----------------------------------------\n> ----------------------------\nCombined |     420    654.1565    .9297082    19.0\n> 5335                                            \n>         652.3291                                \n>                      655.984\n---------+----------------------------------------\n> ----------------------------\n    diff |             7.37241    1.823689        \n>         3.787296                                \n>                     10.95752\n--------------------------------------------------\n> ----------------------------\n    diff = mean(0) - mean(1)                      \n>                 t =   4.0426\nH0: diff = 0                     Satterthwaite's d\n> egrees of freedom =  403.607\n\n    Ha: diff < 0                 Ha: diff != 0    \n>              Ha: diff > 0\n Pr(T < t) = 1.0000         Pr(|T| > |t|) = 0.0001\n>           Pr(T > t) = 0.0000\n\n\nSo, in this case we want to assess the difference in test score by groups (being small and large classes), where we as well add the option unequal, which means that variance (or standard deviations) of both groups are unequal (and they are as Table 2.1 clearly shows).\nNow try to find out for yourself that this output gives you the estimation of \\(\\Delta\\) of equation \\(\\ref{eq:estimationlarge}\\), the \\(t\\)-value and corresponding test outcome of equation \\(\\ref{eq:testinglarge}\\) with the corresponding confidence intervals of equation \\(\\ref{eq:cilarge}\\)."
  },
  {
    "objectID": "linear_regression.html#sec:uniregress",
    "href": "linear_regression.html#sec:uniregress",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "2.3 Univariate regression",
    "text": "2.3 Univariate regression\nThe three strategies we adopted in Section 2.2.2 for assessing the difference between groups directly translate to the case of regression analysis. Here we also look at estimation, hypothesis testing and confidence intervals. But before that we first look at the origin of the name regression in Subsection @ref(sec:genesis)\n\n2.3.1 Genesis: regression towards the mean\nThe name regression seems to have a negative connotation, as progress is in general seen as good and regress as bad. And actually this is true, the name regression was deliberatily given as to describe a negative process: in full regression towards the mean. The concept of regression was actually coined by Sir Francis Galton together with other statistical terms, such as correlation and deviation (Senn 2011). Galton was a notorious statistician who measured everything and else, including the length of french bread and the size of human skulls.\nin 1886, Galton started to research the height of adult children with the height of their parents (Galton 1886). The original data can be seen in the scatterplot in Figure @ref(fig:galton2). What Galton expected was that the relation between the height of children and that of their parents was a one-to-one relation. On average children should receive the same height of their parents. So, in fact he expected a \\(45^{\\circ}\\) line—the red line; a line with slope equal to 1.\n\n\n\n\n\nRelation heigh fathers and height children\n\n\n\n\nHowever, he found consistently the blue line, a line with positive slope but lower than 1 (the blue line in Figure @ref(fig:galton1)). That entails that, on average tall parents get tall children but not as tall as themselves. Of course, this goes as well the other way. Short parents get short children but not as short as themselves.\n\n\n\n\n\nRelation heigh fathers and height children\n\n\n\n\nGalton coined this process regression towards the mean.5 In the end we would all converge towards the mean and all look the same. For the Victorian Sir Frances Galton and his contemporaries in an age where income classes were highly separated this was truly a horror. Especially, because his cousin was Charles Darwin who actually claimed that species diverged. Of course, in Galton reasoning there is a mistake as this only models genetic influence and not accidental differences not influenced by genetics. Note that this analysis says something the average, but not about individual differences.\nThis regression towards the mean is now seen as a very important characteristics of regression models, and you can easily be fooled by it. It is now stated as:\n\n\\(\\ldots\\) a concept that refers to the fact that if one sample of a random variable is extreme, the next sampling of the same random variable is likely to be closer to its mean\n\nFor instance, suppose that everything went really well for a course and you got a 9 for an examination. That does not mean that the next time you will do equally well (you will still do, but not that well). Or, your favorite footballclub does extreme well in a particular year (Leicester City FC comes to mind who became premier league champion in 2016). That does not mean that the next year it will do equally well, and so forth and so on.\n\n\n2.3.2 Regression with one regressor\nSo, linear regression allows us to estimate, and make inferences about, population slope coefficients. Inference means drawing conclusions and population refers to the fact that we do not want to say something about our sample, but instead about our whole population. Ultimately our aim is to estimate the causal effect on \\(Y\\) of a unit change in \\(X\\)—but for now, just think of the problem of fitting a straight line to data on two variables, \\(Y\\) and \\(X\\).\nSimilar to Subsection @ref(sec:numevidence) we have three strategies to make inferences:\n\nWe estimation the relation:\n\nThis now boils down to the question how we should draw a line through the data to estimate the (population) slope using Ordinary Least Squares (OLS—a specific an most common type of regression analysis)\nAnd then we have to assess the advantages and disadvantages of OLS\n\nWe could refer to hypothesis testing:\n\nVery often this comes down to testing where the slope is zero. Namely, it the slope is zero, then the data does not show a relation between \\(Y\\) and \\(X\\).\n\nUsing confidence intervals:\n\nThis is related to constructing a confidence interval for the slope\n\n\nBefore we look into this we first need some clarification on notation. As mentioned above, we would like to know the population regression line:\n\\[\\begin{equation}\ntestscr = \\beta_0 + \\beta_1 STR,\n\\end{equation}\\] where \\[\\begin{eqnarray}\n    \\beta_1& =& \\text{slope of population regression line} \\\\\n    &=&   \\frac{\\Delta Testscore}{\\Delta STR}\\\\\n    &=& \\text{change in test score for a unit change in STR}\n\\end{eqnarray}\\] Note the definition here of \\(\\beta_1\\). It gives the marginal effect of a change in \\(STR\\) on \\(testscr\\). So the interpretation of the parameter \\(\\beta_1\\) is very straightforward. However, we do not know the population value of \\(\\beta_1\\) and we therefore have to estimate it using data.\nIn general, the population linear regression model is different as we add element \\(u_1\\). \\[\\begin{equation}\n    Y_i = \\beta_0 + \\beta_1 X_i + u_i, \\qquad i\\ldots n\n    (\\#eq:ols)\n\\end{equation}\\] Now, \\(X\\) denotes the independent variable or regressor, \\(Y\\) the dependent variable, \\(\\beta_0\\) the intercept, \\(\\beta_1\\) the slope, and \\(u_i\\) the regression error. The regression error consists of omitted factors, or possibly measurement error in the measurement of \\(Y\\). In general, these omitted factors are other factors that influence \\(Y\\), other than the variable \\(X\\).\n\n2.3.2.1 Estimating with OLS\nTo estimate the population linear regression model we apply the ordinary least squares estimator. Again, as Figure @ref(fig:unire) shows as well, a linear regression line is a straight line through points in a scatterplot. Actually, we want to draw that line such that the distance of all points to that line is minimized. See that in Figure @ref(fig:unire) the distances between the points and the line are given by the \\(_i\\)’s, the regression errors. So, if we somehow can minimize all \\(u_i\\)’s we are fine. But those distances could be both positive and negative and they might cancel each other out. Therefore, we first square the regression errors and then minimize (hence the name: ordinary least squares). Also, see from Eq. @ref(eq:ols) that:\n\\[\\begin{equation}\nu_i = Y_i - (\\beta_0 + \\beta_1 X_i) \\longleftrightarrow (u_i)^2 = \\left[Y_i - (\\beta_0 + \\beta_1 X_i) \\right]^2\n\\end{equation}\\]\n\n\n\n\n\nDrawing a straight line through data in a scatterplot\n\n\n\n\nBut how can we estimate \\(\\beta_0\\) and \\(\\beta_1\\) from data? For that we will focus on the least squares (ordinary least squares or OLS) estimator of the unknown parameters \\(\\beta_0\\) and \\(\\beta_1\\), which solves, \\[\\begin{equation}\n    \\min_{b_0,b_1} \\sum^n_{i=1} \\left[Y_i - (b_0 + b_1 X_i) \\right]^2\n\\end{equation}\\]\nIn fact, the OLS estimators of the slope \\(\\beta\\)\\(_1\\) and the intercept \\(\\beta\\)\\(_0\\) are:6\n\\[\\begin{eqnarray}\n    \\hat{\\beta}_1 &=& \\frac{\\sum^n_{i=1}(X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sum^n_{i=1}(X_i - \\overline{X})^2} = \\frac{s_{XY}}{s^2_X}\\\\\n    \\hat{\\beta}_0 &=& \\overline{Y} - \\hat{\\beta}_1\\overline{X}\n\\end{eqnarray}\\]\nAlthough you do need to learn these formula’s by heart some insightful comments can be given. First, if a parameter is estimated then it gets a on top. Second, the optimal \\(\\hat{\\beta}_1\\) is equal to \\(\\frac{s_{XY}}{s^2_X}\\) and this is the sampling covariance between \\(X\\) and \\(Y\\) divided by the sampling variance of \\(X\\). This is not a correlation as the units still depend on \\(X\\) and \\(Y\\) and therefore the slope can be large than \\(1\\) or smaller than \\(-1\\), but it does say something about the relation between \\(X\\) and \\(Y\\). Third, the constant is governed by the estimated parameter \\(\\hat{\\beta}_1\\)\nFrom here we can predict the values \\(\\hat{Y}_i\\) and residuals \\(\\hat{u}_i\\) as they are: \\[\\begin{eqnarray}\n    \\hat{Y}_i &=& \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i, \\qquad i = 1, \\ldots, n\\\\\n    \\hat{u}_i &=& Y_i - \\hat{Y}_i, \\qquad i = 1, \\ldots, n\n\\end{eqnarray}\\]\nWhen we apply this to our data cloud in Figure @ref(fig:scattercaschool) then we get the following optimal population regression line:\n\n\n\n\n\nScatterplot and estimated regression line\n\n\n\n\nwhere the estimated slope equals \\(\\hat{\\beta}_1 = -2.28\\), the estimated intercept equals \\(\\hat{\\beta}_0 = 698.9\\) and the total population regression line can be written as: \\(\\widehat{TestScore} = 698.9 - 2.28 \\times STR\\). So, how to interpret the estimated slope and intercept now? First, the slope entails that districts with one more student per teacher on average have test scores that are 2.28 points lower (that is, \\(\\frac{\\Delta TestScore}{\\Delta STR} =-2.28\\)). Secondly, the intercept (taken literally) means that, according to this estimated line, districts with zero students per teacher would have a (predicted) test score of 698.9. Now, this does not make any sense—it actually extrapolates the line outside the range of the data. In this case we can say that the intercept is not economically meaningful.\nNow, how to fill in predictions? One of the districts in the data set is Antelope (CA) for which \\(STR = 19.33\\) and \\(TestScore = 657.8\\) Then the predicted value for the testscore is \\(\\hat{Y}_{Antelope}= 698.9 - 2.28 \\times 19.33 = 654.8\\) and the resulting residual is \\(\\hat{u}_{Antelope} = 657.8 - 654.8 = 3.0\\)\nIn STATA both the constant and the slope can be easily retrieved by:\n\nregress testscr str, robust\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 1, 418)         =      19.26\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.0512\n                                                Ro\n> ot MSE          =     18.581\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -2.279808   .5194892    -4.39   0.\n> 000                                             \n>        -3.300945                                \n>                    -1.258671\n       _cons |    698.933   10.36436    67.44   0.\n> 000                                             \n>         678.5602                                \n>                     719.3057\n--------------------------------------------------\n> ----------------------------\n\n\nWe will discuss the rest of this output later.\n\n\n2.3.2.2 Hypothesis testing\nWe can assess the importance of the line as well with hypothesis testing. Again, recall that in in applied econometrics we will only reject the null-hypothesis, we do not accept an hypothesis based upon one statistical test only. So, we aim to test \\(H_0: E(Y) = \\mu_{Y,0}\\) vs. \\(H_1: E(Y) \\neq \\mu_{Y,0}\\), where \\(\\mu_{Y,0}\\) is some specified quantity that we are interested in. Typically \\(\\mu_{Y,0} = 0\\) as this denotes no relation, but sometimes you could be interested in, e.g., whether \\(\\mu_{Y,0} = 1\\) when testing elasticities. Or you could be interested in other quantities.\nTesting statistical hypotheses is often very confusing, because of two things. First, you actually test whether the data you have corresponds with the null-hypothesis. Or, in other words:\n\nWhat is the probability that your data (\\(D\\)) might be right given the null-hypothesis (\\(H_0\\)): \\(\\Pr(D|H_0)\\)\n\nAnd that is a strange concept. You first imagine a world \\(H_0\\) with the data that it should provide and then test that imaginary world.\nSecondly, there is the notation that often works confusing. First, we have the \\(p\\)-value which equals the probability of drawing a statistic (e.g., \\(\\bar{Y}\\)) at least as adverse to the null (that is: your imaginary world) as the value actually computed with your data, assuming again that the null-hypothesis is true—again, you imaginary world. Secondly, there is the significance level of a test which is a pre-specified probability of incorrectly rejecting the null, when the null is actually true.\nNow, suppose that you want to calculate the \\(p\\)-value based on an estimated coefficient \\(\\hat{\\beta}_1\\), then you construct the following test: \\[\\begin{equation}\np\\text{-value} = \\Pr_{H_0}[|\\hat{\\beta}_1 - \\beta_{1,0}| > |\\hat{\\beta}_1^{act} - \\beta_{1,0}|\n\\end{equation}\\] where \\(\\hat{\\beta}_1^{act}\\) is the value of \\(\\hat{\\beta}_1\\) actually observed, and \\(\\beta_{1,0}\\) is the value of \\(\\beta_1\\) under the null-hypothesis (e.g., \\(\\beta_{1,0} = 0\\)). Now, this is confusing, but in words it states that if you belief the null-hypothesis (the estimated value should be then \\(\\hat{\\beta_1}\\)), (\\(\\hat{\\beta}_1^{act}\\)) or even more extreme values can be estimated.\nTo test the null hypothesis \\(H_0\\) we follow three steps. First, we need to compute the standard error of \\(\\hat{\\beta_1}\\), which is an estimator of \\(\\sigma_{\\hat{\\beta_1}}\\). Using an ordinary least squares estimator, standard errors for coefficients are given by:\n\\[\\begin{equation}\n\\sigma_{\\hat{\\beta_1}} = \\sqrt{\\frac{1}{n} \\frac{\\frac{1}{n-2} \\sum_{i=1}^n (X_i - \\bar{X})^2 u_i^2}{\\left[\\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\right]^2}},\n(\\#eq:olsse)\n\\end{equation}\\] which is a rather daunting expression.\nSecond, we need to compute the \\(t\\)-statistic: \\[\\begin{equation}\nt = \\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{SE(\\hat{\\beta}_1)}\n(\\#eq:olst)\n\\end{equation}\\]\nFinally, we need to calculate the \\(p\\)-value. To do so, we need to know the sampling distribution of \\(\\hat{\\beta}_1\\), which we know is complicated if \\(n\\) is small, but typically you have enough observations to invoke the Central Limit Theorem. So, if \\(n\\) is large, you can use the normal approximation (CLT) as follows \\[\\begin{eqnarray}\n                        p\\text{-value}& = &   \\Pr_{H_0}\\left[\\left|\\hat{\\beta}_1 - \\beta_{1,0}\\right| > \\left|\\hat{\\beta}_1^{act} - \\beta_{1,0}\\right|\\right]\\\\\n            & = &   \\Pr_{H_0}\\left[\\left|\\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{SE(\\hat{\\beta}_1)}\\right| > \\left|\\frac{\\hat{\\beta}_1 ^{act} - \\beta_{1,0}}{SE(\\hat{\\beta}_1)}\\right|\\right]\\\\\n            & = &   \\Pr_{H_0}[|t| > |t^{act}|]\\\\\n                        &\\simeq& \\text{probability under left + right } N(0,1) \\text{ tails}\n\\end{eqnarray}\\] where \\(SE(\\hat{\\beta}_1)\\) again equals the standard error of \\(\\hat{\\beta}_1\\), denoted with \\(\\sigma_{\\hat{\\beta}_1}\\)\nSo, if you know \\(\\hat{\\beta}_1\\) and \\(\\sigma_{\\hat{\\beta}_1}\\) you can calculate this. However, computers are much faster, in doing to. For example, suppose we want to test whether \\(\\beta_{1,0} = 0\\) using the regression output displayed above which gives \\(\\hat{\\beta}_1 = -2.28\\) and \\(\\sigma_{\\hat{\\beta}_1} = 0.52\\). That is step 1. Note that STATA already calculated the standard error of Eq. @ref(eq:olsse). For the next step we need to compute the \\(t\\)-statistic, which is:\n\\[\\begin{equation}\nt = \\frac{2.28 - \\beta_{1,0}}{0.52} = \\frac{2.28 - 0}{0.52} = -4.39.\n(\\#eq:olstemp)\n\\end{equation}\\]\nthen the \\(p\\)-value can be seen from Figure @ref(fig:pvalues):\n\n\n\n\n\nCalculating the \\(p\\)-value of a two-sided test when \\(t^{act} = -4.38\\)\n\n\n\n\nThat is, for large \\(n\\) (and typically we have that), the \\(p\\)-value is the probability that a \\(N(0,1)\\) random variable falls outside \\(|\\hat{\\beta}_1^{act} - \\beta_{1,0})/\\sigma_{\\hat{\\beta}_1} | = |t|\\). That is the blue areas under the normal distribution and they entail a probability mass. Now, if both surfaces on the sides are not larger than 2.5%, then we can reject the null-hypothesis against a 5% significance level. Now, the computer output above gives a \\(p\\)-value of \\(0.000\\), which a bit strange. The \\(p\\)-value is actually not zero, but a very small number and definitely smaller than \\(0.05\\), so we can reject the null-hypothesis being \\(\\beta_{1,0} = 0\\) at a 5% significance level (and at a 1% and 0.1% significance level as well). Now, if the \\(t\\)-statistic is exactly 1.96 in absolute value, then the \\(p\\)-value is 0.05. So, to repeat the steps, but now using computer output for testing the hypothesis that \\(\\beta_1 = \\beta_{1,0}\\)\n\nGet the standard error from computer output\nCompute the \\(t\\)-statistics as in Eq. @ref(eq:olst)\nGet the corresponding \\(p\\)-value. Or, reject the null-hypothesis at the 5% significance level if \\(|t^{act}| > 1.96\\).\n\nNow there is a link between the \\(p\\)-value and the significance level. The significance level is pre-specified. For example, if the pre-specified significance level is 5%, then you reject the null hypothesis if \\(|t| \\geq 1.96\\) or equivalently, you reject if \\(p \\leq 0.05\\). The \\(p\\)-value is sometimes called the marginal significance level. Often, it is better to communicate the \\(p\\)-value than simply whether a test rejects or not—the \\(p\\)-value contains more information than the ``yes/no’’ statement about whether the test rejects.\nBut recently there has been some debate about using \\(p\\)-values (Amrhein, Greenland, and McShane 2019). Why should you use a 5% significance level, what is so special about that number? It is not better just to report coefficients and standard errors? Figure @ref(fig:significance) shows a figure from the journal Nature and how scientists across all fields nowadays see \\(p\\)-values and statistical significance. This is not to say that testing does not matter, but more reporting. First of all, \\(p\\) values in themselves do not contain that much information. In the regression output of above you \\(p\\)-values being equal to 0.000 which is not informative. Secondly, the cut-off point of 5% is a bit harsh and could to publications being published only with \\(p\\)-values below 0.05, leading to what is called publication bias.\n\n\n\n\n\nCritical review on the (mis)use of statistical significance\n\n\n\n\n\n\n2.3.2.3 Confidence intervals\nThe exact definition of confidence intervals is a bit tricky. Namely, a 95% confidence interval for \\(\\hat{\\beta}_1\\) is an interval that contains the true value of \\(\\beta_1\\) in 95% of repeated samples. That means that confidence interval do not give a probability (even though we would like to interpret it that way). But you can state that every value within a confidence interval would not be rejected as null-hypothesis, while every value outside the confidence interval would be rejected. Now, if we know both \\(\\hat{\\beta}_1\\) and \\(\\sigma_{\\hat{\\beta}_1}\\) (again using computer output), then a 95% confidence interval can be very easily constructed. For our regression of output of above this entails\n\\[\\begin{equation}\n\\hat{\\beta}_1 \\pm 1.96 \\times \\sigma_{\\hat{\\beta}_1} = -2.28 \\pm 1.96 \\times 0.52 = [-3.30, -1.26].\n(\\#eq:olsci)\n\\end{equation}\\] So, every value between \\(-3.30\\) and \\(-1.26\\) will not be rejected as null-hypothesis, while every value outside that interval will be rejected. Note that confidence interval is again automatically given by computer output. If one would like a confidence interval against another critical level, say against a 99% critical level, one can use the level() option\n\nregress testscr str, robust level(99)\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 1, 418)         =      19.26\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.0512\n                                                Ro\n> ot MSE          =     18.581\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [99% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -2.279808   .5194892    -4.39   0.\n> 000                                             \n>        -3.624061                                \n>                     -.935556\n       _cons |    698.933   10.36436    67.44   0.\n> 000                                             \n>         672.1137                                \n>                     725.7522\n--------------------------------------------------\n> ----------------------------\n\n\n\n\n2.3.2.4 Regression with a dummy\nSometimes a regressor is binary, meaning an indicator or dichotomous (0/1) variable. Let’s go back again to Subsection @ref(sec:numevidence), where we created such a binary variable with small and large class sizes (\\(X=1\\) if class size is small, \\(X=0\\) if not). Other possible example are gender (\\(X=1\\) if female, \\(X=0\\) if male) or being treated or not (\\(X=1\\) if treated, \\(X=0\\) if not). We refer to these types of variables as being dummy variables—and they are very often used in the social sciences.\nNow, suppose we have a population regression model that looks like:\n\\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_i + u_i\n(\\#eq:olsdummy)\n\\end{equation}\\]\nWhere \\(Y\\) denotes, e.g., test scores, and where \\(X\\), e.g., denotes a dummy variable for a large class (so, if \\(STR \\geq 20\\) then \\(X_i = 1\\); otherwise \\(X_i = 0\\)), so there is then only two possibilities:\n\nFor small class size there should hold that \\(X_i = 0\\) yielding that \\(Y_i = \\beta_0 + u_i\\). Namely \\(\\beta_1 \\times X_i = \\beta_1 \\times 0 = 0\\). That means automatically that the expectation of \\(Y_i\\) is the constant, being \\(\\beta_0\\). Another way or writing is that the expectation of model @ref(eq:olsdummy) conditional on the fact that \\(X_i = 0\\) is \\(\\mathbb{E}(Y_i \\mid X_i = 0) = \\beta_0\\).\nFor large classes there should hold that \\(X_i = 1\\) yielding that \\(X_i = 1\\), \\(Y_i = \\beta_0 + \\beta_1 + u_i\\). This means the expectation of model @ref(eq:olsdummy) conditional on the fact that \\(X_i = 1\\) is \\(\\mathbb{E}(Y_i \\mid X_i = 1) = \\beta_0 + \\beta_1\\)\n\nSo a regression with a dummy as independent variable gives two different constants, for each group (small/large classes) one. You can interpret this as a level-effect (only the level changes, not the slope as there is none here). The interpretation of \\(\\beta_1\\) is in this case rather special and can be denoted as: \\[\\begin{equation}\n\\beta_1 = \\mathbb{E}(Y_i \\mid X_i = 1) - \\mathbb{E}(Y_i \\mid X_i = 0),\n\\end{equation}\\] Which is just the population difference in group means.\nIf we go back to our example with \\(X_i = 1\\) if \\(STR \\geq 20\\) and 0 otherwise then we get the following regression output\n\nregress testscr large, robust\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 1, 418)         =      16.34\n                                                Pr\n> ob > F          =     0.0001\n                                                R-\n> squared         =     0.0369\n                                                Ro\n> ot MSE          =     18.721\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n       large |   -7.37241   1.823578    -4.04   0.\n> 000                                             \n>        -10.95694                                \n>                    -3.787884\n       _cons |   657.3513   1.255147   523.72   0.\n> 000                                             \n>         654.8841                                \n>                     659.8184\n--------------------------------------------------\n> ----------------------------\n\n\nNow, note that this is the same output (\\(\\Delta= -7.4\\), \\(\\sigma_\\Delta = 1.82\\) and \\(t\\)-statistic is \\(-4.04\\)) as when we did the difference in means test in Subsection @ref(sec:smart). To conclude, this is just another way (and much easier) to do a difference-in-means analysis. And this directly carries over for the situation when we have additional regressors."
  },
  {
    "objectID": "linear_regression.html#least-squares-assumptions-for-causal-inference",
    "href": "linear_regression.html#least-squares-assumptions-for-causal-inference",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "2.4 Least squares assumptions for causal inference",
    "text": "2.4 Least squares assumptions for causal inference\nAs stated at the start of Section 2.2 applied econometrics focuses on finding a causal effect. But how do you do know that the \\(\\hat{\\beta}_1\\) you estimate using the population regression model of Eq. \\(\\ref{eq:betacausal}\\) is indeed a causal effect. In other words, if you change \\(X_i\\) with one unit, will \\(Y_i\\) then change with \\(\\beta_1\\) in reality?\n\\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_i + u_i, \\qquad i = 1 \\dots n\n\\label{eq:betacausal}\n\\end{equation}\\]\nFortunately, there is a small set of assumptions that indeed lead to such a causal interpretation. The so-called three least squares assumptions, being:\n\nThe conditional distribution of \\(u\\) given \\(X\\) has mean zero, that is, \\(E(u \\mid X = x) = 0\\).\n\nWe also refer to this assumption as the conditional mean independence assumption\nThis assumption implies that \\(\\hat{\\beta_1}\\) is truly unbiased\n\n\\((X_i,Y_i), i =1 \\ldots n\\) are i.i.d.\n\nThis is true if \\(X\\), \\(Y\\) are collected by simple random sampling\nThis delivers the sampling distribution of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)—again with a relatively large number (say \\(n > 50\\)) the sampling distribution can very well be approximated by a normal distribution\n\nLarge outliers in \\(X\\) and/or \\(Y\\) are rare.\n\nOutliers can result in meaningless values of \\(\\hat{\\beta_1}\\)\n\n\nWe will first discuss these three least squares assumptions and then give some other assumptions (but not directly necessary for the identification of causal effects) as well as you frequently encounter them\n\n2.4.1 Least squares assumption 1: conditional mean independence\nThis first assumption states that \\(E(u \\mid X = x) = 0\\) and is conceptually the most difficult one to grasp. Loosely speaking, it states that the regression error \\(u\\) is not related with the independent variable \\(X\\). They are independent of other. Another way of looking at this is displayed in Figure Figure 2.8. Here, whatever the value of student-teacher ratio is, the expectation of the outcome variable (test scores) is always centered around the population regression line. So, on average, you always predict according to your model, for each value of \\(X\\).\n\n\n\n\n\nFigure 2.8: Condition mean independence assumption\n\n\n\n\nSo, when is this assumption violated? For example, consider again the population regression model: \\(TestScore_i = \\beta_0 + \\beta_1 STR_i + u_i\\), where \\(u_i\\) denotes other factors. Now, these other factors can be everything and else. And you should ask yourself whether it is plausible that \\(E(u|X = x) = 0\\) for all these other factors?\nThis assumption lies as well at the heart of experimental settings. Namely, consider a theoretical ideal randomized controlled experiment, where:\n\n\\(X\\) is randomly assigned to people (students randomly assigned to different size classes or patients randomly assigned to medical treatments).\nBecause \\(X\\) is assigned randomly, all other individual characteristics—the things that make up \\(u\\)—are independently distributed of \\(X\\) by definition.\nThus it automatically follows that: \\(E(u \\mid X = x) = 0\\)\n\nNow, both in actual experiments, or with observational data, we will need to think hard about whether \\(E(u|X = x) = 0\\) holds. Chapters @ref(modeling) and @ref(specification) provide various examples where this assumption is violated. However, if this assumption is violated it means that you have biased inference, which boils down to the fact that your estimated \\(\\hat{\\beta_1}\\) is not the one that you want and that correct inference based upon this estimate cannot be done.\n\n\n2.4.2 Least squares assumption 2: independenty and identically distributed\nThe second least squares assumptions deals with actual sampling of your data, both the dependent (\\(Y\\)) and independent (\\(X\\)) variables. that entail that \\((X_i,Y_i), i = 1 \\dots n\\) should be i.i.d.. This assumptions arises automatically if the entity (individual, district) is sampled by simple random sampling. There are possible violations to this assumption. For example, you sample via your friends on social media (snowballing), or observations are not independent but are correlated, which arises very frequently in the context of temporal correlation or spatial correlation.\nThe consequence of violating the i.i.d. assumption is less severe then violating the conditional mean independence assumption. It leads to wrong standard errors, not to biased estimations.\n\n\n2.4.3 Least squares assumption 3: Large outliers are rare\nThe third and final least square assumption for causal inference is that large outliers are rare. Large outliers are not well defined and depend on the size of both \\(Y\\) and \\(X\\), but in general it can be seen as an extreme value of \\(X\\) or \\(Y\\). The problem is that such a large outlier can strongly influence the results and in general it can be stated that OLS can be rather sensitive to an outlier. Consider the two population regression lines in Figure 2.9. The flat one (with \\(\\hat{\\beta}_1 = 0)\\) does not take the isolated observation in the upper right corner into account. The one with the positive slope does. Now, clearly the one isolated observation in the upper right corner matters to a large extent and is an important driver for the results of the ordinary least squares estimator.\n\n\n\n\n\nFigure 2.9: Effect of outliers on OLS estimations\n\n\n\n\nHowever, this does not automatically entail that the isolated observation should be deleted. What it does entail is that one should go back to her data and investigate whether the outlier could be a mistake—perhaps a typo made when preparing the data or something that went amiss when converting the data from an Excel format to a STATA format."
  },
  {
    "objectID": "linear_regression.html#other-least-squares-assumptions",
    "href": "linear_regression.html#other-least-squares-assumptions",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "2.5 Other least squares assumptions",
    "text": "2.5 Other least squares assumptions\nOftentimes, two other least squares assumptions are frequently encountered. However, keep in mind that you do not need them for causal inference. They are the assumptions of homoskedasticity and normality.\n\n2.5.1 Homoskedasticity\nHomoskedasticity is concerned with the standard errors. Its definition is if \\(var(u \\mid X=x)\\) is constant—that is, if the variance of the conditional distribution of \\(u\\) given \\(X\\) does not depend on \\(X\\)—then \\(u\\) is said to be homoskedastic. Otherwise, \\(u\\) is heteroskedastic.\n\n\n\n\n\nFigure 2.10: Homoskedastic standard errors\n\n\n\n\nConsider Figure 2.10. Clearly the variance around the population regression line is everywhere the same, regardless the value of student-teacher ratio (\\(X\\)). Recall, that \\(E(u \\mid X=x) = 0\\) so \\(u\\) satisfies Least Squares Assumption 1. Now, in addition we also assume that the variance of \\(u\\) does not depend on \\(x\\). This is the case of homoskedasticity\n\n\n\n\n\nFigure 2.11: Heteroskedastic standard errors\n\n\n\n\nNow consider Figure 2.11. Now clearly the variance around the population regression line increases in size of student-teacher ratio (\\(X\\)). So, \\(E(u \\mid X=x) = 0\\) is still satisfied, but the variance of \\(u\\) does now depend on \\(x\\). \\(u\\) is now said to be heteroskedastic.\n\n\n\n\n\nFigure 2.12: Wages versus years of education\n\n\n\n\nVery often data in the social sciences are heteroskedastic. For example, wages are usually heteroskedastic in the amount of education consumed. Figure 2.12 shows the relation between years of education and wages, and the larger the years of education the larger hourly earnings (wages) are—as you would assume it should be. But the variance also increases in years of education. That is because you can easily predict wages when workers enjoyed very few years of schooling—usually those are just above minimum wages—but the spread becomes much wider when years of schooling go up.\n\n\n\n\n\nFigure 2.13: Heteroskedasticity in Californation schools?\n\n\n\n\nIs this now the case for our Californian school dataset. If we look again at the scatterplot between test scores and student-teacher ratios in Figure 2.13, then that is very difficult to see. But then again, does it matter whether you face heteroskedasticity or homoskedasticity.\nNote that so far we have (without saying so) assumed that \\(u\\) might be heteroskedastic Recall again the three least squares assumptions:\n\n\\(E(u \\mid X = x) = 0\\)\n\\((X_i,Y_i), i =1,\\ldots,n\\), are i.i.d.\nLarge outliers are rare\n\nThey do not say anything about homo- or heteroskedasticity and because we have not explicitly assumed homoskedastic errors, we have implicitly allowed for heteroskedasticity.\nBut what if the errors are in fact homoskedastic? Then in fact you can prove that OLS has the lowest variance among estimators that are linear in \\(Y\\). The formula for the variance of \\(\\hat{\\beta_1}\\) and the OLS standard error simplifies: If \\(var(u_i \\mid X_i=x) = \\sigma_u^2\\), then \\[\\begin{equation}\n    var(\\hat{\\beta}_1) = \\frac{\\sigma_u^2}{n\\sigma_X^2}\n    \\label{eq:olssesimple}\n\\end{equation}\\] which is much simpler than Eq. \\(\\ref{eq:olsse}\\). Again note that \\(var(\\hat{\\beta}_1)\\) is inversely proportional to \\(var(X)\\): more spread in \\(X\\) means more information about \\(\\hat{\\beta}_1\\)—we discussed this earlier but it is clearer from this formula.\nBut what does this mean for estimation. Note that STATA does not automatically apply Eq. \\(\\ref{eq:olsse}\\) for its standard errors, but uses the simpler version Eq. \\(\\ref{eq:olssesimple}\\) instead. But if we invoke the , robust option, as we already did above, STATA computes heteroskedasticity-robust standard errors. So if you do not, STATA computes homoskedasticity-only standard errors.\nThe bottom line is that the errors are either homoskedastic or heteroskedastic and if you use heteroskedastic-robust standard errors, you are fine. Namely:\n\nIf the errors are heteroskedastic and you use the homoskedasticity-only formula for standard errors, your standard errors will be wrong (the homoskedasticity-only estimator of the variance of \\(\\hat{\\beta}_1\\) is inconsistent if there is heteroskedasticity).\nThe two formulas coincide (when \\(n\\) is large) in the special case of homoskedasticity.\nSo, you should always use heteroskedasticity-robust standard errors.\n\n\n\n2.5.2 Normal distributed regression term\nFinally, in many introductionary statistic courses, normal distributed error terms are assumed, which facilitates testing with small samples. So, \\(u\\) should be distributed \\(N(0,\\sigma^2)\\). If you have a reasonable amount of observations (\\(n >50\\)), you do not need this assumptions, and especially not for causal inference."
  },
  {
    "objectID": "linear_regression.html#measures-of-fit",
    "href": "linear_regression.html#measures-of-fit",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "2.6 Measures of fit",
    "text": "2.6 Measures of fit\nA natural question that might arise is how well the population regression line fits or explains the data. For ordinary least squares estimators often two regression statistics are given that provide complementary measures of the quality of fit:\n\nThe regression \\(R^2\\): This measures the fraction of the variance of \\(Y\\) that is explained by \\(X\\); it is unitless and ranges between zero (no fit) and one (perfect fit). This one is almost always reported.\nThe standard error of the regression \\(SER\\): This measures the magnitude of a typical regression residual in the units of \\(Y\\).\n\n\n2.6.1 The regression \\(R^2\\)\nThe regression \\(R^2\\) is the fraction of the sample variance of \\(Y_i\\) “explained” by the regression. To see this, first note that \\(Y_i = \\hat{Y}_i + \\hat{u}_i\\) or the observation is equal to the OLS prediction plus the predicted residual. In this notation, the \\(R^2\\) is the ratio between the sample variance of \\(\\hat{Y}\\) and the sample variance of \\(Y\\). Here we make use of the following equity: Total sum of squares = explained “SS” + Residual “SS”—or, \\(TSS = ESS + RSS\\)—, where we can now define \\(R^2\\) as:\n\\[\\begin{equation}\nR^2= \\frac{ESS}{TSS} = \\frac{\\sum^n_{i=1}\\left(\\hat{Y}_i - \\overline{Y}\\right)^2}{\\sum^n_{i=1}\\left(Y_i - \\overline{Y}\\right)^2}.\n\\label{eq:r2}\n\\end{equation}\\] Now if \\(R^2 = 0\\) then that means \\(ESS = 0\\) and if \\(R^2 = 1\\) then that means \\(ESS = TSS\\). So, by definition yields \\(0 \\leq R^2 \\leq 1\\). There is one additional remark to make and that for an univariate regression model (so with single \\(X\\) on the right side), \\(R^2\\) equals the square of the correlation coefficient between \\(X\\) and \\(Y\\).\n\n\n2.6.2 The Standard Error of the Regression\nThe standard error of the regression is defined as:\n\\[\\begin{equation}\nSER = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n \\hat{u}_i^2}\n\\label{eq:ser}\n\\end{equation}\\]\nIn comparison with the \\(R^2\\), the \\(SER\\) is measured in the units of \\(u\\), which are actually the units of \\(Y\\). It measures the average “size” of the OLS residual (so the average ‘mistake’ made by the OLS regression line in absolute terms).\nOften the root mean squared error (\\(RMSE\\)) is used, which is very closely related to the \\(SER\\): \\[\\begin{equation}\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\hat{u}_i^2},\n\\label{eq:rmse}\n\\end{equation}\\] where \\(RMSE\\) only differs from the \\(SER\\) in the degrees of freedom.\nIf we again look at our regression output:\n\nregress testscr str, robust\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 1, 418)         =      19.26\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.0512\n                                                Ro\n> ot MSE          =     18.581\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -2.279808   .5194892    -4.39   0.\n> 000                                             \n>        -3.300945                                \n>                    -1.258671\n       _cons |    698.933   10.36436    67.44   0.\n> 000                                             \n>         678.5602                                \n>                     719.3057\n--------------------------------------------------\n> ----------------------------\n\n\nthen we see that the \\(R^2 = .05\\). So, only 5% of all variation in test scores is explained. This of course makes sense as potential many important variables are not included in the model. However, this does not automatically mean that the impact is biased and especially that the \\(STR\\) is unimportant in a policy sense. Again, we focus on causal inference, not on making a good model for prediction. The \\(RMSE= 18.6\\) indicates that the average error made is 18.6 test score units, which can be seen as sizable. In Chapter (ref-modeling?) we include other, and important, variables and what we then of course will see is that the \\(R^2\\) increases and the \\(SER\\) decreases."
  },
  {
    "objectID": "linear_regression.html#conclusion-and-discussion",
    "href": "linear_regression.html#conclusion-and-discussion",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "2.7 Conclusion and discussion",
    "text": "2.7 Conclusion and discussion\nRegression\n\n\n\n\nAmrhein, Valentin, Sander Greenland, and Blake McShane. 2019. “Scientists Rise up Against Statistical Significance.” Nature Publishing Group.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” The Journal of the Anthropological Institute of Great Britain and Ireland 15: 246–63.\n\n\nSenn, Stephen. 2011. “Francis Galton and Regression to the Mean.” Significance 8 (3): 124–26.\n\n\nStock, James H, Mark W Watson, et al. 2003. Introduction to Econometrics. Vol. 104. Addison Wesley Boston."
  },
  {
    "objectID": "multivariate.html#sec:morevar",
    "href": "multivariate.html#sec:morevar",
    "title": "3  Modeling in the Social Sciences",
    "section": "3.1 Why more independent variables?",
    "text": "3.1 Why more independent variables?\nSo, why do we include more variables? One possible answer is because it makes a better predictive model. That is, a model that is able to explain the variation in the dependent variable \\(Y\\) better.2 So, the R\\(^2\\) increases. But, as argued in Chapter @ref(univariateregression) we are not so much interested in prediction, but more in establishing a causal relation between \\(X\\) and \\(Y\\). So, if you change \\(X\\) (and only \\(X\\)) does \\(Y\\) changes and then with how much?\nAlthough economists often claim that they are the only (social-)science that focuses on causality and provides a statistical framework for that, there are other approaches to causality as well. One that is often used in other sciences is the approach of the mathematican Judea Pearl (Pearl 2009). This approach focuses on the use of Directed Acyclical Graphs (DAGs), which is a graphical visualisation of causality chains (or, what impacts what). We borrow this approach for the most simple setting as explained in Figure @ref(fig:unknown). Here, we go back to our Californian school district dataset again, where we still are interested in the effect of class size on school performance. So, we suppose that there is an effect from student teacher ratio on test scores as displayed with an directed arrow in Figure @ref(fig:unknown). We also know that the R\\(^2\\) of that regression model was rather low (5%), so by default there must be other but yet unknown factors, let us name them for now \\(U\\) (often as well referred to as unobservables), that influence test scores as well (so a directed arrow going from \\(U\\) to test scores).\n\n\n\n\n\nUnrelated omitted variables\n\n\n\n\nNow we are fine with this is as long as \\(U\\) does not impact the student teacher ratio. Then, there is still an isolated effect of student teacher ratio on class size and that is exactly what we want to measure. However, if there is a directed arrow going from \\(U\\) into \\(STR\\) as depicted by Figure @ref(fig:unobshet), then the effect of student teacher ratio is not isolated anymore. Essentially, the effect of student teacher ratio on class size is composed out of two parts:\n\nThe causal effect on student teacher ratio on class size captured by the chain \\(\\text{STR} \\longrightarrow \\text{testscore}\\). The one we are after.\nThe impact of the unknown variables on test scores. As we have not modeled them in our regression model, the effect is captured by the chain \\(U \\longrightarrow \\text{STR} \\longrightarrow \\text{testscore}\\)\n\nEconomists refer to this phenomenon as omitted variable bias, whilst in the statistical world, this is as often called confounding variables or the confounding fork (McElreath 2020) and it, unfortunately, occurs very often.\n\n\n\n\n\nRelated omitted variables\n\n\n\n\nSo, when U is a common cause for both student teacher ratio and test scores there is omitted variable bias. If we go back to our population regression model as follows: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_i + u_i,\n\\end{equation}\\] then we know that the error \\(u\\) arises because of factors that influence \\(Y\\) but are not included in the regression function; so, there are always omitted variables. But they do not always lead to bias. For omitted variable bias to occur, the omitted factor, let’s call it \\(Z\\)3, must be:\n\nA determinant of \\(Y\\) (i.e. \\(Z\\) is part of \\(u\\))\nA determinant of the regressor \\(X\\) (at least, there should hold that \\(corr(Z,X) \\neq 0\\))4\n\nThus, both conditions must hold for the omission of \\(Z\\) to result in omitted variable bias.\nNow, in our Californian district school dataset we have many more variables. One of them is variable that measures the english language ability (whether the student has English as a second language). Note that in California there are many migrants, especially from Latin-America. Now, you can readily argue that not having English as first language plausibly affects standardized test scores: so, \\(Z\\) is a determinant of \\(Y\\). Moreover, immigrant communities tend to be less affluent and thus have smaller school budgets—and, therefore, higher \\(STR\\): \\(Z\\) is most likely as well a determinant of \\(X\\).\nSo, most likely, our original estimation from Chapter @ref(univariateregression), \\(\\hat{\\beta}_1\\), is biased (so not the true causal effect). But can we say something about the direction of that bias? Yes, but the argument tends to become very quickly rather complex. In this case, note that districts with more migrant communities tend to have (i) higher class sizes and (ii) lower test scores. So, to the original estimation they added a negative effect. Thus, following this reasoning, the “true” effect must be less negative. Now, especially with negative signs this becomes rather complex, so if common sense fails you, then there is the following formula:\n\\[\\begin{equation}\n\\hat{\\beta}_1 \\overset{p}{\\to} \\beta_1 + \\frac{\\sigma_u}{\\sigma_X}\\rho_{Xu},\n\\end{equation}\\] where you should focus on the sign of the correlation between \\(X\\) and the regression residual \\(u\\) (all standard errors, \\(\\sigma\\), are always positive by default). Now, the first least squares assumption states that \\(\\rho_{Xu} = 0\\)—no correlation between the regressor and the regression residual. But now there is correlation because of omitted variable bias. And because there is a negative relation between immigrants communities and school performance, \\(\\rho_{Xu}\\) should be negative. Furthermore, because the original estimation from Chapter @ref(univariateregression) was already negative to begin with the “true” \\(\\beta_1\\) should be less negative. In conclusion, districts with more English learning students (i) do worse on standardized tests and (ii) have bigger classes (smaller budgets), so ignoring the English learning factor results in overstating the class size effect (in an absolute sense).\nYou might wonder whether this is actually going on in the Californian district school data. To see this, Figure @ref(fig:omitca) offers a cross tabulation of test scores by class size and percentage English learners.\n\n\n\n\n\nCross tabulation of test scores by class size and percentage English learners\n\n\n\n\nNow, the table depicted in Figure @ref(fig:omitca) is complex in its various dimensions. We have our two categories of class size (small and large), together with the difference in test scores, but we now stratify this by four categories of percentage English learners. There are several important observations to make here:\n\ndistricts with fewer English Learners (so less migrants) have on average higher test scores (what we assumed above);\ndistricts with fewer English Learners (so less migrants) have smaller classes (what we assumed above);\nthe effect of class size with comparable percentages English learners is still (mostly negative), but not as much as we compare for all districts together (the Difference-column). This confirms our reasoning that our original estimate was too negative.\n\nNo, as already mentioned above, omitted variable bias occurs very often. So, how to correct for this such that the bias disappaers. In general, there are three strategies:\n\nwe can run a randomized controlled experiment in which treatment (\\(STR\\)) is randomly assigned: then percentage English learners (\\(PctEL\\)) is still a determinant of test scores, but by construction \\(PctEL\\) should be uncorrelated with \\(STR\\). Unfortunately, is it very difficult to randomize class size in reality and often this strategy is just not attainable as being too costly or unethical (this accounts for all sciences);\nwe can adopt the cross tabulation approach of above, with finer gradations of \\(STR\\) and \\(PctEL\\). Then by construction, within each group all classes have the same \\(PctEL\\) so we control for \\(PctEL\\). A disadvantages is that one needs many observations, especially when one wants to stratify upon other variables as well;\nfinally, and perhaps the easiest approach, we can use a population regression model in which the omitted variable (\\(PctEL\\)) is no longer omitted. We just include \\(PctEL\\) as an additional regressor in a multiple regression model. This is what the next section deals with. Obviously, a disadvantage of this approach is that you need observations for the omitted variable (but that also accounts for method 2)."
  },
  {
    "objectID": "multivariate.html#sec:multivariate",
    "href": "multivariate.html#sec:multivariate",
    "title": "3  Modeling in the Social Sciences",
    "section": "3.2 Multivariate regression analysis",
    "text": "3.2 Multivariate regression analysis\nSo, if we have information about an important omitted variable, as in the case of the size of migrant communities in the example above, then we can use that information in a multivariate population regression model. In the case of two regressors, that would look like: \\[\\begin{equation}\nY_i =\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i, i=1,\\ldots,n\n\\end{equation}\\] where:\n\n\\(Y\\) is the dependent variable\n\\(X_1\\), \\(X_2\\) are the two independent variables (regressors)\n\\((Y_i, X_{1i}, X_{2i})\\) denote the i\\(^{\\mathrm{th}}\\) observation on \\(Y\\), \\(X_1\\), and \\(X_2\\).\n\\(\\beta_0\\) is the unknown population intercept\n\\(\\beta_1\\) is the effect on \\(Y\\) of a change in \\(X_1\\), holding \\(X_2\\) constant\n\\(\\beta_2\\) is the effect on \\(Y\\) of a change in \\(X_2\\), holding \\(X_1\\) constant\n\\(u_i\\) is the the regression error (omitted factors)\n\nNow, the only element that changes is the interpretation of a parameter, say \\(\\beta_1\\). In this case, it can still be seen as a ‘slope’ parameter, although now in 3-dimensional space, but it now states specifically that the other parameter(s) should be held constant. This does facilitate the interpretation of \\(\\beta_1\\). For example, consider changing \\(X_1\\) by \\(\\Delta X_1\\) while holding \\(X_2\\) constant. That means that the population regression line before the change looks like: \\[\\begin{equation}\nY = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_{2},\n\\end{equation}\\] whilst the population regression line, after the change, looks like: \\[\\begin{equation}\nY + \\Delta Y = \\beta_0 + \\beta_1 (X_{1} + \\Delta X_1) + \\beta_2 X_{2}\n\\end{equation}\\] And if we take the difference, then the interpretation of \\(\\beta_1\\) boils down again to the marginal effect:\\(\\Delta Y = \\beta_1 \\Delta X_1\\). Or, \\(\\beta_1 = \\frac{\\Delta Y}{\\Delta X_1}\\) when holding \\(X_2\\) constant and, likewise, \\(\\beta_2 = \\frac{\\Delta Y}{\\Delta X_2}\\) when holding \\(X_1\\) constant. \\(\\beta_0\\) is now the predicted value of \\(Y\\) when \\(X_1 = X_2 = 0\\)\nIf we do this for the the Californian school district data, then the original population regression line was estimated as: \\[\\begin{equation}\n\\widehat{TestScore} = 698.9- 2.28 STR\n\\end{equation}\\] But if we now include include percent English Learners in the district (\\(PctEL\\)) to the model then the population regression ‘line’ becomes: \\[\\begin{equation}\n\\widehat{TestScore} = 686.0- 1.10 STR - 0.65  PctEL\n\\end{equation}\\]\nClearly, the effect of student teacher ratio becomes smaller (that is, less negative). That indicates that the original regression suffers from omitted variable bias. And this is what should happen as reasoned above. The STATA syntax for a multivariate regression model is now rather straightforward. You basically add another to the regression equation, as below:\n\nreg testscr str el_pct, robust\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 2, 417)         =     223.82\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.4264\n                                                Ro\n> ot MSE          =     14.464\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -1.101296   .4328472    -2.54   0.\n> 011                                             \n>         -1.95213                                \n>                    -.2504616\n      el_pct |  -.6497768   .0310318   -20.94   0.\n> 000                                             \n>         -.710775                                \n>                    -.5887786\n       _cons |   686.0322   8.728224    78.60   0.\n> 000                                             \n>         668.8754                                \n>                      703.189\n--------------------------------------------------\n> ----------------------------\n\n\nObviously, the effect of student teacher ration reduces with 50%! The interpretation of the rest of the statistical output, such as measures of fit and test statistics, follows in the subsections below.\n\n3.2.1 Measures of fit for multiple regression\nIn multivariate regression models, there are four commonly used measures of fit, three of them we have seen before.\n\nThe standard error of regression or the \\(SER\\) denotes the standard deviation of \\(\\hat{u}_i\\) and includes a degrees of freedom correction (degrees of freedom in this case denotes how many variables your have used and typically is denoted with \\(k\\). The \\(SER\\) is defined as: \\[\\begin{equation}\nSER = s_{\\hat{u}} = \\sqrt{\\frac{1}{n-k-1} \\sum_{i=1}^n \\hat{u}^2_i},\n\\end{equation}\\] where \\(k\\) is the number of variables (including the constant) use in the regression model. Note that in the univariate regression model \\(k=2\\)—the slope coefficient and the constant.\nThe root mean square error (RMSE) which denotes as well the stdandard deviation of \\(\\hat{u}_i\\) but now without degrees of freedom. We have seen this before in Eq. @ref(eq:rmse) and does not change.\nThe \\(R^2\\) which measures the fraction of variance of \\(Y\\) explained by the independent variables. Again, we have seen this one before\nThe adjusted “adjusted \\(R^2\\)” (or \\(\\bar{R}^2\\)) which is equal to the \\(R^2\\) with a degrees-of-freedom correction that adjusts for estimation uncertainty. It can be formulated as: \\[\\begin{equation}\n\\bar{R}^2 = 1 - \\frac{n-1}{n-k-1}\\frac{SSR}{TSS}.\n\\end{equation}\\] Note that using this formulation, in a multivariate setting, it always should hold that \\(\\bar{R}^2 <R^2\\). But why do we care so much for the amount of variables that we use (denoted with \\(k\\)). That is because with each additional variable the \\(R^2\\) always increases. And it is essential to notice that when \\(k=n\\), the \\(R^2 = 1\\), so there is no variation left anymore. But that feels like cheating. You just have a parameter for each observation that you have, but such a model must be meaningless. Therefore, you always want to correct for the number of variables that you use.\n\nIn our Californian school district example that would amount to the following two outcomes. First for the univariate model: \\[\\begin{eqnarray}\nTestScore &= &698.9- 2.28  STR \\\\\n&&R^2 = .05, SER = 18.6\n\\end{eqnarray}\\]\nAnd then for the multivariate model.\n\\[\\begin{eqnarray}\nTestScore &=& 686.0 - 1.10  STR - 0.65 PctEL \\\\\n&&R^2=.426, \\bar{R}^2=0.424, SER = 14.5\n\\end{eqnarray}\\]\nNote that all measures of fit increases. The \\(\\bar{R}^2\\) now indicates that 42% of all variation in test scores are explained. That is a huge improvement compared to the 5% explanatory power of the univariate case. That indicates that the \\(PctEL\\) strongly correlates with testscores. But again, we are not so much interested in prediction, but want to find the causal impact of class size instead. Another thing to notice here is that the \\(R^2\\) and the \\(\\bar{R}^2\\) are very close. That is because the number of variables is much smaller than the number of observations \\(k \\ll n\\), so that the impact of \\(k\\) is not very big.\nA final remark concerns a peculiarity of STATA. In the regression output of above, STATA does not provide the \\(\\bar{R}^2\\). That is because of the option , robust. Without that option, the regression output would give both measures of fit.\n\nreg testscr str el_pct\n\n      Source |       SS           df       MS     \n>  Number of obs   =       420\n-------------+----------------------------------  \n>  F(2, 417)       =    155.01\n       Model |  64864.3011         2  32432.1506  \n>  Prob > F        =    0.0000\n    Residual |  87245.2925       417  209.221325  \n>  R-squared       =    0.4264\n-------------+----------------------------------  \n>  Adj R-squared   =    0.4237\n       Total |  152109.594       419  363.030056  \n>  Root MSE        =    14.464\n\n--------------------------------------------------\n> ----------------------------\n     testscr | Coefficient  Std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -1.101296   .3802783    -2.90   0.\n> 004                                             \n>        -1.848797                                \n>                    -.3537945\n      el_pct |  -.6497768   .0393425   -16.52   0.\n> 000                                             \n>        -.7271112                                \n>                    -.5724423\n       _cons |   686.0322   7.411312    92.57   0.\n> 000                                             \n>         671.4641                                \n>                     700.6004\n--------------------------------------------------\n> ----------------------------\n\n\nAnother option is to specifically ask STATA to display the \\(\\bar{R}^2\\) by invoking the command display, then some text (text always goes between strings), and finally the thing you want to see (e(r2_a)). Something like:\n\ndisplay \"adjusted R2 = \" e(r2_a)\n\nadjusted R2 = .42368043\n\n\n\n\n3.2.2 The least squares assumptions for multivariate regression\nThus, it is easy to add other variables, so that the multivariate regression model now looks like: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i}+\\ldots + \\beta_k X_{ki}+u_i, i=1,\\ldots,n\n\\end{equation}\\] Suppose we are interested in \\(\\beta_1\\). How do we then know whether our estimation \\(\\hat{\\beta}_1\\) is unbiased? For that we again resort to our least squares assumption, some of them will change a bit and we have to add a fourth one:\n\nThe first least squares assumptions changes slightly. Now, we state that the conditional distribution of \\(u\\) given all \\(X_i\\)’s has mean zero, that is, \\(E(u|X_1 = x_1,\\ldots, X_k = x_k) = 0\\). So, \\(\\beta_1\\) is biased even another variable \\(X_k\\) is correlated with \\(u\\). So, only of the variables \\(X_i\\) has to be correlated with \\(u\\) and then all parameters are to a certain extent biased.\nThe second least squares assumption is more or less as before but now in a multivariate fashion, so the whole set of (X\\(_{1i}\\),,X\\(_{ki}\\),Y\\(_i\\)), with \\(i =1,\\ldots,n\\), should be independent and identical distributed (\\(i.i.d\\)).\nThe third least squares assumptions states again that large outliers are rare for all variables included, so for all \\(X_1,\\ldots, X_k\\), and \\(Y\\).\nThe fourth assumption is new and states that there is no perfect multicollinearity. We discuss this further below.\n\n\n3.2.2.1 Multicollinearity\nMulticollinearity comes in two flavours; perfect and imperfect. The former functions as a multivariate least squares assumptions whilst the latter oftentimes gives the largest problems. We start the discussion with perfect multicollinearity and then continue with the case of imperfect multicollinearity.\n\n3.2.2.1.1 Perfect multicollinearity\nThe official definition of perfect multicollinearity is that there is a perfect linear combination amongst your variables. That means that there is not one optimal solution, but instead many (actually, infinitely many) more. Let us illustrate this by the following example. Suppose you include \\(STR\\) twice in your regression. Now, STATA produces then the following output:\n\nreg testscr str str el_pct, robust\n\nnote: str omitted because of collinearity.\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 2, 417)         =     223.82\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.4264\n                                                Ro\n> ot MSE          =     14.464\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -1.101296   .4328472    -2.54   0.\n> 011                                             \n>         -1.95213                                \n>                    -.2504616\n         str |          0  (omitted)\n      el_pct |  -.6497768   .0310318   -20.94   0.\n> 000                                             \n>         -.710775                                \n>                    -.5887786\n       _cons |   686.0322   8.728224    78.60   0.\n> 000                                             \n>         668.8754                                \n>                      703.189\n--------------------------------------------------\n> ----------------------------\n\n\nSee that STATA drops one of the \\(STR\\) variables. But why is that? See that the impact of twice this variable should be equivalent to: \\[\\begin{equation}\n\\beta_1 STR = w_1 \\beta_1 STR + w_2 \\beta_1 STR = (w_1 + w_2) \\beta_1 STR ,\n\\end{equation}\\] where \\(w_1\\) and \\(w_2\\) are weights chosen such that they satisfy the condition that \\(w_1 + w_2 = 1\\). But there is an infinite number of combinations that satisfy this condition! So, there is not an optimal solution and one of these variables should be dropped.\nThe violation of no perfect multicollearity often occurs when using dummies (see again Subsection @ref(sec:dummy)). Suppose that we regress \\(TestScore\\) on a constant, \\(D\\), and \\(B\\), where:\\(D_i =1\\) if \\(STR \\leq 20\\), \\(=0\\) otherwise ; \\(B_i =1\\) if \\(STR>20\\), \\(= 0\\) otherwise. This example is slightly more complex as there is no perfect correlation between \\(B\\) and \\(D\\). However, the model contains as well a constant and that create a perfect linear combination, namely \\(B_i + D_i = 1\\) and that is the definition of a constant (\\(\\beta_1 \\times 1\\)), so there is perfect multicollinearity in the model.\nA different way of seeing this is to consider the following regression model and note that by definition \\(D_i = 1- B_i\\):\n\\[\\begin{align}\nTestscr_i &= \\beta_0 + \\beta_1 D_i + \\beta_2 B_i + u_i\\\\\n          &= \\beta_0 + \\beta_1 D_i + \\beta_2 (1 - D_i) + u_i\\\\\n          &= (\\beta_0 + \\beta_2) + (\\beta_1 - \\beta_2) D_i + u_i.\n\\end{align}\\] Suppose that the true constant equals \\(680\\) and the slope parameter equals \\(7\\). Then it is not difficult to see that there is an infinite amount of combinations possible of values for \\(\\beta_0, \\beta_1\\) and \\(\\beta_2\\) that leads to these numbers.\nNow, this example is a special case of the so-called dummy variable trap. Suppose you have a set of multiple binary (dummy) variables, which are mutually exclusive and exhaustive—that is, there are multiple categories and every observation falls in one and only one category (e.g., infant, child, teenager, adult). If you include all these dummy variables and a constant, you will have perfect multicollinearity—the dummy variable trap.\nThere are possible solutions to the dummy variable trap:\n\nOmit one of the groups (e.g., the infants), or\nOmit the intercept\n\nIn most cases you omit one of the groups (typically the one with the lowest value). This give the constant then the interpretation of the average value of that left-out category, where the dummy variables are then the relative differences to that left-out category.\nNow, perfect multicollinearity usually reflects a mistake in the definitions of the regressors, or an oddity in the data. And, usually this is not a problem, because if you have perfect multicollinearity, your statistical software will let you know—either by crashing or giving an error message or by “dropping” one of the variables arbitrarily and very often the solution to perfect multicollinearity is to modify your list of regressors such that you no longer have perfect multicollinearity.\n\n\n3.2.2.1.2 Imperfect multicollinearity\nNow imperfect and perfect multicollinearity are quite different despite the similarity of the names. Imperfect multicollinearity, namely, occurs when two or more regressors are very highly correlated. And if two regressors are very highly correlated, then their scatterplot will pretty much look like a straight line—they are collinear—but unless the correlation is exactly \\(\\pm\\) 1, that collinearity is imperfect. What this implies is that one or more of the regression coefficients will be imprecisely estimated. Why is that? That is because of the definition of the coefficient in a multivariate regression model. Namely, the coefficient on \\(X_1\\) is the effect of \\(X_1\\) holding \\(X_2\\) constant, but if \\(X_1\\) and \\(X_2\\) are highly correlated, then there is very little variation in \\(X_1\\) once \\(X_2\\) is held constant. That means that the data are pretty much uninformative about what happens when \\(X_1\\) changes but \\(X_2\\) doesn’t, so the variance of the OLS estimator of the coefficient on \\(X_1\\) will be large. And this results in large standard errors for one or more of the OLS coefficients. But often this is very hard to detect. Are standard errors high because of imperfect multicollinearity, because the number of observations is very low, or because there is large variation in the data? The answer to this unfortunately boils down to reasoning, but before you start estimating your statistical models it always good to look at scatterplots and correlations between variables.\nBut what is a high correlation? With a reasonable amount of observations all correlations below \\(0.9\\) can be considered fine. In practice, only correlations between variables higher than say \\(0.95\\) start to impose problems.\n\n\n\n\n3.2.3 Testing with multivariate regression models\n\n3.2.3.1 Hypothesis tests and confidence intervals for a single coefficient in multiple regression\nRecall from Subsection @ref(sec:unitesting) that for hypothesis testing in a classical statistical framework we make use of the fact that \\(\\frac{\\hat{\\beta}_1- E(\\hat{\\beta}_1)}{\\sqrt{var(\\hat{\\beta}_1)}}\\) is approximately distributed as \\(N(0,1)\\) according to the Central Limit theorem. Thus hypotheses on \\(\\beta_1\\) can be tested using the usual \\(t\\)-statistic, and confidence intervals are constructed as \\(\\{\\hat{\\beta}_1 \\pm 1.96 SE (\\hat{\\beta}_1)\\}\\). And this finding carries over to the multivariate setting where for \\(\\beta_2,\\ldots, \\beta_k\\) we make use of the same framework. One thing to keep in mind is that \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) are generally not independently distributed—so neither are their \\(t\\)-statistics (more on this later).\nNow, if we return to our Californian school district data set then we find that for the univariate case holds:\n\\[\\begin{equation}\nTestScore =\\underbrace{698.9}_{10.4} - \\underbrace{2.28}_{0.52}  STR,\n\\end{equation}\\]\nAnd the population regression “line” for the multivariate case is estimated as: \\[\\begin{equation}\nTestScore = \\underbrace{686.0}_{8.7} - \\underbrace{1.10}_{0.43} STR - \\underbrace{0.650}_{0.031} PctEL\n    (\\#eq:testmulti)\n\\end{equation}\\]\nRemember, the coefficient on \\(STR\\) in Eq. @ref(eq:testmulti) is the effect on \\(TestScores\\) of a unit change in \\(STR\\), holding constant the percentage of English Learners in the district. The corresponding 95% confidence interval for coefficient on \\(STR\\) in (2) is \\(\\{-1.10 \\pm 1.96 \\times 0.43\\} = (-1.95,-0.26)\\). And the \\(t\\)-statistic testing \\(\\beta_{STR} = 0\\) is \\(t = -1.10/0.43 = -2.54\\), so we reject the null-hypothesis at the 5% significance level. More evidence for the strength of the \\(PctEL\\) variable can be seen from the fact that, under the null-hypothesis of \\(\\beta_2 = 0\\), the following must hold: \\(t\\text{-statistic} = \\frac{\\hat{\\beta_1}}{\\sigma_{\\hat{\\beta_1}}} = \\frac{0.65}{0.03} = 21.7\\), which is a very high number for a \\(t\\)-statistic.\n\n\n3.2.3.2 Tests of joint hypotheses\nSo, testing of single coefficients is just as before. Now in the Californian school district dataset there is as well a variable called \\(Expn\\) denoting the expenditures per pupil. Consider the following population regression model: \\[\\begin{equation}\nTestScore_i = \\beta0 + \\beta_1 STR_i + \\beta_2 Expn_i + \\beta_3PctEL_i + u_i\n\\end{equation}\\] The null hypothesis that “school resources don’t matter” and the alternative that they do, corresponds to:\n\n\\(H_0:\\beta_1 =0\\) and \\(\\beta_2 =0\\) vs\n\\(H_1:\\) either \\(\\beta_1 \\neq 0\\) or \\(\\beta_2 \\neq 0\\) or both\n\nThis is a joint hypothesis specifying a value for two or more coefficients. That is, it imposes a restriction on two or more coefficients. In general, a joint hypothesis will involve \\(q\\) restrictions. In the example above, \\(q = 2\\), and the two restrictions are \\(\\beta_1 = 0\\) and \\(\\beta_2 = 0\\). A “common sense” idea is to reject if either of the individual \\(t\\)-statistics exceeds 1.96 in absolute value. But this “one at a time” test isn’t valid: the resulting test rejects too often under the null hypothesis (more than 5%)! That is because the \\(t\\)-statistics themselves are often not independent. Instead, we need a \\(F\\)-statistic, which tests all parts of a joint hypothesis at once. Unfortunately, these types of formulas can become quickly rather complex. Consider the \\(F\\)-test for the special case of the joint hypothesis \\(\\beta_1 = \\beta_{1,0}\\) and \\(\\beta_2 = \\beta_{2,0}\\) in a regression with two regressors:\n\\[\\begin{equation}\nF = \\frac{1}{2} \\left(\\frac{t_1^2 + t_2^2 - 2\\hat{\\rho}_{t_1,t_2}t_1 t_2}{1-\\hat{\\rho}^2_{t_1 t_2}}  \\right)\n\\end{equation}\\]\nwhere \\(\\hat{\\rho}_{t_1,t_2}\\) estimates the correlation between \\(t_1\\) and \\(t_2\\). Reject when \\(F\\) is large (typically to be determined from large statistical tables). The F-statistic is large when \\(t_1\\) and/or \\(t_2\\) is large and the F-statistic corrects (in just the right way) for the correlation between \\(t_1\\) and \\(t_2\\). The formula for more than two \\(\\beta\\)’s is nasty unless you use matrix algebra. There is a nice large-sample (\\(n>50\\)) approximate distribution, which is the tail probability of the \\(\\chi^2_q /q\\) distribution beyond the \\(F\\)-statistic actually computed.\nNow, STATA does this in a much easier way by invoking the test command right after the regression. So, for example, we want to test the joint hypothesis that the population coefficients on \\(STR\\) and expenditures per pupil (\\(expn\\)) are both zero, against the alternative that at least one of the population coefficients is nonzero.\n\nreg testscr str expn_stu el_pct, r \ntest str expn_stu\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 3, 416)         =     147.20\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.4366\n                                                Ro\n> ot MSE          =     14.353\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -.2863992   .4820728    -0.59   0.\n> 553                                             \n>        -1.234002                                \n>                      .661203\n    expn_stu |   .0038679   .0015807     2.45   0.\n> 015                                             \n>         .0007607                                \n>                     .0069751\n      el_pct |  -.6560227   .0317844   -20.64   0.\n> 000                                             \n>        -.7185008                                \n>                    -.5935446\n       _cons |   649.5779   15.45834    42.02   0.\n> 000                                             \n>         619.1917                                \n>                     679.9641\n--------------------------------------------------\n> ----------------------------\n\n\n ( 1)  str = 0\n ( 2)  expn_stu = 0\n\n       F(  2,   416) =    5.43\n            Prob > F =    0.0047\n\n\nThe output shows an \\(F\\)-statistic with \\(q=2\\) restrictions with outcome 5.43. Do not directly interpret this number, but know that \\(\\text{Prob} > F = 0.0047\\) gives the probability that under the null-hypothesis this outcome is produced. So the joint null-hypothesis that both types of expenditures are zero (at the same time), can be rejected at a 5% (and a 1%) significance level. Other types of joint tests can easily be constructed as well. For example, when you want to know whether both coefficient add up to 1, then you would state test str + expn_stu = 1. The final point to make is the \\(F\\)-test in the regression output itself. Here, that is for example F(3, 416) = 147.20. This is a joint test that all variables, except the constant, have no impact. So, \\(\\beta_i = 0\\) for all \\(i\\) at the same time. It not often that you come across a general regression \\(F\\)-test that does not reject the null-hypothesis. It namely implies that your independent variables do not contain any information about the dependent variable.\nAnd with the \\(F\\)-test, we now have discussed all regression outcome components displayed by STATA. Most of this information you do not need for your report but we will come back later to this."
  },
  {
    "objectID": "multivariate.html#sec:nonlinear",
    "href": "multivariate.html#sec:nonlinear",
    "title": "3  Modeling in the Social Sciences",
    "section": "3.3 Non-linear specifications",
    "text": "3.3 Non-linear specifications\nThe model we are using is coined the linear regression model, and, indeed, one of the underlying assumptions is that the relations between the independent and dependent are linear. Consider the relation again between test scores and class sizes in the Californian school district data. Using the following code (note now the twoway command that ‘binds’ a scatter plot with a population regression line):\n\ngraph twoway (lfit testscr str) (scatter testscr str)\n\nWhich provides the following STATA output.\n\n\n\n\n\nA linear relation\n\n\n\n\nIndeed, there might be evidence that the relation depicted in Figure @ref(fig:scatterlfitcaschool)—if anything—is linear. But, clearly that is not the case for the relation between test scores and average district income. Namely, the syntax below:\n\ngraph twoway (lfit testscr avginc) (scatter testscr avginc)\n\nprovides the following STATA output.\n\n\n\n\n\nA non-linear relation\n\n\n\n\nFigure @ref(fig:scatterincome) shows a non-linear relation, where the effect of income tapers off (note the resemble with Figure @ref(fig:marginalutility))—or, there is a marginal decreasing effect of average district income on average school test scores. Thus, in affluent neighborhood test scores are higher, but increasingly less so. Of course, you can still try to estimate this with a linear population regression line as in Figure @ref(fig:scatterincome), but this introduces a bias. The estimate does not capture that what you want. Namely, it now holds that \\(E(u \\mid X = x) \\neq 0\\), because for small \\(X\\), say \\(X<10\\), the residuals are negative, for medium sized \\(X\\)s most residuals are positive and for large \\(X>40\\) all residuals are negative again. So, there is a clear relation between \\(X\\) and \\(u\\) and they fail to be independent. This particular form of bias is coined specification bias. There is another issue here and that is that the effect on \\(Y\\) of a change in \\(X\\) depends on the value of \\(X\\)—that is, the marginal effect of \\(X\\) is not constant.\nTo remedy the specification bias, we will use nonlinear regression population regression functions of \\(X\\), or we estimate a regression function that is nonlinear in \\(X\\). Here, it is important to see that we do so by transforming \\(X\\), so the population regression ‘line’. The estimator still remains a linear regression model.\nWe will analyse below two complementary and often adopted approaches:\n\nUsing polynomials to transform \\(X\\). That means that the effect is approximated by a quadratic, cubic, or higher-degree polynomial. This approach as well governs to an extent so-called interaction effects which is a special case, where we multiply two different variables.\nUsing logarithmic transformations of \\(X\\), where \\(Y\\) and/or \\(X\\) is transformed by taking its logarithm. Here, the main focus is on the interpretation of the \\(\\hat{\\beta}\\)s, as they change from a unit increase interpretation to a percentages interpretation which often can be found useful.\n\n\n3.3.1 Polynomials\nOur first approach to non-linear specification is applying polynomials of the variables that we suspect has a non-linear impact. If that is the independent variable \\(X\\), the we can construct the following linear regression model by using polynomials: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X^2_i + \\ldots + \\beta_r X_i^r + u_i\n(\\#eq:poly)\n\\end{equation}\\] Note again that this is just the linear regression model—except that the regressors are powers of \\(X\\)! So, in effect we transform the data—actually create new variables \\(X^r\\)—, but the specification in parameters remains linear. Estimation, hypothesis testing, etc. proceeds as in the multiple regression model using OLS. However, the coefficients are now a bit more difficult to interpret. Consider the example of above about the relation between test scores average district income, where \\(Income_i\\) is defined as the average district income in the \\(i^{\\mathrm{th}}\\) district (thousands of dollars per capita). For a quadratic specification, we specify the linear regression model as below: \\[\\begin{equation}\nTestScore_i = \\beta_0 + \\beta_1 Income_i + \\beta_2 (Income_i)^2 + u_i\n\\end{equation}\\] For a cubic specification the linear regression model becomes: \\[\\begin{equation}\nTestScore_i = \\beta_0 + \\beta_1 Income_i + \\beta_2 (Income_i)^2 +\n\\beta_3 (Income_i)^3 + u_i\n\\end{equation}\\]\nFirst, we focus on the estimation of the quadratic function. In STATA this would look like:\n\nreg testscr c.avginc##c.avginc, r\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 2, 417)         =     428.52\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.5562\n                                                Ro\n> ot MSE          =     12.724\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n      avginc |   3.850995   .2680941    14.36   0.\n> 000                                             \n>          3.32401                                \n>                     4.377979\n             |\n    c.avginc#|\n    c.avginc |  -.0423085   .0047803    -8.85   0.\n> 000                                             \n>         -.051705                                \n>                    -.0329119\n             |\n       _cons |   607.3017   2.901754   209.29   0.\n> 000                                             \n>         601.5978                                \n>                     613.0056\n--------------------------------------------------\n> ----------------------------\n\n\nNow, it is straightforward to test the null-hypothesis of linearity against the alternative that the regression function is a quadratic. Namely, we only have to consider the \\(t\\)-statistic of the quadratic term. And that is larger than 1.96, so against a 5% significance level we reject the null-hypothesis of linearity.\nNote by the way the syntax c.avginc##c.avginc which seems a bit strange. However, this particular line of code is very useful for later tabulation, plotting and other manipulations of the output. In this way STATA knows that there should be a quadratic effect of the same variable (avginc). The syntax c. denotes that the variable should be considered as continuous instead of as an integer (try it and behold the horrible output). There are four useful operators that you want to know when working with polynomials and interaction effect:\n\ni. operator: this specifies that the following variable is an integers and should be considered on all its level. This actually create indicator or dummies variables\nc. operator: this specifies that the following variable is a continuous variables and should be treated as continuous.\n# binary operator that specifies an interaction between two variables\n## binary operator that specifies both interaction between two variables and the individual variable effect\n\nPlotting, non-linear population regression lines are a bit tricky. Namely, you want to combine a polynomial with a linear dimension. One way of doing this is as follows:\n\npredict hat1 \nscatter (testscr avginc) || (line hat1 avginc, sort)\n\nwhere after the regression we predict the test scores (and name it something like hat1) and then we ask for a line of the prediction for each value of average district income. Note, though, that we have to sort the prediction from small to large to get a smooth line. And this provides the nice curved population regression line in the following STATA output.\n\n\n\n\n\nA non-linear relation\n\n\n\n\nBut what is now the marginal effect of average district income. That, now, depends on itself. Namely, \\(\\frac{\\partial \\text{testscore}}{\\partial \\text{income}} = \\beta_1 + \\beta_2 \\text{income}\\). Another way of seeing this is to compute the effects for different values of \\(X\\) \\[\\begin{equation}\n\\widehat{TestScore_i} = 607.3 + 3.85 Income_i - 0.0423(Income_i)^2\n\\end{equation}\\] The predicted change in test scores for a change in income from $5,000 per capita to $6,000 per capita then amounts to: \\[\\begin{eqnarray}\n\\Delta \\widehat{TestScore} &=& 607.3 + 3.85 \\times 6 -  0.0423 \\times 6^2 \\\\\n&& - (607.3 + 3.85\\times 5 - 0.0423\\times 5^2)\\\\\n&=&3.4\n\\end{eqnarray}\\]\nAnd if calculate the predicted effects for different values of \\(X\\), then we get the following table:\n\n\n\n\nEffect of $X$\n \n  \n    Change in Income (1000 dollar per capita) \n    $\\Delta \\widehat{TestScore}$ \n  \n \n\n  \n    from 5 to 6 \n    3.4 \n  \n  \n    from 25 to 26 \n    1.7 \n  \n  \n    from 45 to 46 \n    0.0 \n  \n\n\n\n\n\nThus, the effect of a change in income is greater at low than high income levels (perhaps, a declining marginal benefit of an increase in school budgets?). But, be careful here! What is the effect of a change from 65 to 66? That is quite negative and already Figure @ref(fig:scatterqua) shows that a quadratic specification start to decline again the value of about 50; and perhaps that is not the behaviour that you want. So, with polynomials it is essential not to extrapolate outside the range of the data (and still interpret the outcome).\nThe estimation of a cubic specification is straightforward:\n\nreg testscr c.avginc##c.avginc##c.avginc, r\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 3, 416)         =     270.18\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.5584\n                                                Ro\n> ot MSE          =     12.707\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n      avginc |   5.018677   .7073504     7.10   0.\n> 000                                             \n>          3.62825                                \n>                     6.409103\n             |\n    c.avginc#|\n    c.avginc |  -.0958052   .0289537    -3.31   0.\n> 001                                             \n>         -.152719                                \n>                    -.0388913\n             |\n    c.avginc#|\n    c.avginc#|\n    c.avginc |   .0006855   .0003471     1.98   0.\n> 049                                             \n>         3.26e-06                                \n>                     .0013677\n             |\n       _cons |    600.079   5.102062   117.61   0.\n> 000                                             \n>         590.0499                                \n>                      610.108\n--------------------------------------------------\n> ----------------------------\n\n\nWhere if we now want to test the null- hypothesis of linearity, then we have to have invoke an \\(F\\)-test. Namely, the alternative hypothesis is that the population regression is quadratic and/or cubic, that is, it is a polynomial of degree up to 3, so:\n\n\\(H_0\\): Coefficients on \\(Income^2\\) and \\(Income^3 = 0\\)\n\\(H_1\\): at least one of these coefficients is nonzero.\n\nAnd the outcome below shows that the null-hypothesis that the population regression is linear is rejected at the 5% (and 1%) significance level against the alternative that it is a polynomial of degree up to 3.\n\ntest avginc#avginc avginc#avginc#avginc  \n\n ( 1)  c.avginc#c.avginc = 0\n ( 2)  c.avginc#c.avginc#c.avginc = 0\n\n       F(  2,   416) =   37.69\n            Prob > F =    0.0000\n\n\n\n\n3.3.2 Interaction variables\nUsing interaction variables is a special case of polynomial effects. Namely, instead of multiply a variable with itself \\(X\\times X = X^2\\), you now multiple a variable with another variable. And you want to do this to take into account interactions between independent variables. Assume, for example, that a class size reduction is more effective in some circumstances than in others (which is quite conceivable). Perhaps smaller classes help more if there are many English learners (i.e., large migrant communities), who need more individual attention. That is, \\(\\frac{\\partial TestScore}{\\partial STR}\\) might depend on \\(PctEL\\). More generally, this subsection looks into the fact that the marginal effect of \\(\\frac{\\partial Y}{\\partial X_1}\\) might depend on some other variable \\(X_2\\).\n\n3.3.2.1 Interactions between two binary variables\nFirst, we look into the simplest (and perhaps most insightful) case of two binary (dummy variables). Consider therefore the following linear regression model: \\[\\begin{equation}\nY_i =\\beta_0 +\\beta_1 D_{1i} + \\beta_2 D_{2i} +u_i,\n\\end{equation}\\] where both \\(D_{1i}\\) and $ D_{2i}$ are now considered to be binary. Now, of course, \\(\\beta_1\\) is the effect of changing \\(D_1=0\\) to \\(D_1=1\\). So, in this specification, this effect doesn’t depend on the value of \\(D_2\\). To allow the effect of changing \\(D_1\\) to depend on \\(D_2\\), we have to include the interaction term \\(D_{1i} \\times D_{2i}\\) as a regressor: \\[\\begin{equation}\nY_i =\\beta_0 +\\beta_1 D_{1i} + \\beta_2 D_{2i} + \\beta_3 (D_{1i} \\times D_{2i}) + u_i\n\\end{equation}\\]\nTo interpret now the coefficient \\(\\beta_1\\) we compare the two cases for \\(D_1=0\\) to \\(D_1=1\\)” \\[\\begin{eqnarray}\nE(Y_i|D_{1i}=0, D_{2i}=d_2) &=& \\beta_0 + \\beta_2 d_2 \\\\\nE(Y_i|D_{1i}=1, D_{2i}=d_2) &=& \\beta_0 + \\beta_1 + \\beta_2 d_2 + \\beta_3 d_2\n\\end{eqnarray}\\] If we now subtract them from each other: \\[\\begin{equation}\nE(Y_i|D_{1i}=1, D_{2i}=d2) - E(Y_i|D_{1i}=0, D_{2i}=d_2) = \\beta_1 + \\beta_3 d_2\n\\end{equation}\\] then we have the marginal effect of \\(D_1\\) which now depends on \\(d_2\\). The interpretation of \\(\\beta_3\\) boils down to being incremental to the effect of \\(D_1\\), when \\(D_2 = 1\\)\nLet us go back to our Californian school district example with the following variables to be used: test scores, student teacher ratio, and English learners. Let: \\[\\begin{eqnarray}\nHiSTR &=& 1 \\text{ if } STR \\geq 20 \\text{ and } HiEL = 1 \\text{ if }\nPctEL \\geq 10 \\\\\nHiSTR &=& 0 \\text{ if } STR < 20 \\text{ and } HiEL = 0 \\text{ if }\nPctEL < 10 \\\\\n\\end{eqnarray}\\] And if we have the estimation results we get the following outcome. \\[\\begin{equation}\n\\widehat{TestScore} = 664.1 - 18.2 HiEL - 1.9 HiSTR - 3.5(HiSTR \\times\nHiEL)\n\\end{equation}\\] So, how to interpret the various parameters? Perhaps the simple way is to construct the following two-by-two table:\n\n\n\n\nInterpretation of interaction effects with dummies\n \n  \n     \n    $HiEL = 0$ \n    $HiEL = 1$ \n  \n \n\n  \n    $HiSTR = 0$ \n    $664.1$ \n    $664.1 - 18.2 = 645.9$ \n  \n  \n    $HiSTR = 1$ \n    $664.1 - 1.9 = 662.2$ \n    $664.1 - 1.9 - 18.2 - 3.5= 640.5$ \n  \n\n\n\n\n\nNow, Table @ref(tab:intdummies) specifies for each combination (and there are exactly four of them) of \\(HiSTR\\) and \\(HiEL\\) the average expected test score outcome. Clearly, there are different ‘marginal’ effects of \\(HiSTR\\). Namely, the effect of \\(HiSTR\\) when \\(HiEL = 0\\) is \\(-1.9\\), whilst the effect of \\(HiSTR\\) when \\(HiEL = 1\\) is \\(-1.9 - 3.5 = -5.4\\). This points out that a class size reduction is estimated to have a bigger effect when the percent of English learners is large. However, when you estimate this in STATA then you see that this interaction is not statistically significant, because the \\(t\\)-statistic equals \\(3.5/3.1 = 1.1\\)\n\n\n3.3.2.2 Interactions between continuous and binary variables\nThe second case we consider is between a continuous and a binary variable. First assume the following regression model: \\[\\begin{equation}\nY_i =\\beta_0 + \\beta_1 X_i + \\beta_2 D_i + +u_i,\n\\end{equation}\\] where \\(D_i\\) is a binary variable and \\(X\\) is a continuous variable. As specified above, the effect on \\(Y\\) of \\(X\\) (holding \\(D\\) constant) = \\(\\beta_1\\), which does not depend on \\(D\\). To allow the effect of \\(X\\) to depend on \\(D\\), we can include the interaction term \\(D_i \\times X_i\\) as a regressor: \\[\\begin{equation}\nY_i =\\beta_0 + \\beta_1 X_i + \\beta_2 D_i  + \\beta_3 (D_i \\times X_i) + u_i\n\\end{equation}\\]\nWhat this binary-continuous interaction does is essential create two different population regression lines. Namely, for observations with \\(D_i= 0\\) (the \\(D = 0\\) group or the \\(D=0\\) regression line) there is: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_i  + u_i,\n\\end{equation}\\] Whilst for observations with \\(D_i= 1\\) (the \\(D = 1\\) group or the \\(D = 1\\) regression line) the regression line comes down to: \\[\\begin{eqnarray}\nY_i &=&   \\beta_0 + \\beta_2 + \\beta_1 X_i + \\beta_3 X_i + u_i \\\\\n            &=&  (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) X_i + u_i\n\\end{eqnarray}\\]\nAnd these two population regression lines might both differ in the level (the constant) and in the slope of the line. So, there are three possibilities as depicted in Figure @ref(fig:interaction)\n\n\n\n\n\nThree possible binary-continuous interaction outcomes\n\n\n\n\nIn the first panel (a), \\(\\beta_3 = 0\\), so there is only a level effect. In the second panel (b), both \\(\\beta_2\\) and \\(\\beta_3\\) are not 0, so there is both a level and a slope effect. The last panel indicates that \\(\\beta_2 = 0\\), meaning that there is only a slope effect. But how to interpreting the coefficients now? Therefore, we take the marginal effect of \\[\\begin{equation}\nY =\\beta_0  + \\beta_1 X  +\\beta_2 D+ \\beta_3 (D \\times X)\n\\end{equation}\\] which yields: \\[\\begin{equation}\n\\frac{\\partial Y}{\\partial X} = \\beta_1 + \\beta_3 D\n\\end{equation}\\] Thus, the effect of \\(X\\) depends on \\(D\\) and \\(\\beta_3\\) is the increment to the effect of \\(X\\), when \\(D = 1\\) (a slope effect)\nTo see this in our Californian school district example we now use the variables test scores, student teacher ratio and the as previously defined dummy variable \\(HiEL\\) as: \\[\\begin{equation}\n\\widehat{TestScore} = 682.2 - 0.97 STR + 5.6 HiEL - 1.28(STR \\times HiEL)\n\\end{equation}\\] Now when \\(HiEL = 0\\) the population regression line amounts to: \\[\\begin{equation}\n\\widehat{TestScore} = 682.2 - 0.97 STR\n\\end{equation}\\] And when \\(HiEL = 1\\) the population regression line is: \\[\\begin{eqnarray}\n\\widehat{TestScore} &=& 682.2 - 0.97 STR + 5.6 - 1.28 STR \\\\\n&=& 687.8 - 2.25 STR\n\\end{eqnarray}\\] Thus we have two regression lines: one for each \\(HiSTR\\) group. And the conclusion is that a class size reduction is estimated to have a larger effect when the percent of English learners (migrant communities) is large.\nHypothesis testing is as before. To test whether the two regression lines have the same slope, the null-hypothesis boils down to the coefficient of \\(STR \\times HiEL\\) being zero: the \\(t\\)-statistic of this one become \\(-1.28/0.97 = -1.32\\) and thus we do not reject this test. To test whether the two regression lines have the same intercept, the null-hypothesis becomes the coefficient of \\(HiEL\\) being zero, yielding: \\(t = -5.6/19.5 = 0.29\\), so we do not reject that null-hypothesis either. Interestingly, the null-hypothesis that the two regression lines are the same—population coefficient on \\(HiEL = 0\\) and population coefficient on yields \\(STR \\times HiEL = 0\\): \\(F = 89.94 (p-value < .001)\\). So, we reject the joint hypothesis but neither individual hypothesis.\nFinally, the question may arise how to draw such lines as in Figure @ref(fig:interaction). For this the following code is very useful:\n\ngen hiel = (el_pct >= 10)\nreg testscr c.str##i.hiel, r\nmargins hiel, at(str=( 14 ( 2 ) 26 ))\nmarginsplot\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 3, 416)         =      63.67\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.3103\n                                                Ro\n> ot MSE          =      15.88\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -.9684601   .5891016    -1.64   0.\n> 101                                             \n>        -2.126447                                \n>                     .1895268\n      1.hiel |   5.639141   19.51456     0.29   0.\n> 773                                             \n>        -32.72029                                \n>                     43.99857\n             |\n  hiel#c.str |\n          1  |  -1.276613   .9669194    -1.32   0.\n> 187                                             \n>         -3.17727                                \n>                     .6240436\n             |\n       _cons |   682.2458   11.86781    57.49   0.\n> 000                                             \n>         658.9175                                \n>                     705.5742\n--------------------------------------------------\n> ----------------------------\n\n\nAdjusted predictions                              \n>          Number of obs                          \n>                        = 420\nModel VCE: Robust\n\nExpression: Linear prediction, predict()\n1._at: str = 14\n2._at: str = 16\n3._at: str = 18\n4._at: str = 20\n5._at: str = 22\n6._at: str = 24\n7._at: str = 26\n\n--------------------------------------------------\n> ----------------------------\n             |            Delta-method\n             |     Margin   std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n    _at#hiel |\n        1 0  |   668.6874   3.701457   180.66   0.\n> 000                                             \n>         661.4115                                \n>                     675.9633\n        1 1  |    656.454    4.85794   135.13   0.\n> 000                                             \n>         646.9048                                \n>                     666.0031\n        2 0  |   666.7505   2.577328   258.70   0.\n> 000                                             \n>         661.6843                                \n>                     671.8167\n        2 1  |   651.9638   3.391411   192.24   0.\n> 000                                             \n>         645.2974                                \n>                     658.6302\n        3 0  |   664.8136   1.536485   432.68   0.\n> 000                                             \n>         661.7933                                \n>                     667.8338\n        3 1  |   647.4737   2.026549   319.50   0.\n> 000                                             \n>         643.4901                                \n>                     651.4572\n        4 0  |   662.8766   .9248105   716.77   0.\n> 000                                             \n>         661.0588                                \n>                     664.6945\n        4 1  |   642.9835    1.18965   540.48   0.\n> 000                                             \n>          640.645                                \n>                      645.322\n        5 0  |   660.9397   1.458111   453.28   0.\n> 000                                             \n>         658.0735                                \n>                     663.8059\n        5 1  |   638.4934   1.851156   344.92   0.\n> 000                                             \n>         634.8546                                \n>                     642.1321\n        6 0  |   659.0028   2.484598   265.24   0.\n> 000                                             \n>         654.1189                                \n>                     663.8867\n        6 1  |   634.0032    3.18456   199.09   0.\n> 000                                             \n>         627.7434                                \n>                     640.2631\n        7 0  |   657.0659   3.605093   182.26   0.\n> 000                                             \n>         649.9794                                \n>                     664.1523\n        7 1  |   629.5131    4.64319   135.58   0.\n> 000                                             \n>          620.386                                \n>                     638.6401\n--------------------------------------------------\n> ----------------------------\n\n\nVariables that uniquely identify margins: str\nhiel\n\n\nSo, first, we generate a new dummy variable hiel as discussed above. Then we regress testscores on class size, the new hiel dummy variable and the interaction using the two hashtags. We then ask for the marginal effect of hiel, so for both values of it (being 0 and 1), for all class sizes between 14 and 26 (with steps of 2). Finally, we ask for the plots of the margins using the command marginsplot. This provides the following STATA plot.\n\n\n\n\n\nPredicted population regression lines of districts with large and small percentage english learners\n\n\n\n\nClearly, Figure @ref(fig:interactionplot) shows that district with more English learners (containing larger migrant communities) have lower test scores overall. Above that, class size seems to have a large negative effect on districts with more English learners as the slope is more negative.\n\n\n3.3.2.3 Interactions between two continuous variables\nThe last case are interaction between two continuous variables and that is always a difficult case of interpret. Starting again with the model: \\[\\begin{equation}\nY_i =\\beta_0 + \\beta1 X_{1i} +\\beta_2 {X_{2i}} +u_i,\n\\end{equation}\\] where both \\(X_1\\), \\(X_2\\) are continuous and as specified, the effect of \\(X_1\\) doesn’t depend on \\(X_2\\) and the effect of \\(X_2\\) doesn’t depend on \\(X_1\\). Now, to allow the effect of \\(X_1\\) to depend on \\(X_2\\), we include the interaction term \\(X_{1i} \\times X_{2i}\\) as a regressor. Where, to interpret the coefficients, we take the first derivative of \\(X_1\\) in: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 (X_{1i}\n\\times X_{2i}) + u_i\n\\end{equation}\\] which yields: \\[\\begin{equation}\n\\frac{\\partial Y}{\\partial X} = \\beta_1 + \\beta_3 X_2\n\\end{equation}\\] where \\(\\beta_3\\) should be interpreted as the increment to the effect of \\(X_1\\) from a unit change in \\(X_2\\).\n\n\n\n3.3.3 Logarithmic transformations\nTo incorporate non-linear effect, very often logarithmic transformations are used of \\(Y\\) and/or \\(X\\), where typically we use \\(\\ln(X)\\) as the natural logarithm of \\(X\\). One feature of logarithmic transformations is that they permit modeling relations in percentage terms (like elasticities), rather than linearly. That is because: \\[\\begin{equation}\n\\ln(x+\\Delta x) - \\ln(x) = \\ln (1 + \\frac{\\Delta x}{x}) \\cong \\frac{\\Delta x}{x}\n\\end{equation}\\] Note that this is an approximation, but from calculus we know that \\(\\frac{d \\ln(x)}{dx}=\\frac{1}{x})\\). And the above approximation works quite well for small numbers. For example, numerically: \\(\\ln(1.01) = .00995 \\cong .01\\) and \\(\\ln(1.10) = .0953 \\cong .10\\), where the latter is still rather close. Now remember the following rules for natural logarithms 1. \\(\\ln(a\\times b)= \\ln(a)+\\ln(b)\\) 2. \\(\\ln(\\frac{a}{b}) =\\ln(a) - \\ln(b)\\) 3. \\(\\ln(a^\\alpha) = \\alpha \\ln(a)\\) 4. \\(\\ln(e^X) = X\\).\nWhen you encounter a nonlinear model such as the ones adopted in Chapter @ref(surplus) a strategy that often works is log-linearization. That works as follows \\[\\begin{equation}\nY = A K^\\alpha L^{1-\\alpha} \\rightarrow \\ln(Y) = \\ln(A) + \\alpha \\ln(K) + (1-\\alpha) \\ln(L),\n\\end{equation}\\] where you take the natural logarithm on both sides. There are three different cases of logarithmic regression models as specified in Table @ref(tab:logspecification).\n\n\n\n\nThree logarithmic transformation\n \n  \n    Case \n    Population regression model \n  \n \n\n  \n    linear-log \n    $Y_i=\\beta_0 + \\beta_1 \\ln(X_i) + u_i$ \n  \n  \n    log-linear \n    $\\ln(Y_i)=\\beta_0 + \\beta_1 (X_i) + u_i$ \n  \n  \n    log-log \n    $\\ln(Y_i)=\\beta_0 + \\beta_1 \\ln(X_i) + u_i$ \n  \n\n\n\n\n\nThough statistical testing remains the same, the interpretation of the slope coefficient differs in each case. To derive the interpretationwe want to find the marginal effect of \\(X\\) using the first derivative.\n\n3.3.3.1 Linear-log population regression model\nThe linear-log population regression model is specified as: \\[\\begin{equation}\n    Y = \\beta_0 + \\beta_1 \\ln(X)\n\\end{equation}\\] Now take the first derivative: \\[\\begin{equation}\n    \\frac{\\partial Y}{\\partial X} = \\frac{\\beta_1}{X}\n\\end{equation}\\] so \\[\\begin{equation}\n    \\beta_1  = \\frac{\\partial Y}{\\partial X / X}\n\\end{equation}\\] In this case that means that \\(\\beta_1\\) should be interpreted as the absolute change of \\(Y\\) when \\(X\\) changes with \\(\\beta_1/100\\) percent. To illustrate this, consider the case where we take natural logarithm od district income, so we define the new regressor as, \\(\\ln(Income)\\)\nThe model is now linear in \\(\\ln(Income)\\), so the linear-log model can be estimated by OLS, which yields \\[\\begin{equation}\n        \\widehat{TestScore} = 557.8 + 36.42\\times \\ln(Income_i)\n\\end{equation}\\] so an 1% increase in \\(Income\\) is associated with an increase in test scores of 0.36 points on the test. And again, standard errors, confidence intervals, \\(R^2\\)—all the usual tools of regression apply here. But the difficulty in plottin the new regression line remains. Consider the following STATA syntax, where we first have to define the new regressor by invoking the generate command.\n\ngen lninc = ln(avginc)\nreg testscr lninc, r\npredict testhat\ngraph twoway (line testhat avginc, sort) (scatter testscr avginc)\n\nThis now provides the following STATA output.\n\n\n\n\n\nA non-linear relation\n\n\n\n\nWhen you compare @ref(fig:scatterlnincome) with @ref(fig:scatterqua) then you notice that in the case of logarithm the population remains increasing (but less and less steep). This can be considered as an advantage when you want to estimate decreasing (or increasing) return.\n\n\n3.3.3.2 Log-linear population regression model\nThe second case we consider is the log-linear population regression model, as specified by: \\[\\begin{equation}\n    \\ln(Y) = \\beta_0 + \\beta_1 X\n\\end{equation}\\] To find the interpretation of \\(\\beta1_1\\), we again take the first derivative \\(\\frac{\\partial Y}{\\partial X}\\), but first transform the model like this: \\[\\begin{equation}\n    Y = exp( \\beta_0 + \\beta_1 X )\n\\end{equation}\\] then take the first derivative: \\[\\begin{equation}\n    \\frac{\\partial Y}{\\partial X} = \\beta_1  exp( \\beta_0 + \\beta_1 X ) = \\beta_1 Y\n\\end{equation}\\] and collec terms \\[\\begin{equation}\n    \\beta_1  = \\frac{\\partial Y / Y}{\\partial X }\n\\end{equation}\\]\nThe interpretation of \\(\\beta_1\\) now is that one unit change in \\(X\\) causes a \\(\\beta_1\\) percentage in \\(Y\\)\n\n\n3.3.3.3 Log-log population regression model\nFinally, we have our third case, being the log-log population regression model as specified by: \\[\\begin{equation}\n    \\ln(Y) = \\beta_0 + \\beta_1 \\ln(X)\n\\end{equation}\\]\nTo find the interpretation of \\(\\beta1_1\\), we again take the first derivative \\(\\frac{\\partial Y}{\\partial X}\\), but first transform the model like this: \\[\\begin{equation}\n    Y = exp( \\beta_0 + \\beta_1 \\ln(X) )\n\\end{equation}\\] So \\[\\begin{equation}\n    \\frac{\\partial Y}{\\partial X} = \\beta_1 /X  exp( \\beta_0 + \\beta_1 \\ln(X) ) = \\beta_1 Y /X\n\\end{equation}\\] and after collecting terms we end up with an elasticity: \\[\\begin{equation}\n    \\beta_1  = \\frac{\\partial Y / Y}{\\partial X / X }\n\\end{equation}\\]\nAs an example consider the case when we want to regress ln(test scores) on ln(income). To do so, we first define a new dependent variable, ln(TestScore), and a new regressor, ln(Income) The model is now a linear regression of ln(TestScore) against ln(Income), which can be estimated by OLS as follows \\[\\begin{equation}\n\\widehat{ln(TestScore)} = 6.336 + 0.0554 \\times ln(Income_i),\n\\end{equation}\\] where the interpretation is that an 1% increase in \\(Income\\) is associated with an increase of .0554% in \\(TestScore\\) (\\(Income\\) up by a factor of 1.01, \\(TestScore\\) up by a factor of 1.000554)\nSuppose that we now want to plot both the log-linear and the log-log specification, then we can use the following syntax:\n\ngen lninc = ln(avginc)\ngen lntestscr = ln(testscr)\nreg lntestscr lninc, r\npredict testhat1\nreg lntestscr avginc, r\npredict testhat2\ngraph twoway (line testhat1 avginc, sort) (line testhat2 avginc, sort) (scatter lntestscr avginc), legend(order(1 \"log-log specification\" 2 \"log-linear specification\" 3 \"Observations\")) \n\nwhich provides the following STATA output.\n\n\n\n\n\nA non-linear relation\n\n\n\n\nNote that the \\(y\\)-axis is on a logarithmic scale here, thus the log-linear specification is now a linear line.\n\n\n3.3.3.4 Summary: logarithmic transformations\nWe have seen three different cases of logarithmic specification, differing in whether \\(Y\\) and/or \\(X\\) is transformed by taking logarithms. Now, the regression is linear in the new variable(s) \\(\\ln(Y)\\) and/or \\(\\ln(X)\\), and the coefficients can be estimated by OLS where hypothesis tests and confidence intervals are now implemented and interpreted ‘as usual’. Only the interpretation of the coefficients differs from case to case and is directly related to percentage changes (growth) and elasticities. Oftentimes, the choice of specification, however, should be guided by judgment (which interpretation makes the most sense in your application?), tests, and plotting predicted values. Sometimes, though, you have a structural economic model such as Equation @ref(eq:directutility), which defines the type of specification you should use. Finally, see that in economics many models exists with decreasing or increasing return to scale and that these are very closely related with logarithmic specifications."
  },
  {
    "objectID": "multivariate.html#sec:fixedeffects",
    "href": "multivariate.html#sec:fixedeffects",
    "title": "3  Modeling in the Social Sciences",
    "section": "3.4 Using fixed effects in panel data",
    "text": "3.4 Using fixed effects in panel data\nMultivariate regression is a powerfull tool for controlling for the effect of variables for which we have data. But often we do not have data on what we suspect might be important—data, such as individual characteristics like ambition, intelligence, drive or stamina. Or regional of country data, where the type of soil, the ruggedness (hilliness), or population density determine to a large extent the behaviour of people living on it. If we do not have this type of data, then it not always the case that everything is lost. Especially, when we have repeated observations, so observations of the same entity throughout time. This is referred to as panel data and requires one additional subscript \\(t\\) as in \\(X_{it}\\) indicating the observation \\(X\\) on individual made at time \\(t\\). To understand why this sometimes works, we temporarily change to another dataset and that is the ‘fatality’ data collected by Levitt and Porter (2001) and deals with the relation between drunk driving and fatal accidents in the States of the US between 1982 and 1988. For this particular example we look at the impact of the ‘beer tax’, measured as the real tax in dollars on a case of beer, on ‘fatality’, measured as the number of annual traffic deaths per 10,000 people in the population of each stata. For this we first read the data and manipulate the mortality variable\n\nuse \"./data/fatality.dta\", clear\ngen fatality = allmort/pop * 10000\n\nand then run a simple regression:\n\nregress fatality beertax, robust\n\nLinear regression                               Nu\n> mber of obs     =        336\n                                                F(\n> 1, 334)         =      47.59\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.0934\n                                                Ro\n> ot MSE          =     .54374\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n    fatality | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n     beertax |   .3646054   .0528524     6.90   0.\n> 000                                             \n>         .2606399                                \n>                      .468571\n       _cons |   1.853308   .0471297    39.32   0.\n> 000                                             \n>         1.760599                                \n>                     1.946016\n--------------------------------------------------\n> ----------------------------\n\n\nBut these outcomes are very strange. For every dollar increase in tax, number of fatal accidents per 10,000 people increases with 0.36, which is statistically significantly different from 0. What is going on here. Most likely this effect is biased because of omitted variable bias. States in the US differ widely in terms of population density, environment, institutions, religion, poverty, and so on and so forth. And Those state characteristics might influence both the variables beertax and fatality.\nFortunately, for each state we have yearly data. So, that is 7 observations per stata and we can make use of that by using fixed effects, which is a very common technique in the social sciences—especially in economics. We model the use of fixed effects in this example as follows: \\[\\begin{equation}\n\\text{fatality}_{it} = \\beta_0 + \\beta_1\\text{beertax}_{it} + \\beta_3 S_1 + \\ldots + \\beta_51 S_{48} + u_{it},\n\\end{equation}\\] where \\(S_i\\) denote indicator (dummies) for each state which constitute the fixed effects. In total there are 48 states in this dataset, so we have 48 dummies. Note that these fixed effects only depend on the state variation, not on time variation. So, essentially what these fixed effects capture is all state specific characteristics which are constant over time. And most of the characteristics’ examples given above do not vary that much over time, so by using these state fixed effects we can control for them. In STATA you can estimate this in a straightforward way as regress fatality beertax i.state, robust, but this lots of statistical output that you are usually not interested in. Almost just as easy would be is to invoke the areg command, where you specifically state that the state variable should be used as dummies but not shown using absorb(state): and then run a simple regression:\n\nareg fatality beertax, absorb(state) robust\n\nLinear regression, absorbing indicators           \n>   Number of obs                                 \n>                     =    336\nAbsorbed variable: state                          \n>   No. of categories                             \n>                     =     48\n                                                  \n>   F(1, 287)                                     \n>                     =  10.41\n                                                  \n>   Prob > F                                      \n>                     = 0.0014\n                                                  \n>   R-squared                                     \n>                     = 0.9050\n                                                  \n>   Adj R-squared                                 \n>                     = 0.8891\n                                                  \n>   Root MSE                                      \n>                     = 0.1899\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n    fatality | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n     beertax |  -.6558737   .2032797    -3.23   0.\n> 001                                             \n>        -1.055982                                \n>                    -.2557655\n       _cons |   2.377075   .1051516    22.61   0.\n> 000                                             \n>         2.170109                                \n>                     2.584041\n--------------------------------------------------\n> ----------------------------\n\n\nNow, see what happens with the coefficient of the beer tax variable. It changes sign! So from positive it becomes negative. That is how disruptive omitted variable bias can be. Also see that by including all these state fixed effects, the \\(\\bar{R^2}\\) now increase enormously to 91%, which does make sense because the states explain the variation in fatality rate to a large extent (e.g., compare Kansas with Connecticut).\nThis is just a snapshot of the use of fixed effects in panel data, but for now this is enough. But for now, know that the use of fixed effects can go a long way in addressing omitted variable bias."
  },
  {
    "objectID": "multivariate.html#conclusion-and-discussion",
    "href": "multivariate.html#conclusion-and-discussion",
    "title": "3  Modeling in the Social Sciences",
    "section": "3.5 Conclusion and discussion",
    "text": "3.5 Conclusion and discussion\n\n\n\n\nLevitt, Steven D, and Jack Porter. 2001. “How Dangerous Are Drinking Drivers?” Journal of Political Economy 109 (6): 1198–1237.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.\n\n\nPearl, Judea. 2009. Causality. Cambridge university press.\n\n\nStock, James H, Mark W Watson, et al. 2003. Introduction to Econometrics. Vol. 104. Addison Wesley Boston."
  },
  {
    "objectID": "linear_regression.html#sec-uniregress",
    "href": "linear_regression.html#sec-uniregress",
    "title": "2  Regression Analysis in the Social Sciences",
    "section": "2.3 Univariate regression",
    "text": "2.3 Univariate regression\nThe three strategies we adopted in Section 2.2.2 for assessing the difference between groups directly translate to the case of regression analysis. Here we also look at estimation, hypothesis testing and confidence intervals. But before that we first look at the origin of the name regression in Subsection @ref(sec:genesis)\n\n2.3.1 Genesis: regression towards the mean\nThe name regression seems to have a negative connotation, as progress is in general seen as good and regression as bad. And actually this is true, the name regression was deliberately given as to describe a negative process: in full regression towards the mean. The concept of regression was actually coined by Sir Francis Galton together with other statistical terms, such as correlation and deviation (Senn 2011). Galton was a notorious statistician who measured everything and else, including the length of french bread and the size of human skulls.\nin 1886, Galton started to research the height of adult children with the height of their parents (Galton 1886). The original data can be seen in the scatterplot in Figure 2.2. What Galton expected was that the relation between the height of children and that of their parents was a one-to-one relation. On average children should receive the same height of their parents. So, in fact he expected a \\(45^{\\circ}\\) line—the red line; a line with slope equal to 1.\n\n\n\n\n\nFigure 2.2: Relation heigh fathers and height children\n\n\n\n\nHowever, he found consistently the blue line, a line with positive slope but lower than 1 (the blue line in Figure 2.3). That entails that, on average tall parents get tall children but not as tall as themselves. Of course, this goes as well the other way. Short parents get short children but not as short as themselves.\n\n\n\n\n\nFigure 2.3: Relation heigh fathers and height children\n\n\n\n\nGalton coined this process regression towards the mean.5 In the end we would all converge towards the mean and all look the same. For the Victorian Sir Frances Galton and his contemporaries in an age where social and income classes were highly separated this was truly a horror. Especially, because his cousin was Charles Darwin who actually claimed that species diverged. Of course, in Galton reasoning there is a mistake as this only models genetic influence and not accidental differences not influenced by genetics. Note as well that this analysis says something about the average, but not about individual differences.\nThis regression towards the mean is now seen as a very important characteristic of regression models, and you can easily be fooled by it. It is now stated as:\n\n\\(\\ldots\\) a concept that refers to the fact that if one sample of a random variable is extreme, the next sampling of the same random variable is likely to be closer to its mean\n\nFor instance, suppose that everything went really well for a course and you got a 9 for an examination. That does not mean that the next time you will do equally well (you will still do well, but not that well). Or, your favorite football club does extreme well in a particular year (Leicester City FC comes to mind who became premier league champion in 2016). That does not mean that the next year it will do equally well, and so forth and so on.\n\n\n2.3.2 Regression with one regressor\nSo, linear regression allows us to estimate, and make inferences about, population slope coefficients. Inference means drawing conclusions and population refers to the fact that we do not want to say something about our sample, but instead about the whole population. Ultimately our aim is to estimate the causal effect on \\(Y\\) of a unit change in \\(X\\)—but for now, just think of the problem of fitting a straight line to data on two variables, \\(Y\\) and \\(X\\).\nSimilar to Subsection Section 2.2.2) we have three strategies to make inferences:\n\nWe estimation the relation:\n\nThis now boils down to the question how we should draw a line through the data to estimate the (population) slope using Ordinary Least Squares (OLS—a specific and most common type of regression analysis)\nAnd then we have to assess the advantages and disadvantages of OLS\n\nWe could refer to hypothesis testing:\n\nVery often this comes down to testing where the slope is zero. Namely, if the slope is zero, then the data does not show a relation between \\(Y\\) and \\(X\\).\n\nUsing confidence intervals:\n\nThis is related to constructing a confidence interval for the slope\n\n\nBefore we look into this we first need some clarification on notation. As mentioned above, we would like to know the population regression line:\n\\[\\begin{equation}\ntestscr = \\beta_0 + \\beta_1 STR,\n\\end{equation}\\] where \\[\\begin{eqnarray}\n    \\beta_1& =& \\text{slope of population regression line} \\notag \\\\\n    &=&   \\frac{\\Delta Testscore}{\\Delta STR} \\notag \\\\\n    &=& \\text{change in test score for a \\textbf{unit} change in STR}\n\\end{eqnarray}\\] Note the definition here of \\(\\beta_1\\). It gives the marginal effect of a change in \\(STR\\) on \\(testscr\\). So the interpretation of the parameter \\(\\beta_1\\) is very straightforward. However, we do not know the population value of \\(\\beta_1\\) and we therefore have to estimate it using data.\nIn general, the population linear regression model is different as we add element \\(u_1\\). \\[\\begin{equation}\n    Y_i = \\beta_0 + \\beta_1 X_i + u_i, \\qquad i\\ldots n\n    \\label{eq:ols}\n\\end{equation}\\] Now, \\(X\\) denotes the independent variable or regressor, \\(Y\\) the dependent variable, \\(\\beta_0\\) the intercept, \\(\\beta_1\\) the slope, and \\(u_i\\) the regression error. The regression error consists of omitted factors, or possibly measurement error in the measurement of \\(Y\\). In general, these omitted factors are other factors that influence \\(Y\\), other than the variable \\(X\\).\n\n2.3.2.1 Estimating with OLS\n\n\n\n\n\nFigure 2.4: Drawing a straight line through data in a scatterplot\n\n\n\n\nTo estimate the population linear regression model we apply the ordinary least squares estimator. Again, as Figure 2.4 shows as well, a linear regression line is a straight line through points in a scatterplot. Actually, we want to draw that line such that the distance of all points to that line is minimized. See that in Figure 2.4 the distances between the points and the line are given by the \\(u_i\\)’s, the regression errors. So, if we somehow can minimize all \\(u_i\\)’s we are fine. But those distances could be both positive and negative and they might cancel each other out. Therefore, we first square the regression errors and then minimize (hence the name: ordinary least squares). Also, see from Eq. \\(\\ref{eq:ols}\\) that:\n\\[\\begin{equation}\nu_i = Y_i - (\\beta_0 + \\beta_1 X_i) \\longleftrightarrow (u_i)^2 = \\left[Y_i - (\\beta_0 + \\beta_1 X_i) \\right]^2\n\\end{equation}\\]\nBut how can we estimate \\(\\beta_0\\) and \\(\\beta_1\\) from data? For that we will focus on the least squares (ordinary least squares or OLS) estimator of the unknown parameters \\(\\beta_0\\) and \\(\\beta_1\\), which solves, \\[\\begin{equation}\n    \\min_{b_0,b_1} \\sum^n_{i=1} \\left[Y_i - (b_0 + b_1 X_i) \\right]^2\n\\end{equation}\\]\nIn fact, the OLS estimators of the slope \\(\\beta\\)\\(_1\\) and the intercept \\(\\beta\\)\\(_0\\) are:6\n\\[\\begin{eqnarray}\n    \\hat{\\beta}_1 &=& \\frac{\\sum^n_{i=1}(X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sum^n_{i=1}(X_i - \\overline{X})^2} = \\frac{s_{XY}}{s^2_X}\\\\\n    \\hat{\\beta}_0 &=& \\overline{Y} - \\hat{\\beta}_1\\overline{X}\n\\end{eqnarray}\\]\nAlthough you do need to learn these formula’s by heart some insightful comments can be given. First, if a parameter is estimated then it gets a hat symbol on its top. Second, the optimal \\(\\hat{\\beta}_1\\) is equal to \\(\\frac{s_{XY}}{s^2_X}\\) and this is the sampling covariance between \\(X\\) and \\(Y\\) divided by the sampling variance of \\(X\\). This is not a correlation as the units still depend on \\(X\\) and \\(Y\\) and therefore the slope can be larger than \\(1\\) or smaller than \\(-1\\), but it does say something about the relation between \\(X\\) and \\(Y\\). Third, the constant is governed by the estimated parameter \\(\\hat{\\beta}_1\\)\nFrom here we can predict the values \\(\\hat{Y}_i\\) and residuals \\(\\hat{u}_i\\) as they are: \\[\\begin{eqnarray}\n    \\hat{Y}_i &=& \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i, \\qquad i = 1, \\ldots, n \\notag\\\\\n    \\hat{u}_i &=& Y_i - \\hat{Y}_i, \\qquad i = 1, \\ldots, n\n\\end{eqnarray}\\]\nWhen we apply this to our data cloud in Figure 2.1 then we get the following optimal population regression line:\n\n\n\n\n\nFigure 2.5: Scatterplot and estimated regression line\n\n\n\n\nwhere the estimated slope equals \\(\\hat{\\beta}_1 = -2.28\\), the estimated intercept equals \\(\\hat{\\beta}_0 = 698.9\\) and the total population regression line can be written as: \\(\\widehat{TestScore} = 698.9 - 2.28 \\times STR\\). So, how to interpret the estimated slope and intercept now? First, the slope entails that districts with one more student per teacher on average have test scores that are 2.28 points lower (that is, \\(\\frac{\\Delta TestScore}{\\Delta STR} =-2.28\\)). Secondly, the intercept (taken literally) means that, according to this estimated line, districts with zero students per teacher would have a (predicted) test score of 698.9. Now, this does not make any sense—it actually extrapolates the line outside the range of the data. In this case we can say that the intercept is not economically meaningful.\nNow, how to fill in predictions? One of the districts in the data set is Antelope (CA) for which \\(STR = 19.33\\) and \\(TestScore = 657.8\\) Then the predicted value for the testscore is \\(\\hat{Y}_{Antelope}= 698.9 - 2.28 \\times 19.33 = 654.8\\) and the resulting residual is \\(\\hat{u}_{Antelope} = 657.8 - 654.8 = 3.0\\)\nIn STATA both the constant and the slope can be easily retrieved by:\n\nregress testscr str, robust\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 1, 418)         =      19.26\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.0512\n                                                Ro\n> ot MSE          =     18.581\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -2.279808   .5194892    -4.39   0.\n> 000                                             \n>        -3.300945                                \n>                    -1.258671\n       _cons |    698.933   10.36436    67.44   0.\n> 000                                             \n>         678.5602                                \n>                     719.3057\n--------------------------------------------------\n> ----------------------------\n\n\nWe will discuss the rest of this output later.\n\n\n2.3.2.2 Hypothesis testing\nWe can assess the importance of the line as well with hypothesis testing. Again, recall that in in applied econometrics we will only reject the null-hypothesis, we do not accept an hypothesis based upon one statistical test only. So, we aim to test \\(H_0: E(Y) = \\mu_{Y,0}\\) vs. \\(H_1: E(Y) \\neq \\mu_{Y,0}\\), where \\(\\mu_{Y,0}\\) is some specified quantity that we are interested in. Typically \\(\\mu_{Y,0} = 0\\) as this denotes no relation, but sometimes you could be interested in, e.g., whether \\(\\mu_{Y,0} = 1\\) when testing elasticities. Or you could be interested in other quantities.\nTesting statistical hypotheses is often very confusing, because of two things. First, you actually test whether the data you have corresponds with the null-hypothesis. Or, in other words:\n\nWhat is the probability that your data (\\(D\\)) might be right given the null-hypothesis (\\(H_0\\)): \\(\\Pr(D|H_0)\\)\n\nAnd that is a strange concept. You first imagine a world \\(H_0\\) with the data that it should provide and then test that imaginary world.\nSecondly, there is the notation that often works confusing. First, we have the \\(p\\)-value which equals the probability of drawing a statistic (e.g., \\(\\bar{Y}\\)) at least as adverse to the null (that is: your imaginary world) as the value actually computed with your data, assuming again that the null-hypothesis is true—again, your imaginary world. Secondly, there is the significance level of a test which is a pre-specified probability of incorrectly rejecting the null, when the null is actually true.\nNow, suppose that you want to calculate the \\(p\\)-value based on an estimated coefficient \\(\\hat{\\beta}_1\\), then you construct the following test: \\[\\begin{equation}\np\\text{-value} = \\Pr_{H_0}[|\\hat{\\beta}_1 - \\beta_{1,0}| > |\\hat{\\beta}_1^{act} - \\beta_{1,0}|\n\\end{equation}\\] where \\(\\hat{\\beta}_1^{act}\\) is the value of \\(\\hat{\\beta}_1\\) actually observed, and \\(\\beta_{1,0}\\) is the value of \\(\\beta_1\\) under the null-hypothesis (e.g., \\(\\beta_{1,0} = 0\\)). Now, this is confusing, but in words it states that if you belief the null-hypothesis, what is then the probability that the estimated value is \\(\\hat{\\beta}_1^{act}\\) or even more adverse to the value of the null hypothesis (in other words even more extreme values).\nTo test the null hypothesis \\(H_0\\) we follow three steps. First, we need to compute the standard error of \\(\\hat{\\beta_1}\\), which is an estimator of \\(\\sigma_{\\hat{\\beta_1}}\\). Using an ordinary least squares estimator, standard errors for coefficients are given by:\n\\[\\begin{equation}\n\\sigma_{\\hat{\\beta_1}} = \\sqrt{\\frac{1}{n} \\frac{\\frac{1}{n-2} \\sum_{i=1}^n (X_i - \\bar{X})^2 u_i^2}{\\left[\\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\right]^2}},\n\\label{eq:olsse}\n\\end{equation}\\] which is a rather daunting expression.\nSecond, we need to compute the \\(t\\)-statistic: \\[\\begin{equation}\nt = \\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{SE(\\hat{\\beta}_1)}\n\\label{eq:olst}\n\\end{equation}\\]\nFinally, we need to calculate the \\(p\\)-value. To do so, we need to know the sampling distribution of \\(\\hat{\\beta}_1\\), which we know is complicated if \\(n\\) is small, but typically you have enough observations to invoke the Central Limit Theorem. So, if \\(n\\) is large, you can use the normal approximation (CLT) as follows \\[\\begin{eqnarray}\n                        p\\text{-value}& = &   \\Pr_{H_0}\\left[\\left|\\hat{\\beta}_1 - \\beta_{1,0}\\right| > \\left|\\hat{\\beta}_1^{act} - \\beta_{1,0}\\right|\\right] \\notag\\\\\n            & = &   \\Pr_{H_0}\\left[\\left|\\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{SE(\\hat{\\beta}_1)}\\right| > \\left|\\frac{\\hat{\\beta}_1 ^{act} - \\beta_{1,0}}{SE(\\hat{\\beta}_1)}\\right|\\right] \\notag \\\\\n            & = &   \\Pr_{H_0}[|t| > |t^{act}|] \\notag \\\\\n                        &\\simeq& \\text{probability under left + right } N(0,1) \\text{ tails}\n\\end{eqnarray}\\] where \\(SE(\\hat{\\beta}_1)\\) again equals the standard error of \\(\\hat{\\beta}_1\\), denoted with \\(\\sigma_{\\hat{\\beta}_1}\\)\nSo, if you know \\(\\hat{\\beta}_1\\) and \\(\\sigma_{\\hat{\\beta}_1}\\) you can calculate this. However, computers are much faster, in doing to. For example, suppose we want to test whether \\(\\beta_{1,0} = 0\\) using the regression output displayed above which gives \\(\\hat{\\beta}_1 = -2.28\\) and \\(\\sigma_{\\hat{\\beta}_1} = 0.52\\). That is step 1. Note that STATA already calculated the standard error of Eq. \\(\\ref{eq:olsse}\\). For the next step we need to compute the \\(t\\)-statistic, which is:\n\\[\\begin{equation}\nt^{act} = \\frac{2.28 - \\beta_{1,0}}{0.52} = \\frac{2.28 - 0}{0.52} = -4.39.\n\\label{eq:olstemp}\n\\end{equation}\\]\nthen the \\(p\\)-value can be seen from Figure 2.6:\n\n\n\n\n\nFigure 2.6: Calculating the \\(p\\)-value of a two-sided test when \\(t^{act} = -4.38\\)\n\n\n\n\nThat is, for large \\(n\\) (and typically we have that), the \\(p\\)-value is the probability that a \\(N(0,1)\\) random variable falls outside \\(|\\hat{\\beta}_1^{act} - \\beta_{1,0})/\\sigma_{\\hat{\\beta}_1} | = |t|\\). That is the blue areas under the normal distribution and they entail a probability mass. Now, if both surfaces on the sides are not larger than 2.5%, then we can reject the null-hypothesis against a 5% significance level. Now, the computer output above gives a \\(p\\)-value of \\(0.000\\), which a bit strange. The \\(p\\)-value is actually not zero, but a very small number and definitely smaller than \\(0.05\\), so we can reject the null-hypothesis being \\(\\beta_{1,0} = 0\\) at a 5% significance level (and at a 1% and 0.1% significance level as well). Now, if the \\(t\\)-statistic is exactly 1.96 in absolute value, then the \\(p\\)-value is 0.05. So, to repeat the steps, but now using computer output for testing the hypothesis that \\(\\beta_1 = \\beta_{1,0}\\)\n\nGet the standard error from computer output\nCompute the \\(t^{act}\\)-statistics as in Eq. \\(\\ref{eq:olst}\\)\nGet the corresponding \\(p\\)-value. Or, reject the null-hypothesis at the 5% significance level if \\(|t^{act}| > 1.96\\).\n\nNow there is a link between the \\(p\\)-value and the significance level. The significance level is pre-specified. For example, if the pre-specified significance level is 5%, then you reject the null hypothesis if \\(|t| \\geq 1.96\\) or equivalently, you reject if \\(p \\leq 0.05\\). The \\(p\\)-value is sometimes called the marginal significance level. Often, it is better to communicate the \\(p\\)-value than simply whether a test rejects or not—the \\(p\\)-value contains more information than the “yes/no” statement about whether the test rejects.\nBut recently there has been some debate about using \\(p\\)-values (Amrhein, Greenland, and McShane 2019). Why should you use a 5% significance level, what is so special about that number? Is it not better just to report coefficients and standard errors? Figure 2.7 shows a figure from the journal Nature and how scientists across all fields nowadays see \\(p\\)-values and statistical significance. This is not to say that statistical testing does not matter, but more the reporting of that statistical testing. First of all, \\(p\\)-values in themselves do not contain that much information. In the regression output of above you \\(p\\)-values being equal to 0.000 which is not informative. Secondly, the cut-off point of 5% is a bit harsh and could lead to publications being published only with \\(p\\)-values just below 0.05, leading to what is called publication bias.\n\n\n\n\n\nFigure 2.7: Critical review on the (mis)use of statistical significance\n\n\n\n\n\n\n2.3.2.3 Confidence intervals\nThe exact definition of confidence intervals is a bit tricky. Namely, a 95% confidence interval for \\(\\hat{\\beta}_1\\) is an interval that contains the true value of \\(\\beta_1\\) in 95% of repeated samples. That means that a confidence interval does not give a probability (even though we would like to interpret it that way). But you can state that every value within a confidence interval would not be rejected as null-hypothesis, while every value outside the confidence interval would be rejected. Now, if we know both \\(\\hat{\\beta}_1\\) and \\(\\sigma_{\\hat{\\beta}_1}\\) (again using computer output), then a 95% confidence interval can be very easily constructed. For our regression of output of above this entails\n\\[\\begin{equation}\n\\hat{\\beta}_1 \\pm 1.96 \\times \\sigma_{\\hat{\\beta}_1} = -2.28 \\pm 1.96 \\times 0.52 = [-3.30, -1.26].\n\\label{eq:olsci}\n\\end{equation}\\] So, every value between \\(-3.30\\) and \\(-1.26\\) will not be rejected as null-hypothesis, while every value outside that interval will be rejected. Note that confidence intervals are again automatically given by computer output. If one would like a confidence interval against another critical level, say against a 99% critical level, one can use the level() option\n\nregress testscr str, robust level(99)\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 1, 418)         =      19.26\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.0512\n                                                Ro\n> ot MSE          =     18.581\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [99% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -2.279808   .5194892    -4.39   0.\n> 000                                             \n>        -3.624061                                \n>                     -.935556\n       _cons |    698.933   10.36436    67.44   0.\n> 000                                             \n>         672.1137                                \n>                     725.7522\n--------------------------------------------------\n> ----------------------------\n\n\n\n\n2.3.2.4 Regression with a dummy\nSometimes a regressor is binary, meaning an indicator or a dichotomous (0/1) variable. Let’s go back again to Section 2.2.2, where we created such a binary variable with small and large class sizes (\\(X=1\\) if class size is small, \\(X=0\\) if not). Other possible examples are gender (\\(X=1\\) if female, \\(X=0\\) if male) or being treated or not (\\(X=1\\) if treated, \\(X=0\\) if not). We refer to these types of variables as being dummy variables—and they are very often used in the social sciences.\nNow, suppose we have a population regression model that looks like:\n\\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_i + u_i\n\\label{eq:olsdummy}\n\\end{equation}\\]\nWhere \\(Y\\) denotes, e.g., test scores, and where \\(X\\), e.g., denotes a dummy variable for a large class (so, if \\(STR \\geq 20\\) then \\(X_i = 1\\); otherwise \\(X_i = 0\\)), so there is then only two possibilities:\n\nFor small class size there should hold that \\(X_i = 0\\) yielding that \\(Y_i = \\beta_0 + u_i\\). Namely \\(\\beta_1 \\times X_i = \\beta_1 \\times 0 = 0\\). That means automatically that the expectation of \\(Y_i\\) is the constant, being \\(\\beta_0\\). Another way or writing is that the expectation of model \\(\\ref{eq:olsdummy}\\) conditional on the fact that \\(X_i = 0\\) is \\(\\mathbb{E}(Y_i \\mid X_i = 0) = \\beta_0\\).\nFor large classes there should hold that \\(X_i = 1\\) yielding that \\(X_i = 1\\), \\(Y_i = \\beta_0 + \\beta_1 + u_i\\). This means that the expectation of model \\(\\ref{eq:olsdummy}\\) conditional on the fact that \\(X_i = 1\\) is \\(\\mathbb{E}(Y_i \\mid X_i = 1) = \\beta_0 + \\beta_1\\)\n\nSo a regression with a dummy as independent variable gives two different constants, for each group (small/large classes) one. You can interpret this as a level-effect (only the level changes, not the slope as there is none here). The interpretation of \\(\\beta_1\\) is in this case rather special and can be denoted as: \\[\\begin{equation}\n\\beta_1 = \\mathbb{E}(Y_i \\mid X_i = 1) - \\mathbb{E}(Y_i \\mid X_i = 0),\n\\end{equation}\\] Which is just the population difference in group means.\nIf we go back to our example with \\(X_i = 1\\) if \\(STR \\geq 20\\) and 0 otherwise then we get the following regression output\n\nregress testscr large, robust\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 1, 418)         =      16.34\n                                                Pr\n> ob > F          =     0.0001\n                                                R-\n> squared         =     0.0369\n                                                Ro\n> ot MSE          =     18.721\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n       large |   -7.37241   1.823578    -4.04   0.\n> 000                                             \n>        -10.95694                                \n>                    -3.787884\n       _cons |   657.3513   1.255147   523.72   0.\n> 000                                             \n>         654.8841                                \n>                     659.8184\n--------------------------------------------------\n> ----------------------------\n\n\nNow, note that this is the same output (\\(\\Delta= -7.4\\), \\(\\sigma_\\Delta = 1.82\\) and \\(t\\)-statistic is \\(-4.04\\)) as when we did the difference in means test in Section 2.2.3. To conclude, this is just another way (and much easier) to do a difference-in-means analysis. And this directly carries over for the situation when we have additional regressors."
  },
  {
    "objectID": "assessment.html#sec-specificationmodel",
    "href": "assessment.html#sec-specificationmodel",
    "title": "4  Specification and Assessment Issues",
    "section": "4.1 Specification of your model",
    "text": "4.1 Specification of your model\nA long-standing but simple question is how to decide which variables to include in a regression model. Unfortunately, the answer to this question is rather complex. A straightforward but naive approach would be to include them all. So, throw every variable in that is in your database. This, however, leads to “causal salad” (a term coined by McElreath 2020) as displayed in Figure Figure 4.1 and can actually lead to a biased estimator. One reason for this is that if you include a variable that is related to the error term then all other parameters are biased as well.\n\n\n\n\n\nFigure 4.1: Causal salad\n\n\n\n\nSo, for the final time, we return to our Californian school district data and now try to devise a specification that mimimizes the chances upon a biased estimator. So, our focus is to get an unbiased estimate of the effect on test scores of changing class size, holding constant student and school characteristics but not necessarily holding constant the budget (we do not want to control for budget as this actually governs class sizes as well).\nTo do this we need to think about what variables to include and what specification to run—and we should do this before we actually sit down at the computer. Think beforehand about your model specification and try to avoid throwing everything in (your causal salad).\nIn practice, and especially economics, most follow the following general approach to variable selection and model specification:\n\nFirst you specify a base or benchmark model. In this case that is the univariate regression of test scores on class size.\nThen you specify a range of plausible alternative models, which include additional candidate variables.\nThen you assess whether a candidate variable changes the coefficient of interest (\\(\\hat{\\beta}_1\\))? You keep focusing on the effect of class size!\nYou assess whether a candidate variable is statistically significantly different from zero; so whether it has an impact of not.\n\nUse judgment, not a mechanical recipe, meaning that variable statistically insignificant different from zero should not automatically be thrown out.\nIn all cases, do not just try to maximize \\(\\bar{R^2}\\). You focus on identifying a causal effect, not on prediction.\n\n\nConsidering the last point, it is easy to fall into the trap of maximizing the \\(\\bar{R^2}\\)—but this loses sight of our real objective, an unbiased estimator of the class size effect. Recall that a high \\(\\bar{R^2}\\) means that the regressors explain the variation in \\(Y\\). It does not mean\n\nthat you have eliminated omitted variable bias;\nthat you have an unbiased estimator of a causal effect \\((\\beta_1)\\);\nthat the included variables are statistically significant.\n\nSo, in this case, what variables would you want—ideally—to include to estimate the effect on test scores of \\(STR\\) using school district data? There is a whole set of potential relevant variables in the California class size data set, being:\n\nstudent-teacher ratio (\\(STR\\))—the variable we focus on\npercent English learners in the district (\\(PctEL\\))—as a proxy for large migrant communities\nschool expenditures per pupil—largely correlated with student-teacher ratio\nname of the district (so we could look up average rainfall, for example)\npercent eligible for subsidized/free lunch—proxies district income\npercent on public income assistance—proxies district income\naverage district income—a measure for district affuency\n\nSo, which of these variables would you want to include?\n\n\n\n\n\nFigure 4.2: Test scores versus various independent variables\n\n\n\n\nLooking at Figure 4.2, all three percentage variables (English learners, subsidized lunch, and income assistance) behave in a similar manner. But interestingly, the strongest relation is between subsidized lunch and test scores and that is at least the variable that we would like to include."
  },
  {
    "objectID": "assessment.html#sec:presentation",
    "href": "assessment.html#sec:presentation",
    "title": "3  Specification and Assessment Issues",
    "section": "3.2 Presentation of results",
    "text": "3.2 Presentation of results\nSo, we have a number of regressions (also called specifications) and we want to report them. Often, it is awkward and difficult to read regressions written out in equation form, so instead it is conventional to report them in a table. Note that reading regression estimates from computer output is even more difficult. On top of that it is ugly and contains way too much information. Try to avoid statistical computer output as much as possible—at least in your thesis. Now, regression tables should include a couple of elements:\n\nThe estimated regression coefficients.\nThe standard errors or the \\(t\\)-statistics. Having both of them is too much. Do not report \\(p\\)-value, because often they are not informaties (being \\(p = 0.000\\)).\nSome measures of fit (usually just the \\(\\bar{R^2}\\) would do).\nThe number of observations.\nSome relevant \\(F\\)-statistics, if any. Usually they are not included.\nAny other pertinent information but typically there is none.\n\nYou can find most of this information in the final estimation Table @ref(fig:catable) as presented in Stock, Watson, et al. (2003).\n\n\n\n\n\nVarious specifications of test score models\n\n\n\n\nSo, here the variable of interest (student-teacher ratio) is the first variable on top. And the table keeps focusing on that one. Moreover, specification (3) and (5) seems to be preferred as they have the highest \\(\\bar{R^2}\\), although that is perhaps of lesser importance. What we can infer from this is that the estimate for student-teacher ratio remains robust around \\(-1\\) and is significantly different from \\(0\\). Does this now mean that this effect is unbiased? Most likely not, but that is something that the next section will discuss."
  },
  {
    "objectID": "assessment.html#sec:sourcesbias",
    "href": "assessment.html#sec:sourcesbias",
    "title": "3  Specification and Assessment Issues",
    "section": "3.3 Potential sources of bias",
    "text": "3.3 Potential sources of bias\nNo to include we would like to answer the question whether there is a systematic way to assess regression studies? We already have seen that multivariate regression models have some key virtues:\n\nThey provide an estimate of the marginal effect of the variable of interest \\(X\\) on \\(Y\\).\nThey resolve the problem of omitted variable bias, if an omitted variable can be measured and included.\nThey can handle nonlinear relations (effects that vary with the \\(X\\)’s) and therefore resolve the problem of misspecification bias.\n\nStill, OLS might yield a biased estimator of the true causal effect. In other words, it might not yield valid inferences. That what you want to measure is not what you actually measure. In general there is two ways to assess statistical studies: threats to internal and threats to external validity.\n\nInternal validity: the statistical inferences about causal effects are valid for the population being studied.\nExternal validity: the statistical inferences can be generalized from the population and setting studied to other populations and setting.\n\n\n3.3.1 Threats to external validity\nSo, above we came to a (tentative) conclsusion about the impact of class size on test scores. But we have done so in the context of Californian school districts in the year 2005. Can we extend this finding and generalize class size results from California school districts to other population, for example to that of Massachusetts or Mexico in 2005? And can we do so for differences in institutional settings as there are different legal requirements concerning special education, different treatment of bilingual education, and differences in teacher characteristics across regions and countries.\nWe therefore always to be careful to transfer our finding to that of other settings. Note that this as well a special case of omitted variable bias but now outside the scope of our study (our population).\n\n\n3.3.2 Threats to internal validity\nIn applied econometrics, the following five threats to the internal validity of regression studies are usually given (in statistics there is a different framework for this, but in most cases they come down to the same thing)\n\nOmitted variable bias\nWrong functional form\nErrors-in-variables bias or measurement error\nSample selection bias\nSimultaneous causality bias\n\nAll of these imply that \\(E(u_i|X_{1i},\\ldots,X_{ki}) \\neq 0\\), in which case the OLS estimates are therefore biased.\n\n3.3.2.1 Omitted variable bias\nOmitted variable bias arises if an omitted variable is both a determinant of \\(Y\\) and a determinant of at least one included regressor. We first discussed omitted variable bias in regression with a single \\(X\\), but Omitted variable bias will arise when there are multiple \\(X\\)’s as well, if the omitted variable satisfies the two conditions above. Fortunately, there are potential solutions to omitted variable bias\n\nIf the variable can be measured, include it as an additional regressor in multiple regression;\nPossibly, use panel data in which each entity (individual) is observed more than once;\nIf the variable cannot be measured, use instrumental variables regression (for later courses);\nRun a randomized controlled experiment.\n\n\n\n3.3.2.2 Wrong functional form\nThis threat to internal validity arises if the functional form is incorrect. For example, if an interaction term is incorrectly omitted, then inferences on causal effects will be biased. There is a potential solution to functional form misspecification and that is to use the appropriate nonlinear specifications in \\(X\\) (logarithms, interactions, etc.). Sometimes this is not possible and then one has to resort to direct non-linear estimation techniques.\n\n\n3.3.2.3 Errors-in-variables bias or measurement error\nThe third threat is measurement error or sometimes know as Errors-in-variables bias. So far we have assumed that \\(X\\) is measured without error. In reality, (economic) data is often measured with error have measurement error. Especially surveys are prone to measurement error. For example recollection errors that arise with questions as “which month did you start your current job?”. Or ambiguous questions problems as “what was your income last year?” What is meant with latter: monthly or yearly income, gross or net income? Also respondents sometimes have an incentive not to answer honestly (intentionally false response problems) with questions as “What is the current value of your financial assets?” or “How often do you drink and drive?”. There are potential solutions to errors-in-variables bias, such as\n\nObtain better data, but that is bit easy\nDevelop a specific model of the measurement error process. This is only possible if a lot is known about the nature of the measurement error—for example a subsample of the data are cross-checked using administrative records and the discrepancies are analyzed and modeled.\nInstrumental variables regression.\n\n\n\n3.3.2.4 Sample selection bias\nSo far we have assumed simple random sampling of the population. In some cases, simple random sampling is thwarted because the sample, in effect, selects itself. Now, then sample selection bias arises when a selection process both influences the availability of data and if that process is related to the dependent variable. To illustrate this, I will adopt a hypothetical example given by McElreath (2020). Here we want to look at the relation between trustworthy science and newsworthy science. This example is motivated by the fact that newsworthy science (clickbait in the social media) oftentimes turns out not to be true. To given a reason why this might, we first simulate an artificial database of 400 observations of both newsworthy and trustworthy. Both variable are constructed such that they are \\(i.i.d.\\) and standard normally distributed. So, there is no relation whatsoever and, indeed, Figure @ref(fig:Rplot) shows a rather random cloud plot.\n\n\n\n\n\nRandom observations of newsworthy and trustworthy\n\n\n\n\nBut what if editors on social media have a decision rule: scientific output should be either thrustworthy or newsworthy, and preferably both. So, as a rule of thumb the select only the top 10% scientific outcomes , so the ones that score in the top 10% when both scores are added up (\\(\\text{trustworthy} + \\text{newsworthy}\\)). If we now depict the selected ones in grey in Figure @ref(fig:Rplot01), then clearly suddenly a negative relation emerges between newsworthiness and thrustworthiness. And that negative relation is caused by the selection (external) editors make. So, if there is a selection somewhere in the process, estimates of what you want to estimates can quickly become biased.\n\n\n\n\n\nNegative relation amongst the selected points\n\n\n\n\nThis process occurs more often than you might think. Consider the two following examples:\n\nAircraft noise externality. Here the question is to what extent people “value” aircraft noise (that is in a negative sense)? To aim for an answer we adopt the following empirical strategy; we collect housing prices close to Schiphol airport (say Zwanenburg) and compare them with identical houses further away (say Schagen). We have data for individual housing prices (including characteristics) since 1985. As an estimator we assess then the average mean difference between the Zwanenburg and Schagen location. Now the question is whether there is sample selection bias. And indeed there is and that is caused by the fact that humans react on their own situation based upon their preferences. In this case, they react by means of moving residence. So, those who have strong negative preference regarding aircraft noise are the first to move out Zwanenburg (if possible). So the population in both locations is not identical, instead they sorted spatially.\nReturns to education. The question here is rather straightforward and involved the monetary returns to an additional year of education. As empirical strategy we collect data of all employed workers in the Netherlands (actually this data exists and is called micro-data), including worker characteristics, years of education, and hourly wages. Our approach is here to regress \\(\\ln(Earnings)\\) on \\(YearsEducation\\) and a large set of other characteristics. Now, ignore issues of omitted variable bias and measurement error, then the question is: is there sample selection bias? And, indeed, there is again, as you only sample those people who are employed and not the unemployed (they have no current wage). And this leads to a different population than you wanted in the first place.\n\nIn there there are some potential solutions to sample selection bias and most of them deal with data issues. For example, you might want to collect the sample in a way that avoids sample selection. For example you might want to focus on those people who moved between Schagen and Zwanenburg or you include the unemployed as well in the returns to education example.\n\n\n3.3.2.5 Simultaneous causality bias in equations\nFinally, our last threat to causality is simultaneous or reverse causality bias. This means that the causal effect might go either way as in the following system\n\nCausal effect on \\(Y\\) of \\(X\\): \\(Y_i = \\beta_0 + \\beta_1 X_i + u_i\\)\nCausal effect on \\(X\\) of \\(Y\\): \\(X_i = \\gamma_0 + \\gamma_1 Y_i + v_i\\)\n\nWhere a large \\(u_i\\) means a large \\(Y_i\\), which implies large \\(X_i\\) (if \\(\\gamma_1>0\\)) and therefore, by definition, \\(corr(X_i,u_i) \\neq 0\\). Thus, \\(\\hat{\\beta_1}\\) is biased and inconsistent. In our Californian school district example it might as well be that a district with particularly bad test scores given the \\(STR\\) (negative \\(u_i\\)) receives extra resources, thereby lowering its \\(STR\\); so \\(STR_i\\) and \\(u_i\\) are then correlated\nThere are some potential solutions to simultaneous causality bias\nThe first and always the best one is to conduct a randomized controlled experiment. Because,if \\(X_i\\) is chosen at random by the experimenter, there is no feedback from the outcome variable to \\(Y_i\\) (assuming perfect compliance). Secondly, you can develop and estimate a complete model of both directions of causality. This is the idea behind many large macro models (e.g. those of the Federal Reserve Bank in the US). This is difficult in practice. Finally, you can use instrumental variables regression again to estimate the causal effect of interest (effect of \\(X\\) on \\(Y\\), ignoring effect of \\(Y\\) on \\(X\\)). But that is not for this course."
  },
  {
    "objectID": "assessment.html#sec:conclusionspec",
    "href": "assessment.html#sec:conclusionspec",
    "title": "3  Specification and Assessment Issues",
    "section": "3.4 Concluding remarks",
    "text": "3.4 Concluding remarks\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.\n\n\nStock, James H, Mark W Watson, et al. 2003. Introduction to Econometrics. Vol. 104. Addison Wesley Boston."
  },
  {
    "objectID": "assessment.html#sec-presentation",
    "href": "assessment.html#sec-presentation",
    "title": "4  Specification and Assessment Issues",
    "section": "4.2 Presentation of results",
    "text": "4.2 Presentation of results\nSo, we have a number of regressions (also called specifications) and we want to report them. Often, it is awkward and difficult to read regressions written out in equation form, so instead it is conventional to report them in a table. Note that reading regression estimates from computer output is even more difficult. On top of that it is ugly and contains way too much information. Try to avoid statistical computer output as much as possible—at least in your thesis. Now, regression tables should include a couple of elements:\n\nThe estimated regression coefficients.\nThe standard errors or the \\(t\\)-statistics. Having both of them is too much. Do not report \\(p\\)-values, because often they are not informative (as they often are reported as \\(p = 0.000\\)).\nSome measures of fit (usually just the \\(\\bar{R^2}\\) would do).\nThe number of observations.\nSome relevant \\(F\\)-statistics, if any. Usually they are not included.\nAny other pertinent information but typically there is none.\n\nYou can find most of this information in the final estimation Table @ref(fig:catable) as presented in Stock, Watson, et al. (2003).\n\n\n\n\n\nFigure 4.3: Various specifications of test score models\n\n\n\n\nSo, here the variable of interest (student-teacher ratio) is the first variable on top. And the table keeps focusing on that one. Moreover, specification (3) and (5) seems to be preferred as they have the highest \\(\\bar{R^2}\\), although that is perhaps of lesser importance. What we can infer from this is that the estimate for student-teacher ratio remains robust around \\(-1\\) and is significantly different from \\(0\\). Does this now mean that this effect is unbiased? Most likely not, but that is something that the next section will discuss."
  },
  {
    "objectID": "assessment.html#sec-sourcesbias",
    "href": "assessment.html#sec-sourcesbias",
    "title": "4  Specification and Assessment Issues",
    "section": "4.3 Potential sources of bias",
    "text": "4.3 Potential sources of bias\nNo to include we would like to answer the question whether there is a systematic way to assess regression studies? We already have seen that multivariate regression models have some key virtues:\n\nThey provide an estimate of the marginal effect of the variable of interest \\(X\\) on \\(Y\\).\nThey resolve the problem of omitted variable bias, if an omitted variable can be measured and included.\nThey can handle nonlinear relations (effects that vary with the \\(X\\)’s) and therefore resolve the problem of misspecification bias.\n\nStill, OLS might yield a biased estimator of the true causal effect. In other words, it might not yield valid inferences. That what you want to measure is not what you actually measure. In general there is two ways to assess statistical studies: threats to internal and threats to external validity.\n\nInternal validity: the statistical inferences about causal effects are valid for the population being studied.\nExternal validity: the statistical inferences can be generalized from the population and setting studied to other populations and setting.\n\n\n4.3.1 Threats to external validity\nSo, above we came to a (tentative) conclsusion about the impact of class size on test scores. But we have done so in the context of Californian school districts in the year 2005. Can we extend this finding and generalize class size results from California school districts to other population, for example to that of Massachusetts or Mexico in 2005? And can we do so for differences in institutional settings as there are different legal requirements concerning special education, different treatment of bilingual education, and differences in teacher characteristics across regions and countries.\nWe therefore always to be careful to transfer our finding to that of other settings. Note that this as well a special case of omitted variable bias but now outside the scope of our study (our population).\n\n\n4.3.2 Threats to internal validity\nIn applied econometrics, the following five threats to the internal validity of regression studies are usually given (in statistics there is a different framework for this, but in most cases they come down to the same thing)\n\nOmitted variable bias\nWrong functional form\nErrors-in-variables bias or measurement error\nSample selection bias\nSimultaneous causality bias\n\nAll of these imply that \\(E(u_i|X_{1i},\\ldots,X_{ki}) \\neq 0\\), in which case the OLS estimates are therefore biased.\n\n4.3.2.1 Omitted variable bias\nOmitted variable bias arises if an omitted variable is both a determinant of \\(Y\\) and a determinant of at least one included regressor. We first discussed omitted variable bias in regression with a single \\(X\\), but Omitted variable bias will arise when there are multiple \\(X\\)’s as well, if the omitted variable satisfies the two conditions above. Fortunately, there are potential solutions to omitted variable bias\n\nIf the variable can be measured, include it as an additional regressor in multiple regression;\nPossibly, use panel data in which each entity (individual) is observed more than once;\nIf the variable cannot be measured, use instrumental variables regression (for later courses);\nRun a randomized controlled experiment.\n\n\n\n4.3.2.2 Wrong functional form\nThis threat to internal validity arises if the functional form is incorrect. For example, if an interaction term is incorrectly omitted, then inferences on causal effects will be biased. There is a potential solution to functional form misspecification and that is to use the appropriate nonlinear specifications in \\(X\\) (logarithms, interactions, etc.). Sometimes this is not possible and then one has to resort to direct non-linear estimation techniques.\n\n\n4.3.2.3 Errors-in-variables bias or measurement error\nThe third threat is measurement error or sometimes know as Errors-in-variables bias. So far we have assumed that \\(X\\) is measured without error. In reality, (economic) data is often measured with error have measurement error. Especially surveys are prone to measurement error. For example recollection errors that arise with questions as “which month did you start your current job?”. Or ambiguous questions problems as “what was your income last year?” What is meant with latter: monthly or yearly income, gross or net income? Also respondents sometimes have an incentive not to answer honestly (intentionally false response problems) with questions as “What is the current value of your financial assets?” or “How often do you drink and drive?”. There are potential solutions to errors-in-variables bias, such as\n\nObtain better data, but that is bit easy\nDevelop a specific model of the measurement error process. This is only possible if a lot is known about the nature of the measurement error—for example a subsample of the data are cross-checked using administrative records and the discrepancies are analyzed and modeled.\nInstrumental variables regression.\n\n\n\n4.3.2.4 Sample selection bias\nSo far we have assumed simple random sampling of the population. In some cases, simple random sampling is thwarted because the sample, in effect, selects itself. Now, then sample selection bias arises when a selection process both influences the availability of data and if that process is related to the dependent variable. To illustrate this, I will adopt a hypothetical example given by McElreath (2020). Here we want to look at the relation between trustworthy science and newsworthy science. This example is motivated by the fact that newsworthy science (clickbait in the social media) oftentimes turns out not to be true. To given a reason why this might, we first simulate an artificial database of 400 observations of both newsworthy and trustworthy. Both variable are constructed such that they are \\(i.i.d.\\) and standard normally distributed. So, there is no relation whatsoever and, indeed, Figure 4.4 shows a rather random cloud plot.\n\n\n\n\n\nFigure 4.4: Random observations of newsworthy and trustworthy\n\n\n\n\nBut what if editors on social media have a decision rule: scientific output should be either thrustworthy or newsworthy, and preferably both. So, as a rule of thumb the select only the top 10% scientific outcomes , so the ones that score in the top 10% when both scores are added up (\\(\\text{trustworthy} + \\text{newsworthy}\\)). If we now depict the selected ones in grey in Figure 4.5, then clearly suddenly a negative relation emerges between newsworthiness and thrustworthiness. And that negative relation is caused by the selection (external) editors make. So, if there is a selection somewhere in the process, estimates of what you want to estimates can quickly become biased.\n\n\n\n\n\nFigure 4.5: Negative relation amongst the selected points\n\n\n\n\nThis process occurs more often than you might think. Consider the two following examples:\n\nAircraft noise externality. Here the question is to what extent people “value” aircraft noise (that is in a negative sense)? To aim for an answer we adopt the following empirical strategy; we collect housing prices close to Schiphol airport (say Zwanenburg) and compare them with identical houses further away (say Schagen). We have data for individual housing prices (including characteristics) since 1985. As an estimator we assess then the average mean difference between the Zwanenburg and Schagen location. Now the question is whether there is sample selection bias. And indeed there is and that is caused by the fact that humans react on their own situation based upon their preferences. In this case, they react by means of moving residence. So, those who have strong negative preference regarding aircraft noise are the first to move out Zwanenburg (if possible). So the population in both locations is not identical, instead they sorted spatially.\nReturns to education. The question here is rather straightforward and involved the monetary returns to an additional year of education. As empirical strategy we collect data of all employed workers in the Netherlands (actually this data exists and is called micro-data), including worker characteristics, years of education, and hourly wages. Our approach is here to regress \\(\\ln(Earnings)\\) on \\(YearsEducation\\) and a large set of other characteristics. Now, ignore issues of omitted variable bias and measurement error, then the question is: is there sample selection bias? And, indeed, there is again, as you only sample those people who are employed and not the unemployed (they have no current wage). And this leads to a different population than you wanted in the first place.\n\nIn there there are some potential solutions to sample selection bias and most of them deal with data issues. For example, you might want to collect the sample in a way that avoids sample selection. For example you might want to focus on those people who moved between Schagen and Zwanenburg or you include the unemployed as well in the returns to education example.\n\n\n4.3.2.5 Simultaneous causality bias in equations\nFinally, our last threat to causality is simultaneous or reverse causality bias. This means that the causal effect might go either way as in the following system\n\nCausal effect on \\(Y\\) of \\(X\\): \\(Y_i = \\beta_0 + \\beta_1 X_i + u_i\\)\nCausal effect on \\(X\\) of \\(Y\\): \\(X_i = \\gamma_0 + \\gamma_1 Y_i + v_i\\)\n\nWhere a large \\(u_i\\) means a large \\(Y_i\\), which implies large \\(X_i\\) (if \\(\\gamma_1>0\\)) and therefore, by definition, \\(corr(X_i,u_i) \\neq 0\\). Thus, \\(\\hat{\\beta_1}\\) is biased and inconsistent. In our Californian school district example it might as well be that a district with particularly bad test scores given the \\(STR\\) (negative \\(u_i\\)) receives extra resources, thereby lowering its \\(STR\\); so \\(STR_i\\) and \\(u_i\\) are then correlated\nThere are some potential solutions to simultaneous causality bias\nThe first and always the best one is to conduct a randomized controlled experiment. Because,if \\(X_i\\) is chosen at random by the experimenter, there is no feedback from the outcome variable to \\(Y_i\\) (assuming perfect compliance). Secondly, you can develop and estimate a complete model of both directions of causality. This is the idea behind many large macro models (e.g. those of the Federal Reserve Bank in the US). This is difficult in practice.\nFinally, you can use instrumental variables regression again to estimate the causal effect of interest (effect of \\(X\\) on \\(Y\\), ignoring effect of \\(Y\\) on \\(X\\)). But that is not for this course."
  },
  {
    "objectID": "assessment.html#sec-conclusionspec",
    "href": "assessment.html#sec-conclusionspec",
    "title": "4  Specification and Assessment Issues",
    "section": "4.4 Concluding remarks",
    "text": "4.4 Concluding remarks\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.\n\n\nStock, James H, Mark W Watson, et al. 2003. Introduction to Econometrics. Vol. 104. Addison Wesley Boston."
  },
  {
    "objectID": "multivariate.html#sec-morevar",
    "href": "multivariate.html#sec-morevar",
    "title": "3  Modeling in the Social Sciences",
    "section": "3.1 Why more independent variables?",
    "text": "3.1 Why more independent variables?\nSo, why do we include more variables? One possible answer is because it makes a better predictive model. That is, a model that is able to explain the variation in the dependent variable \\(Y\\) better.2 So, the R\\(^2\\) increases. But, as argued in Chapter 2 we are not so much interested in prediction, but more in establishing a causal relation between \\(X\\) and \\(Y\\). So, if you change \\(X\\) (and only \\(X\\)) does \\(Y\\) change and then with how much?\nAlthough economists often claim that they are the only (social-)science that focuses on causality and provides a statistical framework for that, there are other approaches to causality as well. One that is often used in other sciences is the approach of the mathematican Judea Pearl (Pearl 2009). This approach focuses on the use of Directed Acyclical Graphs (DAGs), which is a graphical visualisation of causality chains (or, what impacts what). We borrow this approach for the most simple setting as explained in Figure 3.1. Here, we go back to our Californian school district dataset again, where we still are interested in the effect of class size on school performance. So, we suppose that there is an effect from student teacher ratio on test scores as displayed with an directed arrow in Figure 3.1. We also know that the R\\(^2\\) of that regression model was rather low (5%), so by default there must be other but yet unknown factors, let us name them for now \\(U\\) (often as well referred to as unobservables), that influence test scores as well (so a directed arrow going from \\(U\\) to test scores).\n\n\n\n\n\nFigure 3.1: Unrelated omitted variables\n\n\n\n\nNow we are fine with this is as long as \\(U\\) does not impact the student teacher ratio. Then, there is still an isolated effect of student teacher ratio on class size and that is exactly what we want to measure. However, if there is a directed arrow going from \\(U\\) into \\(STR\\) as depicted by Figure 3.2, then the effect of student teacher ratio is not isolated anymore. Essentially, the effect of student teacher ratio on class size is composed out of two parts:\n\nThe causal effect on student teacher ratio on class size captured by the chain \\(\\text{STR} \\longrightarrow \\text{testscore}\\). The one we are after.\nThe impact of the unknown variables on test scores. As we have not modeled them in our regression model, the effect is captured by the chain \\(U \\longrightarrow \\text{STR} \\longrightarrow \\text{testscore}\\)\n\nEconomists refer to this phenomenon as omitted variable bias, whilst in the statistical world, this is as often called confounding variables or the confounding fork (McElreath 2020) and it, unfortunately, occurs very often.\n\n\n\n\n\nFigure 3.2: Related omitted variables\n\n\n\n\nSo, when U is a common cause for both student teacher ratio and test scores there is omitted variable bias. If we go back to our population regression model as follows: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_i + u_i,\n\\end{equation}\\] then we know that the error \\(u\\) arises because of factors that influence \\(Y\\) but are not included in the regression function; so, there are always omitted variables. But they do not always lead to bias. For omitted variable bias to occur, the omitted factor, let’s call it \\(Z\\)3, must be:\n\nA determinant of \\(Y\\) (i.e. \\(Z\\) is part of \\(u\\))\nA determinant of the regressor \\(X\\) (at least, there should hold that \\(corr(Z,X) \\neq 0\\))4\n\nThus, both conditions must hold for the omission of \\(Z\\) to result in omitted variable bias.\nNow, in our Californian district school dataset we have many more variables. One of them is variable that measures the english language ability (whether the student has English as a second language). Note that in California there are many migrants, especially from Latin-America. Now, you can readily argue that not having English as first language plausibly affects standardized test scores: so, \\(Z\\) is a determinant of \\(Y\\). Moreover, immigrant communities tend to be less affluent and thus have smaller school budgets—and, therefore, higher \\(STR\\): \\(Z\\) is most likely as well a determinant of \\(X\\).\nSo, most likely, our original estimation from Chapter 2, \\(\\hat{\\beta}_1\\), is biased (so, it is not the true causal effect). But can we say something about the direction of that bias? Yes, but the argument tends to become very quickly rather complex. In this case, note that districts with more migrant communities tend to have (i) higher class sizes and (ii) lower test scores. So, to the original estimation they add a negative effect. Thus, following this reasoning, the “true” effect must be less negative. Now, especially with negative signs this reasoning becomes rather complex, so if common sense fails you, then there is the following formula:\n\\[\\begin{equation}\n\\hat{\\beta}_1 \\overset{p}{\\to} \\beta_1 + \\frac{\\sigma_u}{\\sigma_X}\\rho_{Xu},\n\\end{equation}\\] where you should focus on the sign of the correlation between \\(X\\) and the regression residual \\(u\\) (all standard errors, \\(\\sigma\\), are always positive by default). Now, the first least squares assumption states that \\(\\rho_{Xu} = 0\\)—no correlation between the regressor and the regression residual. But now there is correlation because of omitted variable bias. And because there is a negative relation between immigrants communities and school performance, \\(\\rho_{Xu}\\) should be negative. Furthermore, because the original estimation from Chapter 2 was already negative to begin with the “true” \\(\\beta_1\\) should be less negative. In conclusion, districts with more English learning students (i) do worse on standardized tests and (ii) have bigger classes (smaller budgets), so ignoring the English learning factor results in overstating the class size effect (in an absolute sense).\nYou might wonder whether this is actually going on in the Californian district school data. To see this, Figure 3.3 offers a cross tabulation of test scores by class size and percentage English learners.\n\n\n\n\n\nFigure 3.3: Cross tabulation of test scores by class size and percentage English learners\n\n\n\n\nNow, the table depicted in Figure @ref(fig:omitca) is complex in its various dimensions. We have our two categories of class size (small and large), together with the difference in test scores, but we now stratify this by four categories of percentage English learners. There are several important observations to make here:\n\ndistricts with fewer English Learners (so less migrants) have on average higher test scores (what we assumed above);\ndistricts with fewer English Learners (so less migrants) have smaller classes (what we assumed above);\nthe effect of class size with comparable percentages English learners is still (mostly negative), but not as much as we compare for all districts together (the Difference-column). This confirms our reasoning that our original estimate was too negative.\n\nNo, as already mentioned above, omitted variable bias occurs very often. So, how to correct for this such that the bias disappaers. In general, there are three strategies:\n\nwe can run a randomized controlled experiment in which treatment (\\(STR\\)) is randomly assigned: then percentage English learners (\\(PctEL\\)) is still a determinant of test scores, but by construction \\(PctEL\\) should be uncorrelated with \\(STR\\). Unfortunately, is it very difficult to randomize class size in reality and often this strategy is just not attainable as being too costly or unethical (this accounts for all sciences);\nwe can adopt the cross tabulation approach of above, with finer gradations of \\(STR\\) and \\(PctEL\\). Then by construction, within each group all classes have the same \\(PctEL\\) so we control for \\(PctEL\\). A disadvantages is that one needs many observations, especially when one wants to stratify upon other variables as well;\nfinally, and perhaps the easiest approach, we can use a population regression model in which the omitted variable (\\(PctEL\\)) is no longer omitted. We just include \\(PctEL\\) as an additional regressor in a multiple regression model. This is what the next section deals with. Obviously, a disadvantage of this approach is that you need observations for the omitted variable (but that also accounts for method 2)."
  },
  {
    "objectID": "appendix.html#reviewing-probability",
    "href": "appendix.html#reviewing-probability",
    "title": "Appendix A — Reviewing probability and statistics",
    "section": "A.1 Reviewing probability",
    "text": "A.1 Reviewing probability\n\nA.1.1 Probability\nAs a definition of probability we use the concept of empirical probability which is the proportion of time that something (a specific outcome or event \\(X\\)) occurs in the long-run of total events. Usually it is give by:\n\\[\\begin{equation}\n\\text{probability} = p = \\frac{\\text{Number of times specific event $X$ happens}}{\\text{Total amount of events that can happen}}\n\\end{equation}\\]\nNow probabilities are defined by a set of definations (axioms). These are:\n\nProbabilities, \\(p\\), are always between 0 and 1. So, \\(0 \\leq p \\leq 1\\)\nIf something does not happen, then \\(p = 0\\)\nIf something always happens, then \\(p = 1\\)\nProbabilities for the total amount of events always add up to \\(1\\). So, if the probability that something happens is \\(p\\), then the probabilities that it will not happen is \\(1 -p\\) (see that \\(p + 1 - p = 1\\))\n\n\n\nA.1.2 Population & random variables\nIn general we see a population as the the group or collection of all possible entities of interest (school districts, inhabitants of the Netherlands, homeowners) and we will think of populations as infinitely large (\\(\\infty\\)). From this population we then sample specific observations. This sample contains then a random variable \\(Y\\), which denotes a characteristics of the entity (district average test score, prices of houses, prices of meat). An important feature is that the sample characteristics are unknown, that is before measurement (\\(y\\)), after measurement the sample is know and is called data.\nSo, a random variable (also called a stochastic variable) is a mathematical formalization of something that depends on random outcomes. Unfortunately, randomness is not clearly defined and depends on specific scientific philosophical schools. The scientific philosophical school we implicitly assume in this course—and, in fact, in most statistical courses—is that of frequentist statistics. Here we assume that all things we measure are intrinsically random. In fact, this is an ontological argument—in other words, what are our beliefs in the state of the world. Because all things we measure are random, every time we measure something our measurements are (slightly) different. However, the more we measure, the more precise we know something. But there is still randomness.\nIn general, there are two types of random variables. First, there are discrete random variables, where outcomes can be counted, such as \\(0, 1, 2, 3, \\ldots\\) and continuous random variables, where outcomes can be any real number.1\n\n\nA.1.3 Distribution functions\nRandom variables are governed by distribution functions which are mathematical function that provides the probabilities of occurrence of all different possible outcomes of the random variable experiment: e.g. for a discrete distribution, \\(f(x) = \\Pr(Y = y)\\) \\(\\forall y\\). Or, in other words, the distribution function maps discrete outcomes to probabilities. For continuous distribution function, this is not possible as there an infinite number of possible outcomes, so that means that for each \\(y\\) must yield \\(\\Pr(Y = y) = 0\\). Therefore, with continuous distribution, often the cumulative distribution function is used, which is defined as \\(F(x) = \\Pr(Y \\leq y)\\). This is why we always use the surface of areas under the normal distribution function.\nDistribution functions have characteristics of which the most important are: - The mean, also known as the expected value (or expectation) of \\(Y\\). It is usually denoted as \\(E(Y) = \\mu_Y\\) and can as well be interpreted as the long-run average value of \\(Y\\) over repeated realizations of \\(Y\\): \\(\\frac{1}{n}\\sum_{i = 1}^{n}y_{i}\\)\n\nThe variance, which is denoted as \\(E(Y - \\mu_Y)^2\\). Usually it is associated with \\(\\sigma^2_Y\\) and provides a measure of the squared spread of the distribution. If we take the square root then we have the standard deviation (\\(=\\sqrt{\\text{variance}} = \\sigma_Y\\)). For a symmetrical normal distribution, it is useful to know that the mean plus or minus 1 time the standard deviation governs about \\(2/3\\) of all probability while the mean plus or minus 2 times the standard deviation governs about 95% of all probability associated with that random variable.\n\nNow in statistics we are usually related in relations between random variables, and luckily most entities in real life are related. To capture the relation we need two concept, joint distributions and covariance. If we assume that that random variables \\(X\\) and \\(Z\\) have a joint distribution then the covariance between \\(X\\) and \\(Z\\) is: \\[\\begin{equation}\ncov(X,Z) = E[(X- \\mu_X)(Z- \\mu_Z)] = \\sigma_{XZ}\n\\end{equation}\\]\nNote that this covariance is a measure of the linear association between \\(X\\) and \\(Z\\) and that its units are units of \\(X\\) times units of \\(Z\\). \\(cov(X,Z) > 0\\) means a positive relation between \\(X\\) and \\(Z\\), and finally if \\(X\\) and \\(Z\\) are independently distributed, then \\(cov(X,Z) = 0\\). Note that the covariance of a random variable with itself is just its variance: \\[\\begin{equation}\ncov(X,X) = E[(X-\\mu_X)(X - \\mu_X)] = E[(X - \\mu_X)^2] = \\sigma^2_X\n\\end{equation}\\]\nHowever, the covariance is still measured in the units of \\(X\\) and \\(Z\\). To correct for that, we often use the correlation coefficient, defined by: \\[\\begin{equation}\ncorr(X,Z) = \\frac{cov(X,Z)}{\\sqrt{var(X)var(Z)}} = \\frac{\\sigma_{XZ}}{\\sigma{_X}\\sigma{_Z}} = r_{XZ}\n\\end{equation}\\] where \\(-1 \\leq corr(X,Z) \\leq 1\\), a \\(corr(X,Z) = 1\\) means perfect positive linear association, a \\(corr(X,Z) = -1\\) means perfect negative linear association, and a \\(corr(X,Z) = 0\\) denotes no linear association.\n\n\n\n\n\nThe correlation coefficient and the relation between observed \\(x\\) and \\(y\\)\n\n\n\n\nIt is very important to notice that a correlation coefficient measures linear association. So, \\(corr(X,Z) = 0\\) does not mean that there is no relation, there is only no linear correlation. This is illustrated by figure @ref(fig:corrcoef). In panel (a) there is clearly a positive relation, and panel (b) shows a negatve relation, but what about panel (d)? Here, the correlation coefficient is 0, just as in panel (c), but obviously there is a clear non-linear relation.\n\n\nA.1.4 Conditional distributions and conditional means\nAn important notion is applied statistics (and in applied econometrics) is that of the condition distributions, that is the distribution of \\(Y\\), given value(s) of some other random variable, \\(X\\). For example, in our California school example, we might want to know something about the distribution of test scores, given that \\(STR < 20\\). Therefore, we use the concept of conditional mean, which is defined as the mean of a conditional distribution = \\(E(Y|X = x)\\). Note here the \\(|\\) symbol—it means given that a random variable \\(X\\) is measured with \\(x\\). As an example: \\(E(Test scores|STR < 20)\\) which denotes the mean of test scores among districts with small class sizes. We also denote this with the conditional mean.\nNow, if we want to know the difference in means, then we can denote that with \\[\\begin{equation}\n\\Delta = E(Test scores|STR < 20)-  E(Test scores|STR \\geq 20),\n\\end{equation}\\] which is a very important concept in applied economics as it resembles two groups of which one received treatment and the other one not. Other examples of the use of conditional means: difference in wages among gender (glass ceiling) and mortality rate differences between those who are treated and those who are. Now if \\(E(X|Z)\\) is constant, then \\(corr(X,Z) = 0\\). We then say that \\(X\\) and \\(Z\\) are independent."
  },
  {
    "objectID": "appendix.html#secssampling",
    "href": "appendix.html#secssampling",
    "title": "Appendix A — Reviewing probability and statistics",
    "section": "A.2 Sampling in frequentist statistics",
    "text": "A.2 Sampling in frequentist statistics\nSo, we mentioned above that we sample from the population which is assumed to be infinitely large. Now, how does this sampling then carry over to statistics. For that we need a statistical framework based on random sampling. First, choose an individual, \\(i\\), (or district, firm, etc.) at random from the population. Now, prior to sample selection, the value of what we want to know \\(Y_i\\) is random because the individual selected is random. Once the individual is selected and the value of \\(Y\\) is observed, then \\(Y\\) is just a number—not random anymore but data. And then we say it has the value \\(y\\). Hence the notation \\(\\Pr(Y = y\\)).\nIf we sample multiple entities, the we construct a data set that looks like \\((y_1, y_2,\\dots, y_n)\\), where \\(y_i\\) = value of \\(y\\) for the \\(i^{\\mathrm{th}}\\) individual (district, entity) sampled. Again the lower case here denotes a realisation—the dataset. Now, we want to know what the distribution of the random variables \\(Y_1\\),, \\(Y_n\\) is under simple random sampling. Note that because entities (say individuals) #1 and #2 are selected at random, the value of Y\\(_1\\) has no information content for \\(Y_2\\). Thus: \\(Y_1\\) and \\(Y_2\\) are independently distributed. And if \\(Y_1\\) and \\(Y_2\\) come from the same distribution, that is, \\(Y_1\\), \\(Y_2\\) are identically distributed, then we say that, under simple random sampling, Y\\(_1\\) and Y\\(_2\\) are independently and identically distributed (i.i.d.). More generally, under simple random sampling, Y\\(_i\\), \\(i = 1,\\ldots, n\\), are i.i.d—this term always come back in all sorts of statistics.\nThis simple framework already allows rigorous statistical inferences about, e.g., the mean \\(\\bar{Y}\\) of population distributions using a sample of data from that population. The next subsection does this because the mean is not only an important, but mainly because the results immediately can be transferred to the regression context.\n\nA.2.1 The sampling distribution of \\(\\bar{Y}\\)\nNow because \\(\\bar{Y}\\) is formed by a sample of \\(\\{Y_i\\}'s\\) it is as well a random variable, and its properties are determined by the sampling distribution of \\(\\bar{Y}\\). Again, we assume that the elements in the sample are drawn at random, that thus the values of \\((Y_1,\\ldots, Y_n)\\) are random, and that thus functions of \\((Y_1,\\ldots, Y_n)\\), such as \\(\\bar{Y}\\), are random: had a different sample been drawn, they would have taken on a different value. Finally, the distribution of \\(\\bar{Y}\\) over different possible samples of size \\(n\\) is called the sampling distribution of \\(\\bar{Y}\\), which underpins all of frequentists statistics.\n\n\nA.2.2 Example: simple binomial random variables\nSo how this work. Let’s take the easiest statistical example: coin flipping, where the coin is this case is notoriously biased. Suppose the random variable \\(Y\\) takes on 0 (head) or 1 (tails) with the following probability distribution, \\(\\Pr[Y = 0] = 0.22\\), \\(\\Pr(Y =1) = 0.78\\). Then the mean and variance are given by: \\[\\begin{eqnarray}\n\\mu_{Y} &=& p \\times 1 + (1-  p) \\times 0 = p = 0.78\\\\\n\\sigma^2_Y&=& E[Y - \\mu_{Y}]^2 = p(1 - p) \\\\\n&=& 0.78 \\times 0.22 = 0.17\n\\end{eqnarray}\\] But this is only one throw (\\(n= 1\\)). We would like to have multiple observations to derive at our sampling distribution of \\(\\bar{Y}\\), which we assume to depend on the number of throws, \\(n\\).\nConsider therefore first the case of \\(n = 2\\). The sampling distribution of \\(\\bar{Y}\\) is, \\[\\begin{eqnarray}\n\\Pr(\\bar{Y}  = 0) &= 0.22^2 &= 0.05 \\\\\n\\Pr(\\bar{Y}  = 1/2) &=  2 \\times 0.22 \\times 0.78 &= 0.34\\\\\n\\Pr(\\bar{Y}  = 1) &= 0.78^2 &= 0.61.\n\\end{eqnarray}\\]\nbut this start to become boring as \\(n\\) increases. Therefore, we turn to STATA. Let’s first check for \\(n = 2\\).\n\nset obs 10000\ngenerate Y = rbinomial(2,0.78)/2 \nhist(Y), fraction\n\nThe first line of this code sets the number of experiments (called obs). So, I throw a coin twice, for 10000 times in a row. The second line generates the outcomes, which in this case are no heads (0), head once (1), or two heads (2). To arrive at probabilities I divide by 2 again. Finally, the third line gives a history of fraction (not counts). And this provides the following STATA output.\n\n\n\n\n\nSampling distribution when you throw a coin two times\n\n\n\n\nBut what if I do this a 100 times, so \\(n = 100\\)?\n\nclear\nset obs 10000\ngenerate Y = rbinomial(100,0.78)/100 \nhist(Y), fraction\n\nNote that fhe first line now clears STATA’s memory as I actually create a new dataset. The histogram can be seen now in Figure @ref(fig:coin100).\n\n\n\n\n\nSampling distribution when you throw a coin 100 times\n\n\n\n\nBut isn’t this strange. Apart from some omitted bars (which is a fluke of STATA), we can now observe a couple of things. First, the average of the distribution of Figure @ref(fig:coin100) is very close to 0.78, which is the actual probability that our biased coin provides tails. But, more importantly the distribution starts to look like a symmetric normal distribution. And we started with a binomial distribution!\nThis is the result of two amazing statistical theorems:\n\nThe law of large numbers: the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed. That is, if there a no biases in the experiment itself. It also means that with more experiments the precision become better, or the variance decreases. In general this implies that:\n\n\\(\\bar{Y}\\) is an unbiased estimator of \\(\\mu_Y\\) (that is, \\(E(\\bar{Y}) = \\mu_Y\\))\nvar(\\(\\bar{Y}\\)) is inversely proportional to \\(n\\)\nThus the standard error associated with \\(\\bar{Y}\\) is \\(\\sqrt{\\frac{\\sigma_Y^2}{n}}\\) (that means that with larger samples there is less uncertainty but see the square-root law)\n\nThe Central Limit Theorem: when independent random variables are summed up, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed. So \\(\\bar{Y}\\) is approximately distributed \\(N(\\mu_Y,\\frac{\\sigma^2_Y}{n})\\)\n\nWhen working with standardized variables then \\(\\bar{Y} = \\frac{\\bar{Y}-\\mu_Y}{\\sigma_Y/\\sqrt{n}}\\) is approximately distributed as \\(N(0,1)\\) \nThe larger is \\(n\\), the better is the approximation. And this already holds for \\(n \\geq 50\\). So with a reasonable amount of observations, the mean of i.i.d. variables is normally distributed"
  },
  {
    "objectID": "multivariate.html#sec-multivariate",
    "href": "multivariate.html#sec-multivariate",
    "title": "3  Modeling in the Social Sciences",
    "section": "3.2 Multivariate regression analysis",
    "text": "3.2 Multivariate regression analysis\nSo, if we have information about an important omitted variable, as in the case of the size of migrant communities in the example above, then we can use that information in a multivariate population regression model. In the case of two regressors, that would look like: \\[\\begin{equation}\nY_i =\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i, i=1,\\ldots,n\n\\end{equation}\\] where:\n\n\\(Y\\) is the dependent variable\n\\(X_1\\), \\(X_2\\) are the two independent variables (regressors)\n\\((Y_i, X_{1i}, X_{2i})\\) denote the i\\(^{\\mathrm{th}}\\) observation on \\(Y\\), \\(X_1\\), and \\(X_2\\).\n\\(\\beta_0\\) is the unknown population intercept\n\\(\\beta_1\\) is the effect on \\(Y\\) of a change in \\(X_1\\), holding \\(X_2\\) constant\n\\(\\beta_2\\) is the effect on \\(Y\\) of a change in \\(X_2\\), holding \\(X_1\\) constant\n\\(u_i\\) is the the regression error (omitted factors)\n\nNow, the only element that changes is the interpretation of a parameter, say \\(\\beta_1\\). In this case, it can still be seen as a ‘slope’ parameter, although now in 3-dimensional space, but it now states specifically that the other parameter(s) should be held constant. This does facilitate the interpretation of \\(\\beta_1\\). For example, consider changing \\(X_1\\) by \\(\\Delta X_1\\) while holding \\(X_2\\) constant. That means that the population regression line before the change looks like: \\[\\begin{equation}\nY = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_{2},\n\\end{equation}\\] whilst the population regression line, after the change, looks like: \\[\\begin{equation}\nY + \\Delta Y = \\beta_0 + \\beta_1 (X_{1} + \\Delta X_1) + \\beta_2 X_{2}\n\\end{equation}\\] And if we take the difference, then the interpretation of \\(\\beta_1\\) boils down again to the marginal effect:\\(\\Delta Y = \\beta_1 \\Delta X_1\\). Or, \\(\\beta_1 = \\frac{\\Delta Y}{\\Delta X_1}\\) when holding \\(X_2\\) constant and, likewise, \\(\\beta_2 = \\frac{\\Delta Y}{\\Delta X_2}\\) when holding \\(X_1\\) constant. \\(\\beta_0\\) is now the predicted value of \\(Y\\) when \\(X_1 = X_2 = 0\\)\nIf we do this for the the Californian school district data, then the original population regression line was estimated as: \\[\\begin{equation}\n\\widehat{TestScore} = 698.9- 2.28 STR\n\\end{equation}\\] But if we now include include percent English Learners in the district (\\(PctEL\\)) to the model then the population regression ‘line’ becomes: \\[\\begin{equation}\n\\widehat{TestScore} = 686.0- 1.10 STR - 0.65  PctEL\n\\end{equation}\\]\nClearly, the effect of student teacher ratio becomes smaller (that is, less negative). That indicates that the original regression suffers from omitted variable bias. And this is what should happen as reasoned above. The STATA syntax for a multivariate regression model is now rather straightforward. You basically add another to the regression equation, as below:\n\nreg testscr str el_pct, robust\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 2, 417)         =     223.82\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.4264\n                                                Ro\n> ot MSE          =     14.464\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -1.101296   .4328472    -2.54   0.\n> 011                                             \n>         -1.95213                                \n>                    -.2504616\n      el_pct |  -.6497768   .0310318   -20.94   0.\n> 000                                             \n>         -.710775                                \n>                    -.5887786\n       _cons |   686.0322   8.728224    78.60   0.\n> 000                                             \n>         668.8754                                \n>                      703.189\n--------------------------------------------------\n> ----------------------------\n\n\nObviously, the effect of student teacher ration reduces with 50%! The interpretation of the rest of the statistical output, such as measures of fit and test statistics, follows in the subsections below.\n\n3.2.1 Measures of fit for multiple regression\nIn multivariate regression models, there are four commonly used measures of fit, three of them we have seen before.\n\nThe standard error of regression or the \\(SER\\) denotes the standard deviation of \\(\\hat{u}_i\\) and includes a degrees of freedom correction (degrees of freedom in this case denotes how many variables your have used and typically is denoted with \\(k\\). The \\(SER\\) is defined as: \\[\\begin{equation}\nSER = s_{\\hat{u}} = \\sqrt{\\frac{1}{n-k-1} \\sum_{i=1}^n \\hat{u}^2_i},\n\\end{equation}\\] where \\(k\\) is the number of variables (including the constant) use in the regression model. Note that in the univariate regression model \\(k=2\\)—the slope coefficient and the constant.\nThe root mean square error (RMSE) which denotes as well the stdandard deviation of \\(\\hat{u}_i\\) but now without degrees of freedom. We have seen this before in Eq. @ref(eq:rmse) and does not change.\nThe \\(R^2\\) which measures the fraction of variance of \\(Y\\) explained by the independent variables. Again, we have seen this one before\nThe adjusted “adjusted \\(R^2\\)” (or \\(\\bar{R}^2\\)) which is equal to the \\(R^2\\) with a degrees-of-freedom correction that adjusts for estimation uncertainty. It can be formulated as: \\[\\begin{equation}\n\\bar{R}^2 = 1 - \\frac{n-1}{n-k-1}\\frac{SSR}{TSS}.\n\\end{equation}\\] Note that using this formulation, in a multivariate setting, it always should hold that \\(\\bar{R}^2 <R^2\\). But why do we care so much for the amount of variables that we use (denoted with \\(k\\)). That is because with each additional variable the \\(R^2\\) always increases. And it is essential to notice that when \\(k=n\\), the \\(R^2 = 1\\), so there is no variation left anymore. But that feels like cheating. You just have a parameter for each observation that you have, but such a model must be meaningless. Therefore, you always want to correct for the number of variables that you use.\n\nIn our Californian school district example that would amount to the following two outcomes. First for the univariate model: \\[\\begin{eqnarray}\nTestScore &= &698.9- 2.28  STR \\\\\n&&R^2 = .05, SER = 18.6\n\\end{eqnarray}\\]\nAnd then for the multivariate model.\n\\[\\begin{eqnarray}\nTestScore &=& 686.0 - 1.10  STR - 0.65 PctEL \\\\\n&&R^2=.426, \\bar{R}^2=0.424, SER = 14.5\n\\end{eqnarray}\\]\nNote that all measures of fit increases. The \\(\\bar{R}^2\\) now indicates that 42% of all variation in test scores are explained. That is a huge improvement compared to the 5% explanatory power of the univariate case. That indicates that the \\(PctEL\\) strongly correlates with testscores. But again, we are not so much interested in prediction, but want to find the causal impact of class size instead. Another thing to notice here is that the \\(R^2\\) and the \\(\\bar{R}^2\\) are very close. That is because the number of variables is much smaller than the number of observations \\(k \\ll n\\), so that the impact of \\(k\\) is not very big.\nA final remark concerns a peculiarity of STATA. In the regression output of above, STATA does not provide the \\(\\bar{R}^2\\). That is because of the option , robust. Without that option, the regression output would give both measures of fit.\n\nreg testscr str el_pct\n\n      Source |       SS           df       MS     \n>  Number of obs   =       420\n-------------+----------------------------------  \n>  F(2, 417)       =    155.01\n       Model |  64864.3011         2  32432.1506  \n>  Prob > F        =    0.0000\n    Residual |  87245.2925       417  209.221325  \n>  R-squared       =    0.4264\n-------------+----------------------------------  \n>  Adj R-squared   =    0.4237\n       Total |  152109.594       419  363.030056  \n>  Root MSE        =    14.464\n\n--------------------------------------------------\n> ----------------------------\n     testscr | Coefficient  Std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -1.101296   .3802783    -2.90   0.\n> 004                                             \n>        -1.848797                                \n>                    -.3537945\n      el_pct |  -.6497768   .0393425   -16.52   0.\n> 000                                             \n>        -.7271112                                \n>                    -.5724423\n       _cons |   686.0322   7.411312    92.57   0.\n> 000                                             \n>         671.4641                                \n>                     700.6004\n--------------------------------------------------\n> ----------------------------\n\n\nAnother option is to specifically ask STATA to display the \\(\\bar{R}^2\\) by invoking the command display, then some text (text always goes between strings), and finally the thing you want to see (e(r2_a)). Something like:\n\ndisplay \"adjusted R2 = \" e(r2_a)\n\nadjusted R2 = .42368043\n\n\n\n\n3.2.2 The least squares assumptions for multivariate regression\nThus, it is easy to add other variables, so that the multivariate regression model now looks like: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i}+\\ldots + \\beta_k X_{ki}+u_i, i=1,\\ldots,n\n\\end{equation}\\] Suppose we are interested in \\(\\beta_1\\). How do we then know whether our estimation \\(\\hat{\\beta}_1\\) is unbiased? For that we again resort to our least squares assumption, some of them will change a bit and we have to add a fourth one:\n\nThe first least squares assumptions changes slightly. Now, we state that the conditional distribution of \\(u\\) given all \\(X_i\\)’s has mean zero, that is, \\(E(u|X_1 = x_1,\\ldots, X_k = x_k) = 0\\). So, \\(\\beta_1\\) is biased even another variable \\(X_k\\) is correlated with \\(u\\). So, only of the variables \\(X_i\\) has to be correlated with \\(u\\) and then all parameters are to a certain extent biased.\nThe second least squares assumption is more or less as before but now in a multivariate fashion, so the whole set of (X\\(_{1i}\\),,X\\(_{ki}\\),Y\\(_i\\)), with \\(i =1,\\ldots,n\\), should be independent and identical distributed (\\(i.i.d\\)).\nThe third least squares assumptions states again that large outliers are rare for all variables included, so for all \\(X_1,\\ldots, X_k\\), and \\(Y\\).\nThe fourth assumption is new and states that there is no perfect multicollinearity. We discuss this further below.\n\n\n3.2.2.1 Multicollinearity\nMulticollinearity comes in two flavours; perfect and imperfect. The former functions as a multivariate least squares assumptions whilst the latter oftentimes gives the largest problems. We start the discussion with perfect multicollinearity and then continue with the case of imperfect multicollinearity.\n\n3.2.2.1.1 Perfect multicollinearity\nThe official definition of perfect multicollinearity is that there is a perfect linear combination amongst your variables. That means that there is not one optimal solution, but instead many (actually, infinitely many) more. Let us illustrate this by the following example. Suppose you include \\(STR\\) twice in your regression. Now, STATA produces then the following output:\n\nreg testscr str str el_pct, robust\n\nnote: str omitted because of collinearity.\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 2, 417)         =     223.82\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.4264\n                                                Ro\n> ot MSE          =     14.464\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -1.101296   .4328472    -2.54   0.\n> 011                                             \n>         -1.95213                                \n>                    -.2504616\n         str |          0  (omitted)\n      el_pct |  -.6497768   .0310318   -20.94   0.\n> 000                                             \n>         -.710775                                \n>                    -.5887786\n       _cons |   686.0322   8.728224    78.60   0.\n> 000                                             \n>         668.8754                                \n>                      703.189\n--------------------------------------------------\n> ----------------------------\n\n\nSee that STATA drops one of the \\(STR\\) variables. But why is that? See that the impact of twice this variable should be equivalent to: \\[\\begin{equation}\n\\beta_1 STR = w_1 \\beta_1 STR + w_2 \\beta_1 STR = (w_1 + w_2) \\beta_1 STR ,\n\\end{equation}\\] where \\(w_1\\) and \\(w_2\\) are weights chosen such that they satisfy the condition that \\(w_1 + w_2 = 1\\). But there is an infinite number of combinations that satisfy this condition! So, there is not an optimal solution and one of these variables should be dropped.\nThe violation of no perfect multicollearity often occurs when using dummies (see again Subsection @ref(sec:dummy)). Suppose that we regress \\(TestScore\\) on a constant, \\(D\\), and \\(B\\), where:\\(D_i =1\\) if \\(STR \\leq 20\\), \\(=0\\) otherwise ; \\(B_i =1\\) if \\(STR>20\\), \\(= 0\\) otherwise. This example is slightly more complex as there is no perfect correlation between \\(B\\) and \\(D\\). However, the model contains as well a constant and that create a perfect linear combination, namely \\(B_i + D_i = 1\\) and that is the definition of a constant (\\(\\beta_1 \\times 1\\)), so there is perfect multicollinearity in the model.\nA different way of seeing this is to consider the following regression model and note that by definition \\(D_i = 1- B_i\\):\n\\[\\begin{align}\nTestscr_i &= \\beta_0 + \\beta_1 D_i + \\beta_2 B_i + u_i\\\\\n          &= \\beta_0 + \\beta_1 D_i + \\beta_2 (1 - D_i) + u_i\\\\\n          &= (\\beta_0 + \\beta_2) + (\\beta_1 - \\beta_2) D_i + u_i.\n\\end{align}\\] Suppose that the true constant equals \\(680\\) and the slope parameter equals \\(7\\). Then it is not difficult to see that there is an infinite amount of combinations possible of values for \\(\\beta_0, \\beta_1\\) and \\(\\beta_2\\) that leads to these numbers.\nNow, this example is a special case of the so-called dummy variable trap. Suppose you have a set of multiple binary (dummy) variables, which are mutually exclusive and exhaustive—that is, there are multiple categories and every observation falls in one and only one category (e.g., infant, child, teenager, adult). If you include all these dummy variables and a constant, you will have perfect multicollinearity—the dummy variable trap.\nThere are possible solutions to the dummy variable trap:\n\nOmit one of the groups (e.g., the infants), or\nOmit the intercept\n\nIn most cases you omit one of the groups (typically the one with the lowest value). This give the constant then the interpretation of the average value of that left-out category, where the dummy variables are then the relative differences to that left-out category.\nNow, perfect multicollinearity usually reflects a mistake in the definitions of the regressors, or an oddity in the data. And, usually this is not a problem, because if you have perfect multicollinearity, your statistical software will let you know—either by crashing or giving an error message or by “dropping” one of the variables arbitrarily and very often the solution to perfect multicollinearity is to modify your list of regressors such that you no longer have perfect multicollinearity.\n\n\n3.2.2.1.2 Imperfect multicollinearity\nNow imperfect and perfect multicollinearity are quite different despite the similarity of the names. Imperfect multicollinearity, namely, occurs when two or more regressors are very highly correlated. And if two regressors are very highly correlated, then their scatterplot will pretty much look like a straight line—they are collinear—but unless the correlation is exactly \\(\\pm\\) 1, that collinearity is imperfect. What this implies is that one or more of the regression coefficients will be imprecisely estimated. Why is that? That is because of the definition of the coefficient in a multivariate regression model. Namely, the coefficient on \\(X_1\\) is the effect of \\(X_1\\) holding \\(X_2\\) constant, but if \\(X_1\\) and \\(X_2\\) are highly correlated, then there is very little variation in \\(X_1\\) once \\(X_2\\) is held constant. That means that the data are pretty much uninformative about what happens when \\(X_1\\) changes but \\(X_2\\) doesn’t, so the variance of the OLS estimator of the coefficient on \\(X_1\\) will be large. And this results in large standard errors for one or more of the OLS coefficients. But often this is very hard to detect. Are standard errors high because of imperfect multicollinearity, because the number of observations is very low, or because there is large variation in the data? The answer to this unfortunately boils down to reasoning, but before you start estimating your statistical models it always good to look at scatterplots and correlations between variables.\nBut what is a high correlation? With a reasonable amount of observations all correlations below \\(0.9\\) can be considered fine. In practice, only correlations between variables higher than say \\(0.95\\) start to impose problems.\n\n\n\n\n3.2.3 Testing with multivariate regression models\n\n3.2.3.1 Hypothesis tests and confidence intervals for a single coefficient in multiple regression\nRecall from Subsection @ref(sec:unitesting) that for hypothesis testing in a classical statistical framework we make use of the fact that \\(\\frac{\\hat{\\beta}_1- E(\\hat{\\beta}_1)}{\\sqrt{var(\\hat{\\beta}_1)}}\\) is approximately distributed as \\(N(0,1)\\) according to the Central Limit theorem. Thus hypotheses on \\(\\beta_1\\) can be tested using the usual \\(t\\)-statistic, and confidence intervals are constructed as \\(\\{\\hat{\\beta}_1 \\pm 1.96 SE (\\hat{\\beta}_1)\\}\\). And this finding carries over to the multivariate setting where for \\(\\beta_2,\\ldots, \\beta_k\\) we make use of the same framework. One thing to keep in mind is that \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) are generally not independently distributed—so neither are their \\(t\\)-statistics (more on this later).\nNow, if we return to our Californian school district data set then we find that for the univariate case holds:\n\\[\\begin{equation}\nTestScore =\\underbrace{698.9}_{10.4} - \\underbrace{2.28}_{0.52}  STR,\n\\end{equation}\\]\nAnd the population regression “line” for the multivariate case is estimated as: \\[\\begin{equation}\nTestScore = \\underbrace{686.0}_{8.7} - \\underbrace{1.10}_{0.43} STR - \\underbrace{0.650}_{0.031} PctEL\n    (\\#eq:testmulti)\n\\end{equation}\\]\nRemember, the coefficient on \\(STR\\) in Eq. @ref(eq:testmulti) is the effect on \\(TestScores\\) of a unit change in \\(STR\\), holding constant the percentage of English Learners in the district. The corresponding 95% confidence interval for coefficient on \\(STR\\) in (2) is \\(\\{-1.10 \\pm 1.96 \\times 0.43\\} = (-1.95,-0.26)\\). And the \\(t\\)-statistic testing \\(\\beta_{STR} = 0\\) is \\(t = -1.10/0.43 = -2.54\\), so we reject the null-hypothesis at the 5% significance level. More evidence for the strength of the \\(PctEL\\) variable can be seen from the fact that, under the null-hypothesis of \\(\\beta_2 = 0\\), the following must hold: \\(t\\text{-statistic} = \\frac{\\hat{\\beta_1}}{\\sigma_{\\hat{\\beta_1}}} = \\frac{0.65}{0.03} = 21.7\\), which is a very high number for a \\(t\\)-statistic.\n\n\n3.2.3.2 Tests of joint hypotheses\nSo, testing of single coefficients is just as before. Now in the Californian school district dataset there is as well a variable called \\(Expn\\) denoting the expenditures per pupil. Consider the following population regression model: \\[\\begin{equation}\nTestScore_i = \\beta0 + \\beta_1 STR_i + \\beta_2 Expn_i + \\beta_3PctEL_i + u_i\n\\end{equation}\\] The null hypothesis that “school resources don’t matter” and the alternative that they do, corresponds to:\n\n\\(H_0:\\beta_1 =0\\) and \\(\\beta_2 =0\\) vs\n\\(H_1:\\) either \\(\\beta_1 \\neq 0\\) or \\(\\beta_2 \\neq 0\\) or both\n\nThis is a joint hypothesis specifying a value for two or more coefficients. That is, it imposes a restriction on two or more coefficients. In general, a joint hypothesis will involve \\(q\\) restrictions. In the example above, \\(q = 2\\), and the two restrictions are \\(\\beta_1 = 0\\) and \\(\\beta_2 = 0\\). A “common sense” idea is to reject if either of the individual \\(t\\)-statistics exceeds 1.96 in absolute value. But this “one at a time” test isn’t valid: the resulting test rejects too often under the null hypothesis (more than 5%)! That is because the \\(t\\)-statistics themselves are often not independent. Instead, we need a \\(F\\)-statistic, which tests all parts of a joint hypothesis at once. Unfortunately, these types of formulas can become quickly rather complex. Consider the \\(F\\)-test for the special case of the joint hypothesis \\(\\beta_1 = \\beta_{1,0}\\) and \\(\\beta_2 = \\beta_{2,0}\\) in a regression with two regressors:\n\\[\\begin{equation}\nF = \\frac{1}{2} \\left(\\frac{t_1^2 + t_2^2 - 2\\hat{\\rho}_{t_1,t_2}t_1 t_2}{1-\\hat{\\rho}^2_{t_1 t_2}}  \\right)\n\\end{equation}\\]\nwhere \\(\\hat{\\rho}_{t_1,t_2}\\) estimates the correlation between \\(t_1\\) and \\(t_2\\). Reject when \\(F\\) is large (typically to be determined from large statistical tables). The F-statistic is large when \\(t_1\\) and/or \\(t_2\\) is large and the F-statistic corrects (in just the right way) for the correlation between \\(t_1\\) and \\(t_2\\). The formula for more than two \\(\\beta\\)’s is nasty unless you use matrix algebra. There is a nice large-sample (\\(n>50\\)) approximate distribution, which is the tail probability of the \\(\\chi^2_q /q\\) distribution beyond the \\(F\\)-statistic actually computed.\nNow, STATA does this in a much easier way by invoking the test command right after the regression. So, for example, we want to test the joint hypothesis that the population coefficients on \\(STR\\) and expenditures per pupil (\\(expn\\)) are both zero, against the alternative that at least one of the population coefficients is nonzero.\n\nreg testscr str expn_stu el_pct, r \ntest str expn_stu\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 3, 416)         =     147.20\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.4366\n                                                Ro\n> ot MSE          =     14.353\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -.2863992   .4820728    -0.59   0.\n> 553                                             \n>        -1.234002                                \n>                      .661203\n    expn_stu |   .0038679   .0015807     2.45   0.\n> 015                                             \n>         .0007607                                \n>                     .0069751\n      el_pct |  -.6560227   .0317844   -20.64   0.\n> 000                                             \n>        -.7185008                                \n>                    -.5935446\n       _cons |   649.5779   15.45834    42.02   0.\n> 000                                             \n>         619.1917                                \n>                     679.9641\n--------------------------------------------------\n> ----------------------------\n\n\n ( 1)  str = 0\n ( 2)  expn_stu = 0\n\n       F(  2,   416) =    5.43\n            Prob > F =    0.0047\n\n\nThe output shows an \\(F\\)-statistic with \\(q=2\\) restrictions with outcome 5.43. Do not directly interpret this number, but know that \\(\\text{Prob} > F = 0.0047\\) gives the probability that under the null-hypothesis this outcome is produced. So the joint null-hypothesis that both types of expenditures are zero (at the same time), can be rejected at a 5% (and a 1%) significance level. Other types of joint tests can easily be constructed as well. For example, when you want to know whether both coefficient add up to 1, then you would state test str + expn_stu = 1. The final point to make is the \\(F\\)-test in the regression output itself. Here, that is for example F(3, 416) = 147.20. This is a joint test that all variables, except the constant, have no impact. So, \\(\\beta_i = 0\\) for all \\(i\\) at the same time. It not often that you come across a general regression \\(F\\)-test that does not reject the null-hypothesis. It namely implies that your independent variables do not contain any information about the dependent variable.\nAnd with the \\(F\\)-test, we now have discussed all regression outcome components displayed by STATA. Most of this information you do not need for your report but we will come back later to this."
  },
  {
    "objectID": "multivariate.html#sec-nonlinear",
    "href": "multivariate.html#sec-nonlinear",
    "title": "3  Modeling in the Social Sciences",
    "section": "3.3 Non-linear specifications",
    "text": "3.3 Non-linear specifications\nThe model we are using is coined the linear regression model, and, indeed, one of the underlying assumptions is that the relations between the independent and dependent are linear. Consider the relation again between test scores and class sizes in the Californian school district data. Using the following code (note now the twoway command that ‘binds’ a scatter plot with a population regression line):\n\ngraph twoway (lfit testscr str) (scatter testscr str)\n\nWhich provides the following STATA output.\n\n\n\n\n\nA linear relation\n\n\n\n\nIndeed, there might be evidence that the relation depicted in Figure @ref(fig:scatterlfitcaschool)—if anything—is linear. But, clearly that is not the case for the relation between test scores and average district income. Namely, the syntax below:\n\ngraph twoway (lfit testscr avginc) (scatter testscr avginc)\n\nprovides the following STATA output.\n\n\n\n\n\nA non-linear relation\n\n\n\n\nFigure @ref(fig:scatterincome) shows a non-linear relation, where the effect of income tapers off (note the resemble with Figure @ref(fig:marginalutility))—or, there is a marginal decreasing effect of average district income on average school test scores. Thus, in affluent neighborhood test scores are higher, but increasingly less so. Of course, you can still try to estimate this with a linear population regression line as in Figure @ref(fig:scatterincome), but this introduces a bias. The estimate does not capture that what you want. Namely, it now holds that \\(E(u \\mid X = x) \\neq 0\\), because for small \\(X\\), say \\(X<10\\), the residuals are negative, for medium sized \\(X\\)s most residuals are positive and for large \\(X>40\\) all residuals are negative again. So, there is a clear relation between \\(X\\) and \\(u\\) and they fail to be independent. This particular form of bias is coined specification bias. There is another issue here and that is that the effect on \\(Y\\) of a change in \\(X\\) depends on the value of \\(X\\)—that is, the marginal effect of \\(X\\) is not constant.\nTo remedy the specification bias, we will use nonlinear regression population regression functions of \\(X\\), or we estimate a regression function that is nonlinear in \\(X\\). Here, it is important to see that we do so by transforming \\(X\\), so the population regression ‘line’. The estimator still remains a linear regression model.\nWe will analyse below two complementary and often adopted approaches:\n\nUsing polynomials to transform \\(X\\). That means that the effect is approximated by a quadratic, cubic, or higher-degree polynomial. This approach as well governs to an extent so-called interaction effects which is a special case, where we multiply two different variables.\nUsing logarithmic transformations of \\(X\\), where \\(Y\\) and/or \\(X\\) is transformed by taking its logarithm. Here, the main focus is on the interpretation of the \\(\\hat{\\beta}\\)s, as they change from a unit increase interpretation to a percentages interpretation which often can be found useful.\n\n\n3.3.1 Polynomials\nOur first approach to non-linear specification is applying polynomials of the variables that we suspect has a non-linear impact. If that is the independent variable \\(X\\), the we can construct the following linear regression model by using polynomials: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X^2_i + \\ldots + \\beta_r X_i^r + u_i\n(\\#eq:poly)\n\\end{equation}\\] Note again that this is just the linear regression model—except that the regressors are powers of \\(X\\)! So, in effect we transform the data—actually create new variables \\(X^r\\)—, but the specification in parameters remains linear. Estimation, hypothesis testing, etc. proceeds as in the multiple regression model using OLS. However, the coefficients are now a bit more difficult to interpret. Consider the example of above about the relation between test scores average district income, where \\(Income_i\\) is defined as the average district income in the \\(i^{\\mathrm{th}}\\) district (thousands of dollars per capita). For a quadratic specification, we specify the linear regression model as below: \\[\\begin{equation}\nTestScore_i = \\beta_0 + \\beta_1 Income_i + \\beta_2 (Income_i)^2 + u_i\n\\end{equation}\\] For a cubic specification the linear regression model becomes: \\[\\begin{equation}\nTestScore_i = \\beta_0 + \\beta_1 Income_i + \\beta_2 (Income_i)^2 +\n\\beta_3 (Income_i)^3 + u_i\n\\end{equation}\\]\nFirst, we focus on the estimation of the quadratic function. In STATA this would look like:\n\nreg testscr c.avginc##c.avginc, r\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 2, 417)         =     428.52\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.5562\n                                                Ro\n> ot MSE          =     12.724\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n      avginc |   3.850995   .2680941    14.36   0.\n> 000                                             \n>          3.32401                                \n>                     4.377979\n             |\n    c.avginc#|\n    c.avginc |  -.0423085   .0047803    -8.85   0.\n> 000                                             \n>         -.051705                                \n>                    -.0329119\n             |\n       _cons |   607.3017   2.901754   209.29   0.\n> 000                                             \n>         601.5978                                \n>                     613.0056\n--------------------------------------------------\n> ----------------------------\n\n\nNow, it is straightforward to test the null-hypothesis of linearity against the alternative that the regression function is a quadratic. Namely, we only have to consider the \\(t\\)-statistic of the quadratic term. And that is larger than 1.96, so against a 5% significance level we reject the null-hypothesis of linearity.\nNote by the way the syntax c.avginc##c.avginc which seems a bit strange. However, this particular line of code is very useful for later tabulation, plotting and other manipulations of the output. In this way STATA knows that there should be a quadratic effect of the same variable (avginc). The syntax c. denotes that the variable should be considered as continuous instead of as an integer (try it and behold the horrible output). There are four useful operators that you want to know when working with polynomials and interaction effect:\n\ni. operator: this specifies that the following variable is an integers and should be considered on all its level. This actually create indicator or dummies variables\nc. operator: this specifies that the following variable is a continuous variables and should be treated as continuous.\n# binary operator that specifies an interaction between two variables\n## binary operator that specifies both interaction between two variables and the individual variable effect\n\nPlotting, non-linear population regression lines are a bit tricky. Namely, you want to combine a polynomial with a linear dimension. One way of doing this is as follows:\n\npredict hat1 \nscatter (testscr avginc) || (line hat1 avginc, sort)\n\nwhere after the regression we predict the test scores (and name it something like hat1) and then we ask for a line of the prediction for each value of average district income. Note, though, that we have to sort the prediction from small to large to get a smooth line. And this provides the nice curved population regression line in the following STATA output.\n\n\n\n\n\nA non-linear relation\n\n\n\n\nBut what is now the marginal effect of average district income. That, now, depends on itself. Namely, \\(\\frac{\\partial \\text{testscore}}{\\partial \\text{income}} = \\beta_1 + \\beta_2 \\text{income}\\). Another way of seeing this is to compute the effects for different values of \\(X\\) \\[\\begin{equation}\n\\widehat{TestScore_i} = 607.3 + 3.85 Income_i - 0.0423(Income_i)^2\n\\end{equation}\\] The predicted change in test scores for a change in income from $5,000 per capita to $6,000 per capita then amounts to: \\[\\begin{eqnarray}\n\\Delta \\widehat{TestScore} &=& 607.3 + 3.85 \\times 6 -  0.0423 \\times 6^2 \\\\\n&& - (607.3 + 3.85\\times 5 - 0.0423\\times 5^2)\\\\\n&=&3.4\n\\end{eqnarray}\\]\nAnd if calculate the predicted effects for different values of \\(X\\), then we get the following table:\n\n\n\n\nEffect of $X$\n \n  \n    Change in Income (1000 dollar per capita) \n    $\\Delta \\widehat{TestScore}$ \n  \n \n\n  \n    from 5 to 6 \n    3.4 \n  \n  \n    from 25 to 26 \n    1.7 \n  \n  \n    from 45 to 46 \n    0.0 \n  \n\n\n\n\n\nThus, the effect of a change in income is greater at low than high income levels (perhaps, a declining marginal benefit of an increase in school budgets?). But, be careful here! What is the effect of a change from 65 to 66? That is quite negative and already Figure @ref(fig:scatterqua) shows that a quadratic specification start to decline again the value of about 50; and perhaps that is not the behaviour that you want. So, with polynomials it is essential not to extrapolate outside the range of the data (and still interpret the outcome).\nThe estimation of a cubic specification is straightforward:\n\nreg testscr c.avginc##c.avginc##c.avginc, r\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 3, 416)         =     270.18\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.5584\n                                                Ro\n> ot MSE          =     12.707\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n      avginc |   5.018677   .7073504     7.10   0.\n> 000                                             \n>          3.62825                                \n>                     6.409103\n             |\n    c.avginc#|\n    c.avginc |  -.0958052   .0289537    -3.31   0.\n> 001                                             \n>         -.152719                                \n>                    -.0388913\n             |\n    c.avginc#|\n    c.avginc#|\n    c.avginc |   .0006855   .0003471     1.98   0.\n> 049                                             \n>         3.26e-06                                \n>                     .0013677\n             |\n       _cons |    600.079   5.102062   117.61   0.\n> 000                                             \n>         590.0499                                \n>                      610.108\n--------------------------------------------------\n> ----------------------------\n\n\nWhere if we now want to test the null- hypothesis of linearity, then we have to have invoke an \\(F\\)-test. Namely, the alternative hypothesis is that the population regression is quadratic and/or cubic, that is, it is a polynomial of degree up to 3, so:\n\n\\(H_0\\): Coefficients on \\(Income^2\\) and \\(Income^3 = 0\\)\n\\(H_1\\): at least one of these coefficients is nonzero.\n\nAnd the outcome below shows that the null-hypothesis that the population regression is linear is rejected at the 5% (and 1%) significance level against the alternative that it is a polynomial of degree up to 3.\n\ntest avginc#avginc avginc#avginc#avginc  \n\n ( 1)  c.avginc#c.avginc = 0\n ( 2)  c.avginc#c.avginc#c.avginc = 0\n\n       F(  2,   416) =   37.69\n            Prob > F =    0.0000\n\n\n\n\n3.3.2 Interaction variables\nUsing interaction variables is a special case of polynomial effects. Namely, instead of multiply a variable with itself \\(X\\times X = X^2\\), you now multiple a variable with another variable. And you want to do this to take into account interactions between independent variables. Assume, for example, that a class size reduction is more effective in some circumstances than in others (which is quite conceivable). Perhaps smaller classes help more if there are many English learners (i.e., large migrant communities), who need more individual attention. That is, \\(\\frac{\\partial TestScore}{\\partial STR}\\) might depend on \\(PctEL\\). More generally, this subsection looks into the fact that the marginal effect of \\(\\frac{\\partial Y}{\\partial X_1}\\) might depend on some other variable \\(X_2\\).\n\n3.3.2.1 Interactions between two binary variables\nFirst, we look into the simplest (and perhaps most insightful) case of two binary (dummy variables). Consider therefore the following linear regression model: \\[\\begin{equation}\nY_i =\\beta_0 +\\beta_1 D_{1i} + \\beta_2 D_{2i} +u_i,\n\\end{equation}\\] where both \\(D_{1i}\\) and $ D_{2i}$ are now considered to be binary. Now, of course, \\(\\beta_1\\) is the effect of changing \\(D_1=0\\) to \\(D_1=1\\). So, in this specification, this effect doesn’t depend on the value of \\(D_2\\). To allow the effect of changing \\(D_1\\) to depend on \\(D_2\\), we have to include the interaction term \\(D_{1i} \\times D_{2i}\\) as a regressor: \\[\\begin{equation}\nY_i =\\beta_0 +\\beta_1 D_{1i} + \\beta_2 D_{2i} + \\beta_3 (D_{1i} \\times D_{2i}) + u_i\n\\end{equation}\\]\nTo interpret now the coefficient \\(\\beta_1\\) we compare the two cases for \\(D_1=0\\) to \\(D_1=1\\)” \\[\\begin{eqnarray}\nE(Y_i|D_{1i}=0, D_{2i}=d_2) &=& \\beta_0 + \\beta_2 d_2 \\\\\nE(Y_i|D_{1i}=1, D_{2i}=d_2) &=& \\beta_0 + \\beta_1 + \\beta_2 d_2 + \\beta_3 d_2\n\\end{eqnarray}\\] If we now subtract them from each other: \\[\\begin{equation}\nE(Y_i|D_{1i}=1, D_{2i}=d2) - E(Y_i|D_{1i}=0, D_{2i}=d_2) = \\beta_1 + \\beta_3 d_2\n\\end{equation}\\] then we have the marginal effect of \\(D_1\\) which now depends on \\(d_2\\). The interpretation of \\(\\beta_3\\) boils down to being incremental to the effect of \\(D_1\\), when \\(D_2 = 1\\)\nLet us go back to our Californian school district example with the following variables to be used: test scores, student teacher ratio, and English learners. Let: \\[\\begin{eqnarray}\nHiSTR &=& 1 \\text{ if } STR \\geq 20 \\text{ and } HiEL = 1 \\text{ if }\nPctEL \\geq 10 \\\\\nHiSTR &=& 0 \\text{ if } STR < 20 \\text{ and } HiEL = 0 \\text{ if }\nPctEL < 10 \\\\\n\\end{eqnarray}\\] And if we have the estimation results we get the following outcome. \\[\\begin{equation}\n\\widehat{TestScore} = 664.1 - 18.2 HiEL - 1.9 HiSTR - 3.5(HiSTR \\times\nHiEL)\n\\end{equation}\\] So, how to interpret the various parameters? Perhaps the simple way is to construct the following two-by-two table:\n\n\n\n\nInterpretation of interaction effects with dummies\n \n  \n     \n    $HiEL = 0$ \n    $HiEL = 1$ \n  \n \n\n  \n    $HiSTR = 0$ \n    $664.1$ \n    $664.1 - 18.2 = 645.9$ \n  \n  \n    $HiSTR = 1$ \n    $664.1 - 1.9 = 662.2$ \n    $664.1 - 1.9 - 18.2 - 3.5= 640.5$ \n  \n\n\n\n\n\nNow, Table @ref(tab:intdummies) specifies for each combination (and there are exactly four of them) of \\(HiSTR\\) and \\(HiEL\\) the average expected test score outcome. Clearly, there are different ‘marginal’ effects of \\(HiSTR\\). Namely, the effect of \\(HiSTR\\) when \\(HiEL = 0\\) is \\(-1.9\\), whilst the effect of \\(HiSTR\\) when \\(HiEL = 1\\) is \\(-1.9 - 3.5 = -5.4\\). This points out that a class size reduction is estimated to have a bigger effect when the percent of English learners is large. However, when you estimate this in STATA then you see that this interaction is not statistically significant, because the \\(t\\)-statistic equals \\(3.5/3.1 = 1.1\\)\n\n\n3.3.2.2 Interactions between continuous and binary variables\nThe second case we consider is between a continuous and a binary variable. First assume the following regression model: \\[\\begin{equation}\nY_i =\\beta_0 + \\beta_1 X_i + \\beta_2 D_i + +u_i,\n\\end{equation}\\] where \\(D_i\\) is a binary variable and \\(X\\) is a continuous variable. As specified above, the effect on \\(Y\\) of \\(X\\) (holding \\(D\\) constant) = \\(\\beta_1\\), which does not depend on \\(D\\). To allow the effect of \\(X\\) to depend on \\(D\\), we can include the interaction term \\(D_i \\times X_i\\) as a regressor: \\[\\begin{equation}\nY_i =\\beta_0 + \\beta_1 X_i + \\beta_2 D_i  + \\beta_3 (D_i \\times X_i) + u_i\n\\end{equation}\\]\nWhat this binary-continuous interaction does is essential create two different population regression lines. Namely, for observations with \\(D_i= 0\\) (the \\(D = 0\\) group or the \\(D=0\\) regression line) there is: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_i  + u_i,\n\\end{equation}\\] Whilst for observations with \\(D_i= 1\\) (the \\(D = 1\\) group or the \\(D = 1\\) regression line) the regression line comes down to: \\[\\begin{eqnarray}\nY_i &=&   \\beta_0 + \\beta_2 + \\beta_1 X_i + \\beta_3 X_i + u_i \\\\\n            &=&  (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) X_i + u_i\n\\end{eqnarray}\\]\nAnd these two population regression lines might both differ in the level (the constant) and in the slope of the line. So, there are three possibilities as depicted in Figure @ref(fig:interaction)\n\n\n\n\n\nThree possible binary-continuous interaction outcomes\n\n\n\n\nIn the first panel (a), \\(\\beta_3 = 0\\), so there is only a level effect. In the second panel (b), both \\(\\beta_2\\) and \\(\\beta_3\\) are not 0, so there is both a level and a slope effect. The last panel indicates that \\(\\beta_2 = 0\\), meaning that there is only a slope effect. But how to interpreting the coefficients now? Therefore, we take the marginal effect of \\[\\begin{equation}\nY =\\beta_0  + \\beta_1 X  +\\beta_2 D+ \\beta_3 (D \\times X)\n\\end{equation}\\] which yields: \\[\\begin{equation}\n\\frac{\\partial Y}{\\partial X} = \\beta_1 + \\beta_3 D\n\\end{equation}\\] Thus, the effect of \\(X\\) depends on \\(D\\) and \\(\\beta_3\\) is the increment to the effect of \\(X\\), when \\(D = 1\\) (a slope effect)\nTo see this in our Californian school district example we now use the variables test scores, student teacher ratio and the as previously defined dummy variable \\(HiEL\\) as: \\[\\begin{equation}\n\\widehat{TestScore} = 682.2 - 0.97 STR + 5.6 HiEL - 1.28(STR \\times HiEL)\n\\end{equation}\\] Now when \\(HiEL = 0\\) the population regression line amounts to: \\[\\begin{equation}\n\\widehat{TestScore} = 682.2 - 0.97 STR\n\\end{equation}\\] And when \\(HiEL = 1\\) the population regression line is: \\[\\begin{eqnarray}\n\\widehat{TestScore} &=& 682.2 - 0.97 STR + 5.6 - 1.28 STR \\\\\n&=& 687.8 - 2.25 STR\n\\end{eqnarray}\\] Thus we have two regression lines: one for each \\(HiSTR\\) group. And the conclusion is that a class size reduction is estimated to have a larger effect when the percent of English learners (migrant communities) is large.\nHypothesis testing is as before. To test whether the two regression lines have the same slope, the null-hypothesis boils down to the coefficient of \\(STR \\times HiEL\\) being zero: the \\(t\\)-statistic of this one become \\(-1.28/0.97 = -1.32\\) and thus we do not reject this test. To test whether the two regression lines have the same intercept, the null-hypothesis becomes the coefficient of \\(HiEL\\) being zero, yielding: \\(t = -5.6/19.5 = 0.29\\), so we do not reject that null-hypothesis either. Interestingly, the null-hypothesis that the two regression lines are the same—population coefficient on \\(HiEL = 0\\) and population coefficient on yields \\(STR \\times HiEL = 0\\): \\(F = 89.94 (p-value < .001)\\). So, we reject the joint hypothesis but neither individual hypothesis.\nFinally, the question may arise how to draw such lines as in Figure @ref(fig:interaction). For this the following code is very useful:\n\ngen hiel = (el_pct >= 10)\nreg testscr c.str##i.hiel, r\nmargins hiel, at(str=( 14 ( 2 ) 26 ))\nmarginsplot\n\nLinear regression                               Nu\n> mber of obs     =        420\n                                                F(\n> 3, 416)         =      63.67\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.3103\n                                                Ro\n> ot MSE          =      15.88\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n     testscr | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n         str |  -.9684601   .5891016    -1.64   0.\n> 101                                             \n>        -2.126447                                \n>                     .1895268\n      1.hiel |   5.639141   19.51456     0.29   0.\n> 773                                             \n>        -32.72029                                \n>                     43.99857\n             |\n  hiel#c.str |\n          1  |  -1.276613   .9669194    -1.32   0.\n> 187                                             \n>         -3.17727                                \n>                     .6240436\n             |\n       _cons |   682.2458   11.86781    57.49   0.\n> 000                                             \n>         658.9175                                \n>                     705.5742\n--------------------------------------------------\n> ----------------------------\n\n\nAdjusted predictions                              \n>          Number of obs                          \n>                        = 420\nModel VCE: Robust\n\nExpression: Linear prediction, predict()\n1._at: str = 14\n2._at: str = 16\n3._at: str = 18\n4._at: str = 20\n5._at: str = 22\n6._at: str = 24\n7._at: str = 26\n\n--------------------------------------------------\n> ----------------------------\n             |            Delta-method\n             |     Margin   std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n    _at#hiel |\n        1 0  |   668.6874   3.701457   180.66   0.\n> 000                                             \n>         661.4115                                \n>                     675.9633\n        1 1  |    656.454    4.85794   135.13   0.\n> 000                                             \n>         646.9048                                \n>                     666.0031\n        2 0  |   666.7505   2.577328   258.70   0.\n> 000                                             \n>         661.6843                                \n>                     671.8167\n        2 1  |   651.9638   3.391411   192.24   0.\n> 000                                             \n>         645.2974                                \n>                     658.6302\n        3 0  |   664.8136   1.536485   432.68   0.\n> 000                                             \n>         661.7933                                \n>                     667.8338\n        3 1  |   647.4737   2.026549   319.50   0.\n> 000                                             \n>         643.4901                                \n>                     651.4572\n        4 0  |   662.8766   .9248105   716.77   0.\n> 000                                             \n>         661.0588                                \n>                     664.6945\n        4 1  |   642.9835    1.18965   540.48   0.\n> 000                                             \n>          640.645                                \n>                      645.322\n        5 0  |   660.9397   1.458111   453.28   0.\n> 000                                             \n>         658.0735                                \n>                     663.8059\n        5 1  |   638.4934   1.851156   344.92   0.\n> 000                                             \n>         634.8546                                \n>                     642.1321\n        6 0  |   659.0028   2.484598   265.24   0.\n> 000                                             \n>         654.1189                                \n>                     663.8867\n        6 1  |   634.0032    3.18456   199.09   0.\n> 000                                             \n>         627.7434                                \n>                     640.2631\n        7 0  |   657.0659   3.605093   182.26   0.\n> 000                                             \n>         649.9794                                \n>                     664.1523\n        7 1  |   629.5131    4.64319   135.58   0.\n> 000                                             \n>          620.386                                \n>                     638.6401\n--------------------------------------------------\n> ----------------------------\n\n\nVariables that uniquely identify margins: str\nhiel\n\n\nSo, first, we generate a new dummy variable hiel as discussed above. Then we regress testscores on class size, the new hiel dummy variable and the interaction using the two hashtags. We then ask for the marginal effect of hiel, so for both values of it (being 0 and 1), for all class sizes between 14 and 26 (with steps of 2). Finally, we ask for the plots of the margins using the command marginsplot. This provides the following STATA plot.\n\n\n\n\n\nPredicted population regression lines of districts with large and small percentage english learners\n\n\n\n\nClearly, Figure @ref(fig:interactionplot) shows that district with more English learners (containing larger migrant communities) have lower test scores overall. Above that, class size seems to have a large negative effect on districts with more English learners as the slope is more negative.\n\n\n3.3.2.3 Interactions between two continuous variables\nThe last case are interaction between two continuous variables and that is always a difficult case of interpret. Starting again with the model: \\[\\begin{equation}\nY_i =\\beta_0 + \\beta1 X_{1i} +\\beta_2 {X_{2i}} +u_i,\n\\end{equation}\\] where both \\(X_1\\), \\(X_2\\) are continuous and as specified, the effect of \\(X_1\\) doesn’t depend on \\(X_2\\) and the effect of \\(X_2\\) doesn’t depend on \\(X_1\\). Now, to allow the effect of \\(X_1\\) to depend on \\(X_2\\), we include the interaction term \\(X_{1i} \\times X_{2i}\\) as a regressor. Where, to interpret the coefficients, we take the first derivative of \\(X_1\\) in: \\[\\begin{equation}\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 (X_{1i}\n\\times X_{2i}) + u_i\n\\end{equation}\\] which yields: \\[\\begin{equation}\n\\frac{\\partial Y}{\\partial X} = \\beta_1 + \\beta_3 X_2\n\\end{equation}\\] where \\(\\beta_3\\) should be interpreted as the increment to the effect of \\(X_1\\) from a unit change in \\(X_2\\).\n\n\n\n3.3.3 Logarithmic transformations\nTo incorporate non-linear effect, very often logarithmic transformations are used of \\(Y\\) and/or \\(X\\), where typically we use \\(\\ln(X)\\) as the natural logarithm of \\(X\\). One feature of logarithmic transformations is that they permit modeling relations in percentage terms (like elasticities), rather than linearly. That is because: \\[\\begin{equation}\n\\ln(x+\\Delta x) - \\ln(x) = \\ln (1 + \\frac{\\Delta x}{x}) \\cong \\frac{\\Delta x}{x}\n\\end{equation}\\] Note that this is an approximation, but from calculus we know that \\(\\frac{d \\ln(x)}{dx}=\\frac{1}{x})\\). And the above approximation works quite well for small numbers. For example, numerically: \\(\\ln(1.01) = .00995 \\cong .01\\) and \\(\\ln(1.10) = .0953 \\cong .10\\), where the latter is still rather close. Now remember the following rules for natural logarithms 1. \\(\\ln(a\\times b)= \\ln(a)+\\ln(b)\\) 2. \\(\\ln(\\frac{a}{b}) =\\ln(a) - \\ln(b)\\) 3. \\(\\ln(a^\\alpha) = \\alpha \\ln(a)\\) 4. \\(\\ln(e^X) = X\\).\nWhen you encounter a nonlinear model such as the ones adopted in Chapter @ref(surplus) a strategy that often works is log-linearization. That works as follows \\[\\begin{equation}\nY = A K^\\alpha L^{1-\\alpha} \\rightarrow \\ln(Y) = \\ln(A) + \\alpha \\ln(K) + (1-\\alpha) \\ln(L),\n\\end{equation}\\] where you take the natural logarithm on both sides. There are three different cases of logarithmic regression models as specified in Table @ref(tab:logspecification).\n\n\n\n\nThree logarithmic transformation\n \n  \n    Case \n    Population regression model \n  \n \n\n  \n    linear-log \n    $Y_i=\\beta_0 + \\beta_1 \\ln(X_i) + u_i$ \n  \n  \n    log-linear \n    $\\ln(Y_i)=\\beta_0 + \\beta_1 (X_i) + u_i$ \n  \n  \n    log-log \n    $\\ln(Y_i)=\\beta_0 + \\beta_1 \\ln(X_i) + u_i$ \n  \n\n\n\n\n\nThough statistical testing remains the same, the interpretation of the slope coefficient differs in each case. To derive the interpretationwe want to find the marginal effect of \\(X\\) using the first derivative.\n\n3.3.3.1 Linear-log population regression model\nThe linear-log population regression model is specified as: \\[\\begin{equation}\n    Y = \\beta_0 + \\beta_1 \\ln(X)\n\\end{equation}\\] Now take the first derivative: \\[\\begin{equation}\n    \\frac{\\partial Y}{\\partial X} = \\frac{\\beta_1}{X}\n\\end{equation}\\] so \\[\\begin{equation}\n    \\beta_1  = \\frac{\\partial Y}{\\partial X / X}\n\\end{equation}\\] In this case that means that \\(\\beta_1\\) should be interpreted as the absolute change of \\(Y\\) when \\(X\\) changes with \\(\\beta_1/100\\) percent. To illustrate this, consider the case where we take natural logarithm od district income, so we define the new regressor as, \\(\\ln(Income)\\)\nThe model is now linear in \\(\\ln(Income)\\), so the linear-log model can be estimated by OLS, which yields \\[\\begin{equation}\n        \\widehat{TestScore} = 557.8 + 36.42\\times \\ln(Income_i)\n\\end{equation}\\] so an 1% increase in \\(Income\\) is associated with an increase in test scores of 0.36 points on the test. And again, standard errors, confidence intervals, \\(R^2\\)—all the usual tools of regression apply here. But the difficulty in plottin the new regression line remains. Consider the following STATA syntax, where we first have to define the new regressor by invoking the generate command.\n\ngen lninc = ln(avginc)\nreg testscr lninc, r\npredict testhat\ngraph twoway (line testhat avginc, sort) (scatter testscr avginc)\n\nThis now provides the following STATA output.\n\n\n\n\n\nA non-linear relation\n\n\n\n\nWhen you compare @ref(fig:scatterlnincome) with @ref(fig:scatterqua) then you notice that in the case of logarithm the population remains increasing (but less and less steep). This can be considered as an advantage when you want to estimate decreasing (or increasing) return.\n\n\n3.3.3.2 Log-linear population regression model\nThe second case we consider is the log-linear population regression model, as specified by: \\[\\begin{equation}\n    \\ln(Y) = \\beta_0 + \\beta_1 X\n\\end{equation}\\] To find the interpretation of \\(\\beta1_1\\), we again take the first derivative \\(\\frac{\\partial Y}{\\partial X}\\), but first transform the model like this: \\[\\begin{equation}\n    Y = exp( \\beta_0 + \\beta_1 X )\n\\end{equation}\\] then take the first derivative: \\[\\begin{equation}\n    \\frac{\\partial Y}{\\partial X} = \\beta_1  exp( \\beta_0 + \\beta_1 X ) = \\beta_1 Y\n\\end{equation}\\] and collec terms \\[\\begin{equation}\n    \\beta_1  = \\frac{\\partial Y / Y}{\\partial X }\n\\end{equation}\\]\nThe interpretation of \\(\\beta_1\\) now is that one unit change in \\(X\\) causes a \\(\\beta_1\\) percentage in \\(Y\\)\n\n\n3.3.3.3 Log-log population regression model\nFinally, we have our third case, being the log-log population regression model as specified by: \\[\\begin{equation}\n    \\ln(Y) = \\beta_0 + \\beta_1 \\ln(X)\n\\end{equation}\\]\nTo find the interpretation of \\(\\beta1_1\\), we again take the first derivative \\(\\frac{\\partial Y}{\\partial X}\\), but first transform the model like this: \\[\\begin{equation}\n    Y = exp( \\beta_0 + \\beta_1 \\ln(X) )\n\\end{equation}\\] So \\[\\begin{equation}\n    \\frac{\\partial Y}{\\partial X} = \\beta_1 /X  exp( \\beta_0 + \\beta_1 \\ln(X) ) = \\beta_1 Y /X\n\\end{equation}\\] and after collecting terms we end up with an elasticity: \\[\\begin{equation}\n    \\beta_1  = \\frac{\\partial Y / Y}{\\partial X / X }\n\\end{equation}\\]\nAs an example consider the case when we want to regress ln(test scores) on ln(income). To do so, we first define a new dependent variable, ln(TestScore), and a new regressor, ln(Income) The model is now a linear regression of ln(TestScore) against ln(Income), which can be estimated by OLS as follows \\[\\begin{equation}\n\\widehat{ln(TestScore)} = 6.336 + 0.0554 \\times ln(Income_i),\n\\end{equation}\\] where the interpretation is that an 1% increase in \\(Income\\) is associated with an increase of .0554% in \\(TestScore\\) (\\(Income\\) up by a factor of 1.01, \\(TestScore\\) up by a factor of 1.000554)\nSuppose that we now want to plot both the log-linear and the log-log specification, then we can use the following syntax:\n\ngen lninc = ln(avginc)\ngen lntestscr = ln(testscr)\nreg lntestscr lninc, r\npredict testhat1\nreg lntestscr avginc, r\npredict testhat2\ngraph twoway (line testhat1 avginc, sort) (line testhat2 avginc, sort) (scatter lntestscr avginc), legend(order(1 \"log-log specification\" 2 \"log-linear specification\" 3 \"Observations\")) \n\nwhich provides the following STATA output.\n\n\n\n\n\nA non-linear relation\n\n\n\n\nNote that the \\(y\\)-axis is on a logarithmic scale here, thus the log-linear specification is now a linear line.\n\n\n3.3.3.4 Summary: logarithmic transformations\nWe have seen three different cases of logarithmic specification, differing in whether \\(Y\\) and/or \\(X\\) is transformed by taking logarithms. Now, the regression is linear in the new variable(s) \\(\\ln(Y)\\) and/or \\(\\ln(X)\\), and the coefficients can be estimated by OLS where hypothesis tests and confidence intervals are now implemented and interpreted ‘as usual’. Only the interpretation of the coefficients differs from case to case and is directly related to percentage changes (growth) and elasticities. Oftentimes, the choice of specification, however, should be guided by judgment (which interpretation makes the most sense in your application?), tests, and plotting predicted values. Sometimes, though, you have a structural economic model such as Equation @ref(eq:directutility), which defines the type of specification you should use. Finally, see that in economics many models exists with decreasing or increasing return to scale and that these are very closely related with logarithmic specifications."
  },
  {
    "objectID": "multivariate.html#sec-fixedeffects",
    "href": "multivariate.html#sec-fixedeffects",
    "title": "3  Modeling in the Social Sciences",
    "section": "3.4 Using fixed effects in panel data",
    "text": "3.4 Using fixed effects in panel data\nMultivariate regression is a powerfull tool for controlling for the effect of variables for which we have data. But often we do not have data on what we suspect might be important—data, such as individual characteristics like ambition, intelligence, drive or stamina. Or regional of country data, where the type of soil, the ruggedness (hilliness), or population density determine to a large extent the behaviour of people living on it. If we do not have this type of data, then it not always the case that everything is lost. Especially, when we have repeated observations, so observations of the same entity throughout time. This is referred to as panel data and requires one additional subscript \\(t\\) as in \\(X_{it}\\) indicating the observation \\(X\\) on individual made at time \\(t\\). To understand why this sometimes works, we temporarily change to another dataset and that is the ‘fatality’ data collected by Levitt and Porter (2001) and deals with the relation between drunk driving and fatal accidents in the States of the US between 1982 and 1988. For this particular example we look at the impact of the ‘beer tax’, measured as the real tax in dollars on a case of beer, on ‘fatality’, measured as the number of annual traffic deaths per 10,000 people in the population of each stata. For this we first read the data and manipulate the mortality variable\n\nuse \"./data/fatality.dta\", clear\ngen fatality = allmort/pop * 10000\n\nand then run a simple regression:\n\nregress fatality beertax, robust\n\nLinear regression                               Nu\n> mber of obs     =        336\n                                                F(\n> 1, 334)         =      47.59\n                                                Pr\n> ob > F          =     0.0000\n                                                R-\n> squared         =     0.0934\n                                                Ro\n> ot MSE          =     .54374\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n    fatality | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n     beertax |   .3646054   .0528524     6.90   0.\n> 000                                             \n>         .2606399                                \n>                      .468571\n       _cons |   1.853308   .0471297    39.32   0.\n> 000                                             \n>         1.760599                                \n>                     1.946016\n--------------------------------------------------\n> ----------------------------\n\n\nBut these outcomes are very strange. For every dollar increase in tax, number of fatal accidents per 10,000 people increases with 0.36, which is statistically significantly different from 0. What is going on here. Most likely this effect is biased because of omitted variable bias. States in the US differ widely in terms of population density, environment, institutions, religion, poverty, and so on and so forth. And Those state characteristics might influence both the variables beertax and fatality.\nFortunately, for each state we have yearly data. So, that is 7 observations per stata and we can make use of that by using fixed effects, which is a very common technique in the social sciences—especially in economics. We model the use of fixed effects in this example as follows: \\[\\begin{equation}\n\\text{fatality}_{it} = \\beta_0 + \\beta_1\\text{beertax}_{it} + \\beta_3 S_1 + \\ldots + \\beta_51 S_{48} + u_{it},\n\\end{equation}\\] where \\(S_i\\) denote indicator (dummies) for each state which constitute the fixed effects. In total there are 48 states in this dataset, so we have 48 dummies. Note that these fixed effects only depend on the state variation, not on time variation. So, essentially what these fixed effects capture is all state specific characteristics which are constant over time. And most of the characteristics’ examples given above do not vary that much over time, so by using these state fixed effects we can control for them. In STATA you can estimate this in a straightforward way as regress fatality beertax i.state, robust, but this lots of statistical output that you are usually not interested in. Almost just as easy would be is to invoke the areg command, where you specifically state that the state variable should be used as dummies but not shown using absorb(state): and then run a simple regression:\n\nareg fatality beertax, absorb(state) robust\n\nLinear regression, absorbing indicators           \n>   Number of obs                                 \n>                     =    336\nAbsorbed variable: state                          \n>   No. of categories                             \n>                     =     48\n                                                  \n>   F(1, 287)                                     \n>                     =  10.41\n                                                  \n>   Prob > F                                      \n>                     = 0.0014\n                                                  \n>   R-squared                                     \n>                     = 0.9050\n                                                  \n>   Adj R-squared                                 \n>                     = 0.8891\n                                                  \n>   Root MSE                                      \n>                     = 0.1899\n\n--------------------------------------------------\n> ----------------------------\n             |               Robust\n    fatality | Coefficient  std. err.      t    P>\n> |t|                                             \n>         [95% con                                \n>                 f. interval]\n-------------+------------------------------------\n> ----------------------------\n     beertax |  -.6558737   .2032797    -3.23   0.\n> 001                                             \n>        -1.055982                                \n>                    -.2557655\n       _cons |   2.377075   .1051516    22.61   0.\n> 000                                             \n>         2.170109                                \n>                     2.584041\n--------------------------------------------------\n> ----------------------------\n\n\nNow, see what happens with the coefficient of the beer tax variable. It changes sign! So from positive it becomes negative. That is how disruptive omitted variable bias can be. Also see that by including all these state fixed effects, the \\(\\bar{R^2}\\) now increase enormously to 91%, which does make sense because the states explain the variation in fatality rate to a large extent (e.g., compare Kansas with Connecticut).\nThis is just a snapshot of the use of fixed effects in panel data, but for now this is enough. But for now, know that the use of fixed effects can go a long way in addressing omitted variable bias."
  }
]